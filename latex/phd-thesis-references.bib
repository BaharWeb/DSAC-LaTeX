@inproceedings{Zarei2017,
author = {Zarei, Bahareh and Heil, Sebastian and Gaedke, Martin},
doi = {10.1007/978-3-319-60131-1},
file = {:Users/baharehzarei/Downloads/demoPaper135.pdf:pdf},
isbn = {9783319601311},
number = {August 2017},
title = {{Intelligent End User Development Platform Towards Enhanced Decision- Making Natural-Language-Enabled End-User Tool Endowed with Ontology-based Development}},
year = {2017}
}
@book{Murugesan2001,
abstract = {In most cases, development of Web-based systems has been ad hoc, lacking systematic approach, and quality control and assurance procedures. Hence, there is now legitimate and growing concern about the manner in which Web-based systems are developed and their quality...},
author = {Murugesan, San and Deshpande, Yogesh and Hansen, Steve and Ginige, Athula},
booktitle = {Springer, Berlin, Heidelberg},
doi = {10.1007/3-540-45144-7_2},
editor = {Murugesan, San and and Deshpande, Yogesh},
isbn = {978-3-540-45144-0},
pages = {3--13},
publisher = {Springer, Berlin, Heidelberg},
title = {{Web Engineering: a New Discipline for Development of Web-Based Systems}},
url = {https://link.springer.com/chapter/10.1007/3-540-45144-7{\_}2},
year = {2001}
}
@article{Huynh2023,
abstract = {Semantic Table Interpretation (STI), or Semantic Table Annotation, is the process of understanding the semantics of tabular data with reference information identified in knowledge graphs (KG). In this paper, we first present insights gained from the design and implementation of DAGOBAH SL, a top performing STI system in state-of-the-art benchmarks, and we discuss the unsolved challenges that need to be addressed to make STI more effective in practice. Pre-trained generative Large Language Models (LLMs) have demonstrated their powerful versatility in tackling a broad spectrum of natural language understanding tasks. We envision their potential for improving STI systems. We describe several appealing research ideas that could lay the foundation for future development of Generative Semantic Table Interpretation.},
author = {Huynh, Viet Phi and Chabot, Yoan and Troncy, Rapha{\"{e}}l},
file = {:Users/baharehzarei/Desktop/Papers/TADA7.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {DAGOBAH,Generative Information Extraction,Knowledge Graph,Large Language Model,Semantic Table Interpretation},
title = {{Towards Generative Semantic Table Interpretation}},
volume = {3462},
year = {2023}
}
@article{Sanchez2012,
author = {S{\'{a}}nchez, David and Moreno, Antonio and Vasto-terrientes, Luis Del},
doi = {10.1016/j.eswa.2011.11.088},
file = {:Users/baharehzarei/Desktop/Papers/1-s2.0-S0957417411016344-main.pdf:pdf},
pages = {5792--5805},
title = {{Expert Systems with Applications Learning relation axioms from text : An automatic Web-based approach}},
volume = {39},
year = {2012}
}
@article{Schae2023,
author = {Schae, Marion and Sesbo, Matthias and Zanni-merk, Cecilia},
doi = {10.1016/j.procs.2023.10.201},
file = {:Users/baharehzarei/Desktop/Papers/1-s2.0-S1877050923013595-main.pdf:pdf},
title = {{ScienceDirect OLAF : An Ontology Learning Applied Framework}},
year = {2023}
}
@article{B2021,
author = {B, Mahda Noura and Wang, Yichen and Heil, Sebastian and Gaedke, Martin},
file = {:Users/baharehzarei/Desktop/Papers/514401{\_}1{\_}En{\_}Print.indd.pdf:pdf},
isbn = {9783030742966},
keywords = {concept,extraction,internet of things,model-driven engineering,natural language generation,ontology,semantic web},
pages = {37--52},
title = {{OntoSpect : IoT Ontology Inspection by Concept Extraction and Natural Language Generation}},
year = {2021}
}
@article{Du2024,
archivePrefix = {arXiv},
arxivId = {arXiv:2404.14991v2},
author = {Du, Rick and An, Huilong and Wang, Keyu and Liu, Weidong},
doi = {https://doi.org/10.48550/arXiv.2404.14991},
eprint = {arXiv:2404.14991v2},
file = {:Users/baharehzarei/Desktop/Papers/2404.14991v2.pdf:pdf},
journal = {arXiv preprint arXiv:2404.14991},
title = {{A SHORT REVIEW FOR ONTOLOGY LEARNING: STRIDE TO LARGE LANGUAGE MODELS TREND}},
volume = {Version 2},
year = {2024}
}
@article{Perera2024,
author = {Perera, Olga and Liu, Jun},
file = {:Users/baharehzarei/Desktop/Papers/4{\_}iis{\_}2024{\_}299-310.pdf:pdf},
keywords = {deep learning,generative ai,large language models,llm,ontology learning},
number = {4},
pages = {299--310},
title = {{Exploring large language models for ontology learning}},
volume = {25},
year = {2024}
}
@incollection{Vaswani2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.03762v7},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Neural Information Processing Systems},
eprint = {arXiv:1706.03762v7},
file = {:Users/baharehzarei/Desktop/Papers/1706.03762v7.pdf:pdf},
number = {Nips},
title = {{Attention Is All You Need}},
url = {https://api.semanticscholar.org/CorpusID:13756489},
year = {2017}
}
@inproceedings{Tang2023,
address = {Bilbao},
archivePrefix = {arXiv},
arxivId = {arXiv:2307.11769v1},
author = {Tang, Yun and Bruto, Antonio A and Patrick, Irvine and Jennings, Paul},
booktitle = {26th IEEE International Conference on Intelligent Transportation Systems},
doi = {https://doi.org/10.48550/arXiv.2307.11769},
eprint = {arXiv:2307.11769v1},
file = {:Users/baharehzarei/Desktop/Papers/2307.11769v1.pdf:pdf},
pages = {3893--3900},
title = {{Domain Knowledge Distillation from Large Language Model : An Empirical Study in the Autonomous Driving Domain}},
year = {2023}
}
@inproceedings{Giglou2023,
archivePrefix = {arXiv},
arxivId = {arXiv:2307.16648v2},
author = {Giglou, Hamed Babaei and D'Souza, Jennifer and Auer, S¨oren},
booktitle = {International Semantic Web Conference},
eprint = {arXiv:2307.16648v2},
file = {:Users/baharehzarei/Desktop/Papers/2307.16648v2.pdf:pdf},
keywords = {large language models,learning,llms,ontologies,ontology,prompt-based learning,prompting},
pages = {408--427},
title = {{LLMs4OL: Large Language Models ‌ for Ontology Learning}},
year = {2023}
}
@article{Raad,
author = {Raad, Joe and Cruz, Christophe},
file = {:Users/baharehzarei/Downloads/A Survey on Ontology Evaluation Methods.pdf:pdf},
title = {{A Survey on Ontology Evaluation Methods}}
}
@article{Crum2024,
archivePrefix = {arXiv},
arxivId = {arXiv:2410.03235v1},
author = {Crum, Elias and Santis, Antonio De and Ovide, Manon and Pan, Jiaxin and Pisu, Alessia and Lazzari, Nicolas and Rudolph, Sebastian},
eprint = {arXiv:2410.03235v1},
file = {:Users/baharehzarei/Desktop/Papers/2410.03235v1.pdf:pdf},
issn = {1613-0073},
keywords = {disjointness learning,large language models,ontology enrichment},
title = {{Enriching Ontologies with Disjointness Axioms using Large Language Models}},
year = {2024}
}
@article{Toro2023,
author = {Toro, Sabrina and Anagnostopoulos, Anna V and Bello, Susan M and Blumberg, Kai and Carmody, Leigh and Diehl, Alexander D and Dooley, Damion M and William, D and Lubiana, Tiago and Munoz-torres, Monica C and Neil, Shawn O},
file = {:Users/baharehzarei/Desktop/Papers/2312.10904v2.pdf:pdf},
journal = {arXiv preprint arXiv:2312.10904},
pages = {1--31},
title = {{Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence ( DRAGON-AI )}},
year = {2023}
}
@article{Gaedke2003,
author = {Gaedke, Martin and Nussbaumer, Martin and Tonkin, Emma},
file = {:Users/baharehzarei/Desktop/paper1/MG/AI/ (7).pdf:pdf},
keywords = {component-based web engineering,composition,evolution,federation,reuse,web service},
number = {Iwwost},
title = {{WebComposition Service Linking System : Supporting development , federation and evolution of service-oriented Web applications}},
year = {2003}
}
@article{Lang2011,
author = {Lang, Jens},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/energie-aes-ni-clt-2014.pdf:pdf},
title = {{Gr{\"{u}}ner verschl{\"{u}}sseln – Messung des Energieverbrauchs von Verschl{\"{u}}sselungsalgorithmen}},
year = {2011}
}
@article{Heterogeneous2015,
author = {Heterogeneous, Exploiting and Resources, Compute},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/DHR{\_}nesus15.pdf:pdf},
keywords = {heterogeneous clusters,numerical simulations,scheduling},
number = {1},
title = {{for Optimizing Lightweight Structures}},
volume = {I},
year = {2015}
}
@article{Zhu2006,
author = {Zhu, Ming and Yang, Laurence T. and Touri{\~{n}}o, Juan and Pan, Lei and Brent, Richard P. and Dongarra, Jack and Gustafson, John and Joubert, Gerhard and Pan, Yi and Zhang, Xiaodong and Abawajy, Jemal H. and Aubanel, Eric and Bahi, Jacques and Banicescu, Ioana and Bhalla, Subhash and Bic, Lubomir F. and Biswas, Rupak and Bourgeois, Anu and Buecker, Martin and Cabaleiro, Jos{\'{e}} Carlos and Cai, Xing and Carretero, Jesus and Chen, Jinjun and Dai, Yuanshun and {De Mello}, Rodrigo and Dillencourt, Michael B. and {Di Martino}, Beniamino and Doallo, Ramon and Doncescu, Andrei and Gravvanis, George A. and Huang, Chun Hsi and Ierotheou, Constantinos and Jie, Wei and Karatza, Helen and Koziris, Nectarios and Lei, Zhou and Leng, Tau and Li, Yiming and Martin, Maria J. and Michielse, Peter H. and Narravula, Harsha and Ng, Michael K. and Ni, Jun and O'Donnell, John and Quintana-Orti, Enrique and Rauber, Thomas and Runger, Gudula and Salem, Fatima Abu and Sarker, Biplab Kumer and Sedukhin, Stanislav G. and Shi, Hongchi and Skjellum, Tony and Strazdins, Peter and Thulasiram, Ruppa K. and Tian, Xinmin and Tomko, Karen and {Van Engelen}, Robert and Verdoscia, Lorenzo and Wu, Jiesheng and Xiao, Bin and Xu, Chengzhong and Xue, Jingling and Yang, Xiaoguang and Zheng, Yao and Zhou, Bingbing and Zhou, Xiaobo and Zlatev, Zahari and Cari{\~{n}}o, Ricolindo and Couturier, Raphael and Guo, Zhuang and Wang, Yang and Ding, Meng and Zekri, Ahmed},
doi = {10.1109/ICPPW.2006.46},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/HPSEC message.pdf:pdf},
isbn = {0769526373},
issn = {15302016},
journal = {Proceedings of the International Conference on Parallel Processing Workshops},
number = {January},
title = {{Message from HPSEC workshop co-chairs}},
year = {2006}
}
@article{Balg2013,
abstract = {For many applications of scientific computing, reduction operations may cause a performance bottleneck. In this article, the performance of different coarse- and fine-grained methods for implementing the reduction is investigated. Fine-grained reductions using atomic operations or fine-grained explicit locks are compared to the coarse-grained reduction operations supplied by OpenMP and MPI. The reduction operations investigated are used for an adaptive FEM. The performance results show that applications can gain a speedup by using fine-grained reduction since this implementation enables to hide the reduction between calculation while minimising the time waiting for synchronisation. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
author = {Balg, Martina and Lang, Jens and Meyer, Arnd and R{\"{u}}nger, Gudula},
doi = {10.1007/978-3-642-35893-7_3},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/BLMR{\_}mc12.pdf:pdf},
isbn = {9783642358920},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {1},
pages = {25--36},
title = {{Array-based reduction operations for a parallel adaptive FEM}},
volume = {7686 LNCS},
year = {2013}
}
@article{Dachsel2012,
abstract = {The Fast Multipole Method (FMM) is an efficient, widely used method for the solution of N-body problems. One of the main data structures is a hierarchical tree data structure describing the separation into near-field and far-field particle interactions. This article presents a method for automatic tuning of the FMM by selecting the optimal FMM tree depth based on an integrated performance prediction of the FMM computations. The prediction method exploits benchmarking of significant parts of the FMM implementation to adapt the tuning to the specific hardware system being used. Furthermore, a separate analysis phase at runtime is used to predict the computational load caused by the specific particle system to be computed. The tuning method was integrated into an FMM implementation. Performance results show that a reliable determination of the tree depth is achieved, thus leading to minimal execution times of the FMM algorithm. {\textcopyright} 2012 IEEE.},
author = {Dachsel, Holger and Hofmann, Michael and Lang, Jens and R{\"{u}}nger, Gudula},
doi = {10.1109/HPCC.2012.88},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/DHLR{\_}hpcc12.pdf:pdf},
isbn = {9780769547497},
journal = {Proceedings of the 14th IEEE International Conference on High Performance Computing and Communications, HPCC-2012 - 9th IEEE International Conference on Embedded Software and Systems, ICESS-2012},
pages = {617--624},
title = {{Automatic tuning of the fast multipole method based on integrated performance prediction}},
year = {2012}
}
@article{Hofmann2013,
abstract = {The article presents two efficient in-place algorithms for the symmetric all-to-all exchange of the MPI-Alltoallv operation. The first algorithm performs a series of pairwise data exchanges similar to the existing algorithm used by MPICH, but with fewer consecutive communication steps and idle processes. The second algorithm uses hierarchical sets of processes that lead to a better locality of communication. Exploiting additionally available memory for performance improvements is described. Performance results for an InfiniBand cluster and an IBM Blue Gene/Q system demonstrate the performance benefits of the algorithms within a generic benchmark program and an FFT application. Copyright 2013 ACM.},
author = {Hofmann, Michael and R{\"{u}}nger, Gudula},
doi = {10.1145/2488551.2488568},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/HR{\_}eurompi13.pdf:pdf},
isbn = {9788461651337},
journal = {ACM International Conference Proceeding Series},
keywords = {Collective communication,In-place,Limited memory,MPI,Symmetric all-to-all},
pages = {105--110},
title = {{In-place algorithms for the symmetric all-to-all exchange with MPI}},
year = {2013}
}
@article{Hannusch2015,
author = {Hannusch, S and Herzog, S and Hofmann, M and Ihlemann, J{\"{o}}rn and Kroll, Lothar and Meyer, A and Ospald, F and R{\"{u}}nger, G and Springer, R and Stockmann, M and Ulke-Winter, Lars},
file = {:Users/baharehzarei/Desktop/paper1/MG/Practical CS/HHH+{\_}imtc15.pdf:pdf},
journal = {Proceedings of the 2nd International MERGE Technologies Conference for Lightweight Structures (IMTC 2015)},
keywords = {bragg grating sensors,distributed computing,fiber,injection,lightweight structures,molding,numerical simulations,optimization,residual stresses},
title = {{Efficient Simulation, Optimization, and Validation of Lightweight Structures}},
year = {2015}
}
@article{,
file = {:Users/baharehzarei/Desktop/Papers/module-9-ontologies.pdf:pdf},
pages = {1995--2011},
title = {{Module 9: Ontologies and Semantic Annotation {\textcopyright}}},
year = {2011}
}
@article{Cimiano2005,
abstract = {In this paper we present Text2Onto, a framework for ontology learning from textual resources. Three main features distinguish Text2Onto from our earlier framework TextToOnto as well as other state-of-the-art ontology learning frameworks. First, by representing the learned knowledge at a meta-level in the form of instantiated modeling primitives within a so called Probabilistic Ontology Model (POM), we remain independent of a concrete target language while being able to translate the instantiated primitives into any (reasonably expressive) knowledge representation formalism. Second, user interaction is a core aspect of Text2Onto and the fact that the system calculates a confidence for each learned object allows to design sophisticated visualizations of the POM. Third, by incorporating strategies for data-driven change discovery, we avoid processing the whole corpus from scratch each time it changes, only selectively updating the POM according to the corpus changes instead. Besides increasing efficiency in this way, it also allows a user to trace the evolution of the ontology with respect to the changes in the underlying corpus. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Cimiano, Philipp and V{\"{o}}lker, Johanna},
doi = {10.1007/11428817_21},
file = {:Users/baharehzarei/Desktop/Papers/LNCS 3513 - Natural Language Processing and Information Systems.pdf:pdf},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {227--238},
title = {{Text2Onto A framework for ontology learning and data-driven change discovery}},
volume = {3513},
year = {2005}
}
@article{Benslimane2008,
abstract = {The New Generation of Web Applications T he Internet and related tech­ nologies have created an inter­ connected world in which we can exchange information easily, process tasks collaboratively, and form commu­ nities among users with similar inter­ ests to achieve efficiency and improve performance. Web services are emerg­ ing as a major technology for deploying automated interactions between distrib­ uted and heterogeneous applications, and for connecting business processes, which might span companies' bound­ aries. 1 Various standards support this deployment, including, for enterpris­ es, the Web Services Description Lan­ guage (WSDL), UDDI, and SOAP. These standards support the definition of Web services and their advertisement to the potential user community, binding for invocation purposes, and reuse. At the same time, use of " lighter­weight " ap­ proaches to services, especially for Web applications, is increasing. Here, the Web APIs and RESTful (Representa­ tional State Transfer) reign supreme.},
author = {Benslimane, Djamal and Dustdar, Schahram and Sheth, Amit},
file = {:Users/baharehzarei/Desktop/Papers/04620089.pdf:pdf},
issn = {10897801},
journal = {IEEE Internet Computing},
number = {Sept/Oct},
pages = {13--15},
title = {{Services Mashups}},
url = {http://www.infosys.tuwien.ac.at/staff/sd/papers/ServicesMashups{\_}IC.pdf},
volume = {12},
year = {2008}
}
@article{Al-Zubaide2011,
abstract = {A new ontology based approach is proposed to model and operate chatbots (OntBot). OntBot uses appropriate mapping technique to transform ontologies and knowledge into relational database and then use that knowledge to drive its chats. The proposed approach overcomes a number of traditional chatbots drawbacks including: the need to learn and use chatbot specific language such as AIML, high botmaster interference, and the use of non-matured technology. OntBot has the additional power of easy users interactions using their natural language, and the seamless support of different application domains. This gives the proposed approach a number of unique scalability and interoperability properties that are going to be evaluated in future phases of this research project. {\textcopyright} 2011 IEEE.},
author = {Al-Zubaide, Hadeel and Issa, Ayman A.},
doi = {10.1109/ISIICT.2011.6149594},
file = {:Users/baharehzarei/Desktop/paper1/chatbot.pdf:pdf},
isbn = {9781612846750},
journal = {2011 4th International Symposium on Innovation in Information and Communication Technology, ISIICT'2011},
pages = {7--12},
publisher = {IEEE},
title = {{OntBot: Ontology based ChatBot}},
year = {2011}
}
@article{Kim2016,
author = {Kim, Sang Il},
file = {:Users/baharehzarei/Desktop/paper1/Ontology-based Open API Composition Method For.pdf:pdf},
isbn = {9781509017249},
keywords = {barrier that thwarts various,composition,in addition,in this paper,mash-up,open,open api,passively receiving the services,services,this forms a,user demands for personalized,we propose an ontology-based,web service},
pages = {351--356},
title = {{Ontology-based Open API Composition Method For Automatic Mash-up Service Generation}},
year = {2016}
}
@article{Bohn2010,
author = {Bohn, Angela and Frey, Mathias},
file = {:Users/baharehzarei/Desktop/paper1/Unit{\_}4-Information{\_}Extraction.pdf:pdf},
title = {{Extracting Information from Text}},
year = {2010}
}
@article{Toledo-Alvarado2012,
abstract = {In this paper we show a procedure to build automatically an ontology from a corpus of text documents without external help such as dictionaries or thesauri. The method proposed finds relevant concepts in the form of multi-words in the corpus and non-hierarchical relations between them in an unsupervised manner.},
author = {Toledo-Alvarado, J. I. and Guzm{\'{a}}n-Arenas, A. and Mart{\'{i}}nez-Luna, G. L.},
doi = {10.22201/icat.16656423.2012.10.3.395},
file = {:Users/baharehzarei/Desktop/Papers/v10n3a9 (1).pdf:pdf},
issn = {16656423},
journal = {Journal of Applied Research and Technology},
keywords = {Apriori algorithm,Data Mining,Machine Learning,Ontology learning},
number = {3},
pages = {398--404},
title = {{Automatic building of an ontology from a corpus of text documents using data mining tools}},
volume = {10},
year = {2012}
}
@article{Issues2020,
author = {Issues, Security},
file = {:Users/baharehzarei/Downloads/Papers/DRF-Dissertation-Noura.pdf:pdf},
pages = {6--7},
title = {{Contents 推荐序一 推荐序二 前言 第 1 章 面试的流程}},
year = {2020}
}
@article{Asim2018,
abstract = {Ontologies have gained a lot of popularity and recognition in the semantic web because of their extensive use in Internet-based applications. Ontologies are often considered a fine source of semantics and interoperability in all artificially smart systems. Exponential increase in unstructured data on the web has made automated acquisition of ontology from unstructured text a most prominent research area. Several methodologies exploiting numerous techniques of various fields (machine learning, text mining, knowledge representation and reasoning, information retrieval and natural language processing) are being proposed to bring some level of automation in the process of ontology acquisition from unstructured text. This paper describes the process of ontology learning and further classification of ontology learning techniques into three classes (linguistics, statistical and logical) and discusses many algorithms under each category. This paper also explores ontology evaluation techniques by highlighting their pros and cons. Moreover, it describes the scope and use of ontology learning in several industries. Finally, the paper discusses challenges of ontology learning along with their corresponding future directions.},
author = {Asim, Muhammad Nabeel and Wasim, Muhammad and Khan, Muhammad Usman Ghani and Mahmood, Waqar and Abbasi, Hafiza Mahnoor},
doi = {10.1093/database/bay101},
file = {:Users/baharehzarei/Downloads/Papers/ontology leanring.pdf:pdf},
issn = {17580463},
journal = {Database},
number = {2018},
pages = {1--24},
pmid = {30295720},
title = {{A survey of ontology learning techniques and applications}},
volume = {2018},
year = {2018}
}
@article{Franch2014,
author = {Franch, Xavier and Ghose, Aditya K. and Lewis, Grace A. and Bhiri, Sami},
doi = {10.1007/978-3-662-45391-9},
file = {:Users/baharehzarei/Downloads/Papers/ADynamicServiceCompositionModelforAdaptiveSystemsinMobileComputingEnvironments.pdf:pdf},
isbn = {9783662453902},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {November 2014},
pages = {1},
title = {{Service-oriented computing}},
volume = {8831},
year = {2014}
}
@article{Mellal2021,
abstract = {The automatic ontology enrichment consists of automatic knowledge extraction from texts related to a domain of discourse in the aim to enrich automatically an initial ontology of the same domain. However, the passage, from a plain text to an enriched ontology requires a number of steps. In this paper, we present a three steps ontology enrichment approach. In the first step, we apply natural language processing techniques to obtain tagged sentences. The second step allows us to reduce each extracted sentence to an SVO (Subject, Verb, and Object) sentence, supposed to preserve main information carried by the original sentence(s) from which it is extracted. Finally, in the third step, we proceed to enrich an initial ontology built manually by adding extracted terms in the generated SVO as new concepts or instances of concepts and new relations. To validate our approach, we have used “Phytotherapy" domain because of the availability of related texts on the WWW and also because its usefulness for pharmaceutical industry. The first results obtained, after experiments on a set of different texts, testify the performance of the proposed approach.},
author = {Mellal, Nassima and Guerram, Tahar and Bouhalassa, Faiza},
doi = {10.31449/inf.v45i1.2586},
file = {:Users/baharehzarei/Downloads/Papers/2586-7652-1-PB.pdf:pdf},
issn = {18543871},
journal = {Informatica (Slovenia)},
keywords = {AutomaticEnrichment,NLP,Ontology,Phytotherapy,SVO triplet,Texts,Wordnet},
number = {1},
pages = {81--91},
title = {{An approach for automatic ontology enrichment from Texts}},
volume = {45},
year = {2021}
}
@article{Bryant2010a,
abstract = {This paper projects that an important future direction in software engineering is domain-specific software engineering (DSE). From requirements specification to design, and then implementation, a tighter coupling between the description of a software system with its application domain has the potential to improve both the correctness and reliability of the software system, and also lead to greater opportunities for software automation. In this position paper, we explore the impact of this emerging paradigm on requirements specification, design modeling, and implementation, as well as challenge areas benefiting from the new paradigm. Copyright 2010 ACM.},
author = {Bryant, Barrett R. and Gray, Jeff and Mernik, Marjan},
doi = {10.1145/1882362.1882376},
file = {:Users/baharehzarei/Downloads/Papers/Domain-specific{\_}software{\_}engineering.pdf:pdf},
isbn = {9781450304276},
journal = {Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010},
keywords = {Domain-specific languages,Domain-specific modeling,Requirements specification},
number = {November},
pages = {65--68},
title = {{Domain-specific software engineering}},
year = {2010}
}
@article{Watrobski2020,
abstract = {Ontologies are a key element of the Semantic Web. They aim to capture basic knowledge by providing appropriate terms and formal relationships between them, so that they can be used in a machine-processable manner. Accordingly they enable automatic aggregation and practical use as well as unexpected reuse of distributed data sources. Ontologies may come from many different sources, pursuing different goals and quality criteria. However, performed manually ontology construction is a very complex and tedious task, thus many methods proposed offer automatic or semi-automatic way for ontology construction. Many of the methods have their own, specific features. Therefore, this paper proposes an extensive knowledge-based approach covering the domain of ontology learning methods from text. This work aims to collect the knowledge of available approaches for ontology learning and the prominent differences between them, drawing on best practices in ontology engineering. The proposed approach refers to methods and aims to enrich knowledge in the field of ontology learning (OL). In this paper, the author's ontology contains a set of various types of methods with main techniques used, and the necessary features in the miscellaneous approaches. The proposed an extensive knowledge-based approach uses a reasoning mechanism based on competency questions for individual approaches to determine their ontology learning method profiles. The validation stage has also been carried out. At the same time, it is an extension of the previous study in the form of a repository of knowledge about OL tools. In addition, the combination of both ontologies: tools and methods aim to provide a more efficient OL solution from text.},
author = {Watr{\'{o}}bski, Jaroslaw},
doi = {10.1016/j.procs.2020.09.061},
file = {:Users/baharehzarei/Downloads/Papers/1-s2.0-S1877050920319566-main.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Domain ontology learning,Methods for ontology leaninf from text,Ontology integration,Ontology learning from text},
pages = {3356--3368},
publisher = {Elsevier B.V.},
title = {{Ontology learning methods from text -an extensive knowledge-based approach}},
url = {https://doi.org/10.1016/j.procs.2020.09.061},
volume = {176},
year = {2020}
}
@book{Paterno2017b,
abstract = {This book provides an in-depth insight into the emerging paradigm of End-User Development (EUD), discussing the diversity and potential for creating effective environments for end users. Containing a unique set of contributions from a number of international researchers and institutes, many relevant issues are discussed and solutions proposed, making important aspects of end-user development research available to a broader audience. Most people are familiar with the basic functionality and interfaces of computers. However, developing new or modified applications that can effectively support users' goals still requires considerable programming expertise that cannot be expected of everyone. One of the fundamental challenges that lie ahead is the development of environments that enable users with little or no programming experience to develop and modify their own applications. The ultimate goal is to empower people to flexibly employ and personalise advanced information and communication technologies.},
author = {Patern{\`{o}}, Fabio and Wulf, Volker},
booktitle = {New Perspectives in End-User Development},
doi = {10.1007/978-3-319-60291-2},
file = {:Users/baharehzarei/Downloads/Papers/978-3-319-60291-2.pdf:pdf},
isbn = {9783319602912},
pages = {1--459},
title = {{New perspectives in end-user development}},
year = {2017}
}
@article{Kalibatiene2010,
abstract = {While there is a great interest in rule-based systems and their development, none of the proposed languages or methods has been accepted as a standard technology yet. Nowadays tools used in process of information systems (IS) development are not extended and adapted enough for modelling and implementation of application domain rules. A particular contingent of researchers proposes using of ontology for development of intelligent IS, since ontology is suitable to represent application domain knowledge. We are challenged in using ontology for the development of application domain rules. In this paper we present a method for ontology axioms transformation to application domain rules and describe how ontology-based development of application domain rules is integrated through IS development life cycle.},
author = {Kalibatiene, Diana and Vasilecas, Olegas},
file = {:Users/baharehzarei/Downloads/Papers/756{\_}pp{\_}9-32.pdf:pdf},
journal = {Scientific Paper University of Latvia},
keywords = {OCL,PAL,application domain rule,axiom,ontology,transformation},
pages = {9--32},
title = {{Ontology-based application for domain rules development}},
url = {https://dspace.lu.lv/dspace/bitstream/handle/7/2248/LUR-756{\_}Datorika.pdf?sequence=1{\&}isAllowed=y{\#}page=9},
volume = {756},
year = {2010}
}
@article{Koren2018,
abstract = {New Internet-enabled devices and Web services are introduced on a daily basis. Documentation formats are available that describe their functionalities in terms of API endpoints and parameters. In particular, the OpenAPI specification has gained considerable influence over the last years. Web-based solutions exist that generate interactive OpenAPI documentation with HTML5 JavaScript. They allow developers to quickly get an understanding what the services and devices do and how they work. However, the generated user interfaces are far from real-world practices of designers and end users. We present an approach to overcome this gap, by using a model-driven methodology resulting in state-of-the-art responsive Web user interfaces. To this end, we use the Interaction Flow Modeling Language (IFML) as intermediary model specification to bring together APIs and frontends. Our implementation is based on open standards like Web Components and SVG. A screencast of our tool is available at https://youtu.be/KFOPmPShak4.},
annote = {use this paper},
author = {Koren, Istv{\'{a}}n and Klamma, Ralf},
doi = {10.1145/3184558.3188740},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Koren, Klamma - 2018 - The Exploitation of OpenAPI Documentation for the Generation of Web Frontends.pdf:pdf},
isbn = {9781450356404},
journal = {The Web Conference 2018 - Companion of the World Wide Web Conference, WWW 2018},
keywords = {ifml,interaction design,openapi,web components},
pages = {781--787},
title = {{The Exploitation of OpenAPI Documentation for the Generation of Web Frontends}},
year = {2018}
}
@article{Paterno2017a,
abstract = {This book provides an in-depth insight into the emerging paradigm of End-User Development (EUD), discussing the diversity and potential for creating effective environments for end users. Containing a unique set of contributions from a number of international researchers and institutes, many relevant issues are discussed and solutions proposed, making important aspects of end-user development research available to a broader audience. Most people are familiar with the basic functionality and interfaces of computers. However, developing new or modified applications that can effectively support users' goals still requires considerable programming expertise that cannot be expected of everyone. One of the fundamental challenges that lie ahead is the development of environments that enable users with little or no programming experience to develop and modify their own applications. The ultimate goal is to empower people to flexibly employ and personalise advanced information and communication technologies.},
author = {Patern{\`{o}}, Fabio and Wulf, Volker},
doi = {10.1007/978-3-319-60291-2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Patern{\`{o}}, Wulf - 2017 - New perspectives in end-user development.pdf:pdf},
isbn = {9783319602912},
journal = {New Perspectives in End-User Development},
number = {August},
pages = {1--459},
title = {{New perspectives in end-user development}},
year = {2017}
}
@article{Zafar2024,
abstract = {Rapid technological advancements have resulted in increasingly complex software systems, posing challenges during development in terms of time and cost. Adopting domain-specific modeling (DSM) brings numerous benefits to software engineering, including enhanced efficiency, improved maintenance capabilities, higher software quality, reduced development time, and increased potential for cost-effective software solutions through improved reusability. Despite the proven effectiveness of DSM in various domains, a study summarizing recent advancements is hard to find in the state-of-the-art. Therefore, in this article, we present a comprehensive systematic literature review that examines the application of DSM in various domains (4). We selected 99 studies and classified those into four categories, i.e., meta-modeling (42), domain-specific languages (39), UML profiles (9), and general (9) based on the use of DSM approaches. We identified various tools from the selected studies, i.e., 21 existing and 91 proposed or developed. Moreover, model-driven engineering (MDE) techniques, including validation (12), simulation (5), verification (4), and software architectural modeling (3), are presented and analyzed. We further explained the type of model transformation employed in each study, i.e., model-to-text (49) and model-to-model (4). Finally, the regions participating in DSM{\&}{\#}x2019;s growth are also investigated. It is concluded that Ecore is the leading meta-modeling tool, Xtext is the often-used domain-specific tool, Sirius is graphical, and UPPAAL is the most utilized verification tool identified. Moreover, Validation is a frequently used MDE technique, and Model-to-text transformation with Acceleo is the most utilized transformation type in the selected studies. The comprehensive results of this research provide valuable guidance for DSM researchers and practitioners in choosing a suitable tool and technique that meets their specific requirements.},
author = {Zafar, Amina and Azam, Farooque and Latif, Afshan and Anwar, Muhammad Waseem and Safdar, Aon},
doi = {10.1109/ACCESS.2024.3414503},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zafar et al. - 2024 - Exploring the Effectiveness and Trends of Domain-Specific Model Driven Engineering A Systematic Literature Review.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Domain-Specific Language,Domain-Specific Modeling,Model-Driven Engineering},
number = {May},
pages = {86809--86830},
publisher = {IEEE},
title = {{Exploring the Effectiveness and Trends of Domain-Specific Model Driven Engineering: A Systematic Literature Review (SLR)}},
volume = {12},
year = {2024}
}
@article{Dubielewicz2015a,
abstract = {The domain knowledge represented by ontology should be widely used in the design process of information system. The aim of the paper is to outline a systematic approach of developing a CIM model (domain model, precisely) on the basis of a selected domain ontology. There are presented some hints how ontology concepts can be expressed in domain model. Elaborated example realizes some difficulties in proposed approach, e.g. the domain knowledge is spread over many ontologies, some facts are defined at very general level (their interpretation is more difficult), ontology may contain many irrelevant elements. Nevertheless, we are believed that applying ontology in conscious way can help to achieve higher compliance of the domain model with the application domain.},
author = {Dubielewicz, Iwona and Hnatkowska, Bogumi{\l}a and Huzar, Zbigniew and Tuzinkiewicz, Lech},
doi = {10.1515/fcds-2015-0001},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Dubielewicz et al. - 2015 - Domain modeling in the context of ontology.pdf:pdf},
issn = {23003405},
journal = {Foundations of Computing and Decision Sciences},
keywords = {CIM,MDA,SUMO,UML,domain modelling,ontology},
number = {1},
pages = {3--15},
title = {{Domain modeling in the context of ontology}},
volume = {40},
year = {2015}
}
@book{Jacobson2017,
abstract = {In this article a method provides guidance for all the things you need to do when developing software. These things are technical, such as work with requirements, work with code and conduct testing, or people related, such as work setting up a well-collaborating team and an efficient project, as well as improving the capability of the people and collecting metrics. The interesting discovery we made in 2003 was that even if the number of methods in the world is huge it seemed that all these methods were just compositions of a much smaller collection of ‘mini-methods', maybe a few hundred of such ‘mini-methods' in total. These distinct ‘mini-methods' are what people in general call practices. In this paper the term method also stands for related terms such as process, methodology, method framework, even if these terms strictly speaking have a different meaning.},
author = {Jacobson, Ivar and Stimson, Roly},
booktitle = {The Essence of Software Engineering},
doi = {10.1007/978-3-319-73897-0_3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jacobson, Stimson - 2017 - Escaping Method Prison – On the Road to Real Software Engineering.pdf:pdf},
isbn = {9783319738963},
pages = {37--58},
title = {{Escaping Method Prison – On the Road to Real Software Engineering}},
year = {2017}
}
@article{Chhaya2020,
abstract = {Ontology-Driven Software Development has emerged as a significant mechanism in creating domain-specific languages (DSLs), allowing for expressing domain concepts effectively. Once an ontology is built, it is a simple process to generate the language's metamodel and establish relationships among related concepts. However, the creation of the ontology is often a time-consuming and laborious task requiring familiarity with the domain being modeled as well as a basic understanding of ontology engineering. This paper discusses the use of natural language scenarios to create a domain ontology, which can be applied towards generating a metamodel for a DSL. A case study for aviation scenarios is presented by using increasing numbers of scenarios and comparing the models at different stages to observe the evolution of the ontology. The scenario-driven ontology developed is compared to an existing aviation ontology to understand its value.},
author = {Chhaya, Bharvi and Jafer, Shafagh},
doi = {10.22360/SpringSim.2020.ANSS.003},
file = {:Users/baharehzarei/Downloads/Papers/3408207.3408216.pdf:pdf},
isbn = {9781565553705},
journal = {Proceedings of the 2020 Spring Simulation Conference, SpringSim 2020},
keywords = {domain-specific languages,ontologies,scenario-driven development,scenarios},
title = {{Scenario-Based Generation of Ontologies for Domain-Specific Languages}},
year = {2020}
}
@article{Tairas2008,
abstract = {The design stage of domain-specific language development, which includes domain analysis, has not received as much attention compared to the subsequent stage of language implementation. This paper investigates the use of ontology in domain analysis for the development of a domain-specific language. The standard process of ontology development is investigated as an aid to determine the pertinent information regarding the domain (e.g., the conceptualization of the domain and the common and variable elements of the domain) that should be modeled in a language for the domain. Our observations suggest that ontology assists in the initial phase of domain understanding and can be combined with further formal domain analysis methods during the development of a domain-specific language.},
author = {Tairas, Robert and Mernik, Marjan and Gray, Jeff},
file = {:Users/baharehzarei/Downloads/Papers/paper02.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Domain Analysis,Domain-Specific Languages,Ontology},
pages = {20--31},
title = {{Using ontologies in the domain analysis of domain-specific languages}},
volume = {395},
year = {2008}
}
@phdthesis{Diac2020,
abstract = {The automatic composition of web services refers to how services can be used in a complex and aggregate manner, to serve a specific and known functionality. Given a list of services described by the input and output parameters, and a request of a similar structure: the initially known and required parameters; a solution can be designed to automatically search for the set of web services that satisfy the request, under certain constraints. We first propose two very efficient algorithms that solve the problem of the automatic composition of the web services as it was formulated in the competitions organized in 2005 and 2008. The algorithms obtain much better results than the rest of the participants with respect to execution time and even composition size. Evaluation consists of running the previous and the proposed solutions on given benchmarks and generated tests. Further, we design two new models to match service's parameters, extending the semantic expressiveness of the 2008 challenge. The initial goal is to resolve some simple and practical use-cases that cannot be expressed in the previous models. We also adhere to modern service description languages, like OpenAPI and especially schema.org. Algorithms for the new models can solve instances of significant size. Addressing a wider and more realistic perspective, we define the online version of the composition problem. In this regard, we consider that web services and compositions requests can be added and removed in real-time, and the system must handle such operations on the fly. It is necessary to maintain the workflows for users who actively run the compositions over time. As for the new semantic models, we propose new algorithms and provide comprehensive evaluation by generating test cases that simulate all corner cases.},
archivePrefix = {arXiv},
arxivId = {2007.03896},
author = {Diac, Paul},
eprint = {2007.03896},
file = {:Users/baharehzarei/Downloads/Papers/2007.03896v2.pdf:pdf},
number = {April},
school = {Alexandru Ioan Cuza University of Iaşi},
title = {{Automatic Web Service Composition -- Models, Complexity and Applications}},
url = {http://arxiv.org/abs/2007.03896},
year = {2020}
}
@book{Medema2021,
abstract = {Automated Service Composition is one of the “grand challenges” in the area of Service-Oriented Computing. Mike Papazoglou was not only one of the first researchers who identified the importance of the problem, but was also one of the first proposers of formulating it as an AI planning problem. Unfortunately, classical planning algorithms were not sufficient and a number of extensions were needed, e.g., to support extended (rich) goal languages to capture the user intentions, to plan under uncertainty caused by the non-deterministic nature of services; issues that where formulated (and, partially addressed) by Mike, being one of his key contributions to the service community. In this chapter, we look at the development of the original vision of automated service composition as AI planning, going from planning with extended (rich) goals, further developing into composition under uncertainty, extending it to other domains (and reformulating the service composition as composition of sensors and actuators), and then showing possible alternative techniques for highly scalable service composition at the expense of the richness of the domain representation.},
author = {Medema, Michel and Kaldeli, Eirini and Lazovik, Alexander},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-73203-5_2},
file = {:Users/baharehzarei/Downloads/Papers/Medema2021{\_}Chapter{\_}AutomatedServiceCompositionUsi.pdf:pdf},
isbn = {9783030732035},
issn = {16113349},
keywords = {AI planning,Automated service composition,Constraint satisfaction,Internet of Things},
pages = {16--32},
publisher = {Springer International Publishing},
title = {{Automated Service Composition Using AI Planning and Beyond}},
url = {http://dx.doi.org/10.1007/978-3-030-73203-5{\_}2},
volume = {12521 LNCS},
year = {2021}
}
@article{Purohit2020,
abstract = {The dynamic composition of web services is an important research problem to offer value added services to the end user. As per the demands of the end user, the sequence in which services to be combined as well as participating services are to be decided at run-time. Planners based approach is useful to achieve the dynamic web service composition. Based on the functional parameters—input, output, precondition and effect, various AI planners achieve service composition differently. In this work, we present a AI planning-based dynamic web service composition approach using Blackbox planner. The experimental results show the effectiveness of the proposed approach.},
author = {Purohit, Lalit and Chouhan, Satyendra Singh and Jain, Aditi},
doi = {10.1007/978-981-15-2071-6_15},
file = {:Users/baharehzarei/Downloads/Papers/464595{\_}1{\_}En{\_}Print.indd.pdf:pdf},
isbn = {9789811520716},
issn = {23673389},
journal = {Lecture Notes in Networks and Systems},
keywords = {AI planning,Planning domain definition language,Web service,Web service composition},
pages = {183--195},
title = {{Dynamic Web Service Composition Using AI Planning Technique: Case Study on Blackbox Planner}},
volume = {100},
year = {2020}
}
@article{Czarnecki2003,
abstract = {The Model-Driven Architecture (MDA) Fra03 is an initiative by the Object Management Group (OMG) to define an approach to software development based on modeling and automated mapping of models to implementations. The basic MDA pattern involves defining a platformindependent model (PIM) and its automated mapping to one or more platform-specific models},
author = {Czarnecki, Krzysztof and Helsen, Simon},
file = {:Users/baharehzarei/Downloads/Papers/czarnecki{\_}helsen.pdf:pdf},
isbn = {0018-8670 VO - 45},
issn = {00188670},
journal = {Proceedings of the 2nd OOPSLA Workshop on Generative Techniques in the Context of the Model Driven Architecture},
number = {3},
pages = {1--17},
pmid = {15866344},
title = {{Classification of Model Transformation Approaches}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.6773{\&}rep=rep1{\&}type=pdf},
volume = {45},
year = {2003}
}
@book{Santini2019,
author = {Santini, Marina and J{\"{o}}nsson, Arne and Strandqvist, Wiktor and Cederblad, Gustav and Nystr{\"{o}}m, Mikael and Alirezaie, Marjan and Lind, Leili and Blomqvist, Eva},
file = {:Users/baharehzarei/Downloads/Papers/FULLTEXT02.pdf:pdf},
isbn = {9781522593454},
title = {{Designing an Extensible Domain- Specific Web Corpus for “ Layfication ”: A Case Study in eCare at Home Cyber-Physical Systems for Social Applications}},
year = {2019}
}
@article{Brajnik2004,
abstract = {This paper claims that effectiveness of automatic tools for evaluating web site accessibility has to be itself evaluated, given the increasingly important role that these tools play. The paper presents a comparison method for a pair of tools that takes into account correctness, completeness and specificity in supporting the task of assessing the conformance of a web site with respect to established guidelines. The paper presents data acquired during a case study based on comparing LIFT Machine with Bobby. The data acquired from the case study is used to assess the strengths and weaknesses of the comparison method. The conclusion is that even though there is room for improvement of the method, it is already capable of providing accurate and reliable conclusions.},
author = {Brajnik, Giorgio},
doi = {10.1007/s10209-004-0105-y},
file = {:Users/baharehzarei/Downloads/Papers/s10209-004-0105-y.pdf:pdf},
issn = {1615-5289},
journal = {Universal Access in the Information Society},
keywords = {assessments {\ae} automated evaluations,effectiveness,evaluation tools {\ae} tools,web accessibility {\ae} accessibility,{\ae} accessibility},
number = {3-4},
pages = {252--263},
title = {{Comparing accessibility evaluation tools: a method for tool effectiveness}},
volume = {3},
year = {2004}
}
@article{AlSedrani2016,
abstract = {Service composition is the process of constructing new services by combining several existing ones. It considered as one of the complex challenges in distributed and dynamic environment. The composition process includes, in general, the searching for existing services in a specific domain, and selecting the appropriate service, then coordinating composition flow and invoking services. Over the past years, the problem of web service composition has been studied intensively by researchers. Therefore, a significant amount of solutions and new methods to tackle this problem are presented. In this paper, our objective is to investigate algorithms and methodologies to provide a classification of existing methods in each composition phase. Moreover, we aim at conducting a comparative study to discover the main features and limitation in each phase in order to assist future research in this area.},
author = {AlSedrani, Aram and Touir, Ameur},
doi = {10.5121/ijwsc.2016.7101},
file = {:Users/baharehzarei/Downloads/Papers/7116ijwsc01.pdf:pdf},
issn = {22307702},
journal = {International Journal on Web Service Computing},
keywords = {composition planning,discovery method,execution method,selection method,service composition},
number = {1},
pages = {1--21},
title = {{Web Service Composition Processes: A Comparative Study}},
volume = {7},
year = {2016}
}
@article{Kotevski2011,
abstract = {The HTML5 Web Sockets specification defines the Web Sockets API that enables web pages to use the Web Socket protocol for full-duplex communication with a remote host. HTML5 Web Sockets defines a channel for full-duplex communication that operates through a single socket over the Web and represents a colossal advance, especially for real-time, event-driven web applications-significantly reduction in unnecessary network traffic and latency compared to legacy polling and long-polling solutions that are often used to push real-time data to clients. Through this paper should be present benefit from using Web socket in HTML5 and practical realization on several examples.},
author = {Kotevski, Aleksandar and Mikrovski, Gjorgi and Jolevski, Ilija},
file = {:Users/baharehzarei/Downloads/Papers/ICEST2011{\_}2{\_}23.pdf:pdf},
journal = {iCEST 2011},
keywords = {HTML5 Web Sockets,communication,protocol,traffic network},
pages = {353--356},
title = {{HTML5 Web Sockets}},
url = {http://domain.com},
year = {2011}
}
@article{Yu1994,
abstract = {In trying to understand and redesign software processes, it is often necessary to have an understanding of the `why,s' that underlie the `what's' -the motivations, intents, and rationales behind the activities and input-output flows. This paper presents a model which captures the intentional structure of a software process and its embedding organization, in terms of dependency relationships among actors. Actors depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. The model is embedded in the conceptual modelling language Telos. We outline some analytical tools to be developed for the model, and illustrate how the model can help in the systematic design of software processes. The examples used are adaptations of the ISPW-6/7 benchmark example.},
author = {Yu, Eric S K and Mylopoulos, John},
doi = {10.1109/icse.1994.296775},
file = {:Users/baharehzarei/Downloads/Papers/Understanding{\_}why{\_}in{\_}software{\_}process{\_}modelling{\_}analysis{\_}and{\_}design.pdf:pdf},
isbn = {081865855X},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {actor dependency,engineering,organization modelling,requirements,software process modelling},
pages = {159--168},
title = {{Understanding 'why' in software process modelling, analysis, and design}},
year = {1994}
}
@article{Xiao2011,
abstract = {Current service composition techniques and tools are mainly designed for use by Service-Oriented Architecture (SOA) professionals to solve business problems. Little attention has been paid to allowing end-users without sufficient service composition skills to compose services and integrate SOA solutions into their online experience to fulfill their daily activities. To shelter end-users from the complexity of service composition, we propose an approach which can compose services on the fly to meet the situational needs of end-users. We present a tag-based service description schema which allows non-IT professional users to easily understand the description of services and add their own descriptions using descriptive tags. Instead of requiring end-users to specify detailed steps for composition, the end-users only need to describe their goals using a few keywords. Our approach expands the meaning of a user's goal using ontologies then derives a group of keywords to discover services in order to fulfill the goal. A prototype is developed as a proof of concept to show that our approach enables end-users to discover and compose services easily. We conduct a case study to evaluate the effectiveness of our approach that eases end-users to compose services without the knowledge of SOA technologies. The results of our case study show that our approach can effectively generate ad-hoc processes and discover services with relatively high precision and recall. {\textcopyright} 2011 Springer-Verlag London Limited.},
author = {Xiao, Hua and Zou, Ying and Tang, Ran and Ng, Joanna and Nigul, Leho},
doi = {10.1007/s11761-011-0081-z},
file = {:Users/baharehzarei/Downloads/s11761-011-0081-z.pdf:pdf},
issn = {18632386},
journal = {Service Oriented Computing and Applications},
keywords = {Ontology,Service composition,Service discovery,Service-oriented architecture,Web services},
number = {3},
pages = {159--181},
title = {{Ontology-driven service composition for end-users}},
volume = {5},
year = {2011}
}
@article{Dorodnykh2021,
abstract = {The complexity of creating artificial intelligence applications remains high. One of the factors that cause such complexity is the high qualification requirements for developers in the field of programming. Development complexity can be reduced by using methods and tools based on a paradigm known as End-user development. One of the problems that requires the application of the methods of this paradigm is the development of intelligent systems for supporting the search and troubleshooting onboard aircraft. Some tasks connected with this problem are identified, including the task of dynamic formation of task cards for troubleshooting in terms of forming a list of operations. This paper presents a solution to this problem based on some principles of End-user development: model-driven development, visual programming, and wizard form-filling. In particular, an extension of the Prototyping expert systems based on transformations technology, which implements the End-user development, is proposed in the context of the problem to be solved for Sukhoi Superjet aircraft. The main contribution of the work is as follows: expanded the main technology method by supporting event trees formalism (as a popular expert method for formalizing scenarios for the development of problem situations and their localization); created a domain-specific tool (namely, Extended event tree editor) for building standard and extended event trees, including for diagnostic tasks; developed a module for supporting transformations of XML-like event tree representation format for the knowledge base prototyping system - Personal knowledge base designer. A description of the proposed extension and the means of its implementation, as well as an illustrative example, are provided.},
author = {Dorodnykh, N. O. and Kotlov, Y. V. and Nikolaychuk, O. A. and Popov, V. M. and Yurin, A. Y.},
doi = {10.47350/iccs-de.2021.05},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Dorodnykh et al. - 2021 - End-user development of knowledge bases for semi-automated formation of task cards.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {60--73},
title = {{End-user development of knowledge bases for semi-automated formation of task cards}},
volume = {2913},
year = {2021}
}
@article{Debnath2022,
abstract = {Trigger-Action-Programming (TAP) is a most widely used End User Development (EUD) tool for Internet of Things (IoT). However, end users often cannot differentiate between distinct kinds of triggers and actions. They also make erroneous combinations of those. Consequently, inconsistencies, and bugs are exhibited in behavior of IoT objects. To resolve this issue, end users need to be guided to interpret different triggers, actions and their combinations effectively. In this case, precise representation of temporal and contextual aspects of triggers and actions can assist. Moreover, vast and growing numbers of IoT objects as well as increasing numbers customized rules create scalability issues. To address these drawbacks, this paper has proposed an upper level ontology named as Trigger Action Ontology (TAO) that provides meta rule semantics for TAP. The contribution of proposed ontology specification is to present formal semantics of temporal and contextual aspects of triggers and actions. Further, the ontology is implemented in Prot{\'{e}}g{\'{e}}. In addition, the expressiveness of the proposed ontology is illustrated using a suitable case study.},
author = {Debnath, Narayan C. and Banerjee, Shreya and Van, Giau Ung and Quang, Phat Tat and Thanh, Dai Nguyen},
doi = {10.29007/wmfn},
file = {:Users/baharehzarei/Downloads/Papers/An{\_}Ontology{\_}Based{\_}Approach{\_}towards{\_}End{\_}User{\_}Development{\_}of{\_}IoT.pdf:pdf},
issn = {23987340},
journal = {EPiC Series in Computing},
keywords = {End User Development,Internet of Things,Ontology,Trigger Action Programming},
pages = {1--10},
title = {{An ontology based approach towards end user development of iot}},
volume = {82},
year = {2022}
}
@article{Kawamoto2006,
abstract = {A Semantic Wtki is a collaborative Semantic Web authoring system based on the Wiki framework. It provides a scheme where anonymous users on the Internet can collaborate with each other to build a Semantic Web site. Contrasting to traditional Wikis, it is not easy for end users to author Semantic Wiki pages from scratch without knowledge of the complex RDF/OWL syntax and of ontologies to share semantic information. We propose a new Semantic Wiki system called "KawaWiki" on which end and expert users can collaborate to build a Semantic Web site. KawaWiki generates RDF and Wiki pages based on its RDF templates and validates their consistency with the RDFS description. The RDFS description can be created by importing Web ontologies on the Internet. KawaWiki aims at providing a scheme where end users, expert users and ontologists seamlessly collaborate to publish semantically consistent RDF and Wiki pages in a Wiki way. {\textcopyright} 2006 IEEE.},
author = {Kawamoto, Kensaku and Kitamura, Yasuhiko and Tijerino, Yuri},
doi = {10.1109/WI-IATW.2006.85},
file = {:Users/baharehzarei/Downloads/KawaWiki{\_}A{\_}Semantic{\_}Wiki{\_}Based{\_}on{\_}RDF{\_}Templates.pdf:pdf},
isbn = {0769527493},
journal = {Proceedings - 2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2006 Workshops Proceedings)},
pages = {425--429},
title = {{KawaWiki: A Semantic Wiki based on RDF templates}},
year = {2006}
}
@article{Yunianta2019,
abstract = {The implementations of data integration in current days have many issues to be solved. Heterogeneity of data with non-standardization data, data conflicts between various data sources, data with a different representation, as well as semantic aspects problems are among the challenges and still open to research. Semantic data integration using ontology approach is considered as an appropriate solution to deal with semantic aspects problem in data integration. However, most methodologies for ontology development are developed to cover specific purpose and less suitable for common data integration implementation. This research offers an improved methodology for ontology development on data integration to deal with semantic aspects problem, called OntoDI. It is a continuation and improvement of the previous work about ontology development methods on agent system. OntoDI consists of three main parts, namely the pre-development, core-development and postdevelopment, in which every part contains several phases. This paper describes the experiment of OntoDI in the electronic learning system domain. Using OntoDI, the development of ontology knowledge gives simpler phases, complete steps, and clear documentation for the ontology client. In addition, this ontology knowledge is also capable to overcome semantic aspect issues that happen in the sharing and integration process in education area.},
author = {Yunianta, Arda and Basori, Ahmad Hoirul and Prabuwono, Anton Satria and Bramantoro, Arif and Syamsuddin, Irfan and Yusof, Norazah and Almagrabi, Alaa Omran and Alsubhi, Khalid},
doi = {10.14569/IJACSA.2019.0100121},
file = {:Users/baharehzarei/Downloads/OntoDI{\_}The{\_}Methodology{\_}for{\_}Ontology{\_}Development{\_}on.pdf:pdf},
issn = {21565570},
journal = {International Journal of Advanced Computer Science and Applications},
keywords = {Data integration,Methodology,Ontology development,Semantic approach,Semantic issues},
number = {1},
pages = {160--168},
title = {{OntoDI: The methodology for ontology development on data integration}},
volume = {10},
year = {2019}
}
@article{Hamrouni2021,
abstract = {Background: Case-Based Reasoning (CBR) is a problem-solving paradigm that uses knowledge of relevant past experiences (cases) to interpret or solve new problems. CBR systems allow generating explanations easily, as they typically organize and represent knowledge in a way that makes it possible to reason about and thereby generate explanations. An improvement of this paradigm is ontology-based CBR, an approach that combines, in the form of formal ontologies, case-specific knowledge with domain one in order to improve the effectiveness and explanation capability of the system. Intelligent systems make daily activities more easily, efficiently, and represent a real support for sustainable economic development. On the one hand, they improve efficiency, productivity, and quality, and, on the other hand, can reduce costs and cut waste. In this way, intelligent systems facilitate sustainable development, economic growth, societal progress, and improve efficiency. Aim: In this vision, the purpose of this paper is to propose a new generation of intelligent decision support systems for Business Model having the ability to provide explanations to increase confidence in proposed solutions. Findings/result: The performance results obtained show the benefits of the proposed solution with different requirements of an explanatory decision support system. Consequently, applying this paradigm for software tools of business model development will make a great promise for supporting business model design, sustainability, and innovation.},
author = {Hamrouni, Basma and Bourouis, Abdelhabib and Korichi, Ahmed and Brahmi, Mohsen},
doi = {10.3390/su13179819},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hamrouni et al. - 2021 - Explainable ontology-based intelligent decision support system for business model design and sustainability.pdf:pdf},
issn = {20711050},
journal = {Sustainability (Switzerland)},
keywords = {Business model,Case-based reasoning,Decision support system,Explanation,Ontology,Sustainability},
number = {17},
pages = {1--28},
title = {{Explainable ontology-based intelligent decision support system for business model design and sustainability}},
volume = {13},
year = {2021}
}
@article{DeAzevedoJacyntho2021,
abstract = {A knowledge-based system is a kind of decision support system with specialized problem-solving expertise. The expertise consists of knowledge about a particular domain, understanding symptoms, problems, and solutions within that domain. These systems can essentially suggest or recommend actions. We can say that a person-machine team processes knowledge to accomplish a task. In this context, ontologies emerge as a powerful tool to formally representing such a domain of knowledge, allowing the machine to comprehend it and therefore assist us. This chapter presents the benefits of using ontologies in decision-making and how to achieve them. An actual problem-solving Web Ontology Language ontology, namely Issue-Procedure Ontology, and a corresponding fictitious, but with real-world challenges, health care case study are introduced.},
author = {{de Azevedo Jacyntho}, Mark Douglas and Morais, Matheus D.},
doi = {10.1016/B978-0-12-822468-7.00016-X},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/de Azevedo Jacyntho, Morais - 2021 - Ontology-based decision-making(2).pdf:pdf},
isbn = {9780128224687},
journal = {Web Semantics: Cutting Edge and Future Directions in Healthcare},
keywords = {Decision support,Decision-making,Ontology,Problem solving,Web ontology language (OWL)},
number = {1997},
pages = {195--209},
title = {{Ontology-based decision-making}},
year = {2021}
}
@article{Zouch2018,
abstract = {With the increased globalization of the economy, the competitiveness has become ubiquitous and enterprises need to be reactive and to collaborate with different stakeholders to survive in its environment. Within this context, the choice of suitable collaborators and partners with whom collaboration may happen without problems is a key factor of success. In this paper, an ontology-based approach to support interoperability and help enterprises to solve interoperability problems before they occur is proposed.},
author = {Zouch, Mahdi and Guedria, Wided and {Ben Halima}, Riadh},
doi = {10.1007/978-3-319-73805-5_4},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zouch, Guedria, Ben Halima - 2018 - Ontology-based decision support system for enterprise interoperability.pdf:pdf},
isbn = {9783319738048},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Decision support system,Enterprise interoperability,Interoperability problem,Knowledge,Ontology},
pages = {36--45},
title = {{Ontology-based decision support system for enterprise interoperability}},
volume = {10697 LNCS},
year = {2018}
}
@article{Spoladore2023,
abstract = {The competitiveness of nature-based Health Tourism (NHT) industry, especially in the Alpine regions, is increasingly linked to the sustainability and exploitation of unique natural resources of tourism destinations, which often lack the access to knowledge and networks of stakeholders to improve their offerings. In this sense, the use of digital tools can open up further opportunities to reconsider value offerings and better access different knowledge resources and relationships within the industry network. This Chapter illustrates the collaborative design approach adopted in HEALPS2 for the development of an ontology-based Decision Support System for health tourism destinations. The resulting ontology aims to model the relationships between the available natural resources, the value offerings and the target groups of NHT destinations. Moreover, the Collaborative Design approach foresees the involvement of end-users (i.e. not only tourism destinations, but also the network of stakeholders, and the actual and potential future tourists) as both sources of knowledge and validators of the ontology and its outputs, aiming to inform decision-making processes in a shared knowledge model that leverages on digital tools.},
author = {Spoladore, Daniele and Pessot, Elena},
doi = {10.1007/978-3-031-15457-7_4},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Spoladore, Pessot - 2023 - An Ontology-Based Decision Support System to Foster Innovation and Competitiveness Opportunities of Health To.pdf:pdf},
isbn = {9783031154577},
issn = {21915318},
journal = {SpringerBriefs in Applied Sciences and Technology},
keywords = {Alpine region,Collaborative development,Evidence-based health tourism,Health tourism,Ontology,Ontology-based decision support system},
pages = {61--71},
title = {{An Ontology-Based Decision Support System to Foster Innovation and Competitiveness Opportunities of Health Tourism Destinations}},
year = {2023}
}
@article{Sanctorum2022,
abstract = {Knowledge bases store information on certain topics. Applying a well-structured and machine-readable format for a knowledge base is a prerequisite for any AI-based processing or reasoning. Semantic technologies (e.g. RDF) offer such a format and have the advantages that they make it possible to define the semantics of the information and support advanced querying. However, the disadvantage is that using such technologies is challenging for people not trained in IT, such as subject matter experts. This means that they need to rely on semantic technology experts to create, maintain, and query their knowledge bases. However, these experts are, in turn, not trained in the subject matter, while domain knowledge is essential for the construction of high-quality knowledge bases. In this paper, we present an end-user engineering approach for ontology-based knowledge bases. The goal is to allow subject matter experts to develop, maintain, and exploit the knowledge base themselves. We also present the supporting tools developed so far. The tools for the construction and the manual filling of the knowledge base are using the jigsaw metaphor to hide technicalities and guide the users. We also developed tools to automatically import data from spreadsheets into the knowledge base and to perform some type of quality control on the data. The end-user approach and the tools are demonstrated and evaluated for building a knowledge base in the toxicology domain.},
author = {Sanctorum, Audrey and Riggio, Jonathan and Maushagen, Jan and Sepehri, Sara and Arnesdotter, Emma and Delagrange, Mona and {De Kock}, Joery and Vanhaecke, Tamara and Debruyne, Christophe and {De Troyer}, Olga},
doi = {10.1080/0144929X.2022.2092032},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sanctorum et al. - 2022 - End-user engineering of ontology-based knowledge bases.pdf:pdf},
issn = {13623001},
journal = {Behaviour and Information Technology},
keywords = {Knowledge base engineering,data lifting,domain ontology development,end-user development,jigsaw metaphor,quality assurance},
number = {9},
pages = {1811--1829},
title = {{End-user engineering of ontology-based knowledge bases}},
volume = {41},
year = {2022}
}
@book{Saha2021,
abstract = {Intelligent and smart health monitoring is prevalent nowadays with the support of advancement in Internet of Things, machine learning, and ontology-based decision support systems. As a decision support system can analyze current patient vitals based on historical data, effective data representation from different data sources into a common knowledge base is essential. Web semantics has an increasingly important role to play here in terms of storing data following ontology for more usable knowledge repository. The findings of the decision support system can be fed to doctor's smartphone as a message based on which the doctor may intervene in a specific scenario or may validate his own diagnosis with the one provided by the decision support system. As the comfort and convenience of the end-users of remote healthcare is important, in addition to quality of service, quality of experience is a matter of concern among other issues and challenges. This work emphasizes on several Machine Learning (ML) algorithms, ontology techniques to design and implement intelligent decision support system for effective healthcare support satisfying quality of service and quality of experience requirements.},
author = {Saha, Ramesh and Sen, Sayani and Saha, Jayita and Nandy, Asmita and Biswas, Suparna and Chowdhury, Chandreyee},
booktitle = {Web Semantics: Cutting Edge and Future Directions in Healthcare},
doi = {10.1016/B978-0-12-822468-7.00005-5},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Saha et al. - 2021 - Ontology-based intelligent decision support systems A systematic approach.pdf:pdf},
isbn = {9780128224687},
keywords = {Decision support system,Internet of things,Machine learning,Ontology,Web semantics},
pages = {177--193},
publisher = {INC},
title = {{Ontology-based intelligent decision support systems: A systematic approach}},
url = {http://dx.doi.org/10.1016/B978-0-12-822468-7.00005-5},
year = {2021}
}
@article{Vahidnia2023,
abstract = {In recent years, decision support systems (DSSs) have successfully deployed ontologies in their architecture. The result of such a use is information systems that assist users and organizations in semi-structured decision-making activities. Visitors from throughout Iran travel to different cities and regions every year, and they need help making their choices. Some of these tourists are unable to visit the beautiful areas of the destination city due to a lack of awareness. In this study, we design an ontology-based spatial DSS to find entertainment and tourism centers in Arak, Iran. The objective is to provide users with recommendations appropriate for the location, time, age group, type of activity, and other factors. In this model, the demands and concerns of tourists have been managed by creating a domain Web Ontology Language (OWL) for entertainment centers as a knowledge base in the Prot{\'{e}}g{\'{e}} environment. The developed web-based DSS operates on a client-server architecture using technologies such as Werkzeug and Flask. As a result, it makes it possible to ontology reasoning based on the HermiT engine to choose the right center and conduct a semantic search on classes related to the appropriate point of interest. The main distinction between the proposed methodology and the previous studies on spatial DSS is that criteria are object properties in an ontology. Therefore, decision support relies on real-time reasoning rather than transforming criteria into geospatial layers. The evaluation results confirmed efficient interaction with this system, purposeful information retrieval, and rapid decision-making process. The results also indicated that searching for a POI (point of interest) in the study area using the developed system is at least 30{\%} more successful than a search engine or social media. Moreover, to overcome the cold start problem, the proposed technique might be utilized in conjunction with the POI recommender systems. {\textcopyright} 2023 Wuhan University. Published by Informa UK Limited, trading as Taylor {\&} Francis Group.},
author = {Vahidnia, Mohammad H. and Minaei, Mojde and Behzadi, Saeed},
doi = {10.1080/10095020.2022.2161954},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Vahidnia, Minaei, Behzadi - 2023 - An ontology-based web decision support system to find entertainment points of interest in an urban ar.pdf:pdf},
issn = {1009-5020},
journal = {Geo-spatial Information Science},
keywords = {decision,dss,gis,interest,ontology,poi,point of,semantic web,tourism},
number = {00},
pages = {1--18},
publisher = {Taylor {\&} Francis},
title = {{An ontology-based web decision support system to find entertainment points of interest in an urban area}},
url = {https://doi.org/10.1080/10095020.2022.2161954},
volume = {00},
year = {2023}
}
@misc{,
title = {{IEEE Xplore Full-Text PDF:}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=4053285},
urldate = {2024-02-18}
}
@article{Jelodar2019,
abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
archivePrefix = {arXiv},
arxivId = {1711.04305},
author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
doi = {10.1007/s11042-018-6894-4},
eprint = {1711.04305},
file = {:Users/baharehzarei/Downloads/s11042-018-6894-4.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Gibbs sampling,Latent Dirichlet allocation,Semantic web,Tag recommendation,Topic modeling},
number = {11},
pages = {15169--15211},
publisher = {Multimedia Tools and Applications},
title = {{Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey}},
volume = {78},
year = {2019}
}
@article{Noran2018,
abstract = {Technical advances in Information and Communication Technology have enabled the collection and storage of large amounts of data, rising hopes of digitalising and thus potentially improving decision making and related support systems. Unfortunately however, the pre-existing gap between required decision making knowledge and the useful information provided by current technologies appears to increase rather than contract. Thus, the multitude of patterns presently provided by current data analytics techniques do not deliver an adequate set of scenarios to enable effective decision making by humans. This paper advocates a digital decision analytics solution featuring the use of Situated Logic to create ‘narratives' describing the meaning of data analytics results and the use of Channel Theory in order to support adequate situational awareness. This approach is explained in the context of a System-of-Systems paradigm highly relevant to today's typically complex clusters of distributed collaborative decision making centres and their associated decision support systems.},
author = {Noran, Ovidiu and Bernus, Peter},
file = {:Users/baharehzarei/Downloads/Improving Digital Decision Making Through Situational Awareness.pdf:pdf},
isbn = {9789177538769},
journal = {Proceedings of the 27th International Conference on Information Systems Development: Designing Digitalization, ISD 2018},
keywords = {Big data,Channel theory,Data warehousing,Decision model,Decision support systems,Digital decision making,Situated reasoning,Situational awareness},
title = {{Improving digital decision making through situational awareness}},
year = {2018}
}
@article{HenryLiebermanFabioPaterno2004,
author = {{Henry Lieberman Fabio Patern{\`{o}}}},
file = {:Users/baharehzarei/Downloads/L-G-0000004026-0002333322.pdf:pdf},
isbn = {1-4020-2966-7},
journal = {Human-Computer Interaction},
title = {{Human-Computer Interaction Series Volume 3}},
url = {http://www.springeronline.com},
year = {2004}
}
@article{Barricelli2019,
abstract = {End-User Development (EUD), End-Programming (EUP) and End-User Software Engineering (EUSE) are three related research fields that study methods and techniques for empowering end users to modify and create digital artifacts. This paper presents a systematic mapping study aimed at identifying and classifying scientific literature about EUD, EUP and EUSE in the time range January 2000–May 2017. We selected 165 papers found through a manual selection of papers from specific conferences, journal special issues, and books, integrated with an automatic search on the most important digital libraries. The answer to our research question was built through a classification of the selected papers on seven dimensions: type of approach, interaction technique, phase in which the approach is adopted, application domain, target use, class of users, and type of evaluation. Our findings suggest that EUD, EUP and EUSE are active research topics not only in Human–Computer Interaction, but also in other research communities. However, little cross-fertilization exists among the three themes, as well as unifying frameworks and approaches for guiding novice designers and practitioners. Other findings highlight trends and gaps related to the analysis' dimensions, which have implications on the design of future tools and suggest open issues for further investigations.},
author = {Barricelli, Barbara Rita and Cassano, Fabio and Fogli, Daniela and Piccinno, Antonio},
doi = {10.1016/j.jss.2018.11.041},
file = {:Users/baharehzarei/Desktop/paper1/1-s2.0-S0164121218302577-main.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {End-user development,End-user programming,End-user software engineering,Systematic mapping study},
pages = {101--137},
publisher = {Elsevier Inc.},
title = {{End-user development, end-user programming and end-user software engineering: A systematic mapping study}},
url = {https://doi.org/10.1016/j.jss.2018.11.041},
volume = {149},
year = {2019}
}
@article{Blili1998,
abstract = {When compared to organizational computing, end-user computing (EUC) has the tendency to be closely related to individual user tasks and motivations. This article presents the results of an empirical study designed to examine the impact of task uncertainty and end-user involvement on the use and success of EUC. A research model was built and tested with a survey of 505 end-users in five large organizations. Structural equation modeling results confirm that both EUC competence and success are determined by task complexity and the perceived importance users attribute to EUC. {\textcopyright} 1998 Elsevier Science B.V.},
author = {Blili, S. and Raymond, L. and Rivard, S.},
doi = {10.1016/S0378-7206(97)00043-8},
file = {:Users/baharehzarei/Downloads/1-s2.0-S0378720697000438-main.pdf:pdf},
issn = {03787206},
journal = {Information and Management},
keywords = {End-user computing (EUC),End-user computing impacts,End-user computing success,Task uncertainty,User competence,User involvement,User satisfaction},
number = {3},
pages = {137--153},
title = {{Impact of task uncertainty, end-user involvement, and competence on the success of end-user computing}},
volume = {33},
year = {1998}
}
@article{Semiawan2021,
abstract = {Software tools, frameworks, and libraries are relatively crucial in software development tools. However, these tools cannot guarantee that a developer will produce good quality software applications. This study aims to investigate the efficiency and effectiveness of software tools, specifically frameworks, that support developers in producing good quality applications. Using an experimental approach in software development, the developer participants were given a case in which software needed to be developed with and without a framework. The experiment results were then evaluated to obtain facts from the participants' experiences during development, through some data sources. The results of the study show that the use of a framework does not significantly help developers effectively and efficiently develop software or applications. The success of the application that has been measured based on the level of completion, quality, and usability of the application does not show significant differences in value whether developed with or without tools. Furthermore, the study revealed that the software development skill that cannot be obtained through the use of framework was the general or detailed system conceptualization.},
author = {Semiawan, Transmissia and Alifi, Muhammad Riza and Hayati, Hashri and Lieharyani, Djoko Cahyo Utomo},
doi = {10.2991/aer.k.211106.006},
file = {:Users/baharehzarei/Downloads/125963738.pdf:pdf},
journal = {Proceedings of the 2nd International Seminar of Science and Applied Technology (ISSAT 2021)},
keywords = {developer skills,effectiveness of software tools,efficiency,software development tools},
number = {Issat},
pages = {32--39},
title = {{Analysis of the Effectiveness and Efficiency of Software Development Tools}},
volume = {207},
year = {2021}
}
@article{Cao2017,
abstract = {Context Mashup is emerging as a promising software development method for allowing software developers to compose existing Web APIs to create new or value-added composite Web services. However, the rapid growth in the number of available Mashup services makes it difficult for software developers to select a suitable Mashup service to satisfy their requirements. Even though clustering based Mashup discovery technique shows a promise of improving the quality of Mashup service discovery, Mashup service clustering with high accuracy and good efficiency is still a challenge problem. Objective This paper proposes a novel domain-aware Mashup service clustering method with high accuracy and good efficiency by exploiting LDA topic model built from multiple data sources, to improve the quality of Mashup service discovery. Method The proposed method firstly designs a domain-aware Mashup service feature selection and reduction process by refining characterization of their domains to consolidate domain relevance. Then, it presents an extended LDA topic model built from multiple data sources (include Mashup description text, Web APIs and tags) to infer topic probability distribution of Mashup services, which serves as a basis of Mashup service similarity computation. Finally, K-means and Agnes algorithm are used to perform Mashup service clustering in terms of their similarities. Results Compared with other existing Mashup service clustering methods, experimental results show that the proposed method achieves a significant improvement in terms of precision, recall, F-measure, purity and entropy. Conclusion The results of the proposed method help software developers to improve the quality of Mashup service discovery and Mashup-based software development. In the future, there will be a need to extend the method by considering heterogeneous network information among Mashup, Web APIs, tags, users, and applying it to Mashup discovery for software developers.},
author = {Cao, Buqing and {Frank Liu}, Xiaoqing and Liu, Jianxun and Tang, Mingdong},
doi = {10.1016/j.infsof.2017.05.001},
file = {:Users/baharehzarei/Downloads/1-s2.0-S0950584917303865-main.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Domain feature selection and reduction,LDA,Mashup service,Multiple data sources,Service clustering},
pages = {40--54},
publisher = {Elsevier B.V.},
title = {{Domain-aware Mashup service clustering based on LDA topic model from multiple data sources}},
url = {http://dx.doi.org/10.1016/j.infsof.2017.05.001},
volume = {90},
year = {2017}
}
@article{Dzida1998,
abstract = {Scenarios can help remedy the most serious obstacle in the design process that is a chronic lack of knowledge of the application domain. Moreover, scenarios can be employed in analysis and design to serve both illustrating the context of an envisaged usage (user's perspective) and demonstrating the design proposal in terms of the intended usage (analyst's perspective). In contrasting both perspectives by means of a dialectic process a synthesis can be achieved that incorporates a shared understanding. Validation is a process to achieve such an understanding. The semantic structure of types of scenarios is investigated thus illustrating how a context of use analysis according to ISO 9241-11 can be exploited for validation purposes. The role of scenarios in usability engineering is contrasted with traditional concepts of systems analysis as an attempt to narrow the bridge between software engineering and usability engineering.},
author = {Dzida, Wolfgang},
doi = {10.1109/32.738346},
file = {:Users/baharehzarei/Downloads/01 - Peperoni.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Requirements construction,Scenario types, dialectic process, work context, p,Validation},
number = {12},
pages = {1182--1196},
title = {{Making use of scenarios for validating analysis and design}},
volume = {24},
year = {1998}
}
@article{Pahlke2010a,
abstract = {Currently, several Enterprise 2.0 platforms are beginning to emerge. This new generation ofweb-based enterprise platforms significantly influences application development and use. Apart fromthe IT department, the end users participate in the development of business applications by composing their ownwork environments based on their continuously changing needs. This paper introduces EnterpriseMashup technology as ameans to improve IT alignment of individualwork processes and changing business needs. Furthermore, organizational key drivers, technical challenges and inhibitors are discussed to assess the potential business value and explain the emerging expansion ofMashup platforms in companies.},
author = {Pahlke, Immanuel and Beck, Roman and Wolf, Martin},
doi = {10.1007/s12599-010-0121-9},
file = {:Users/baharehzarei/Downloads/s12599-010-0121-9.pdf:pdf},
journal = {Business {\&} Information Systems Engineering},
number = {5},
pages = {305--315},
title = {{Enterprise Mashup Systems as Platform for Situational Applications}},
volume = {2},
year = {2010}
}
@article{Hagele2021,
abstract = {This paper addresses the problem of situation modeling and machine learning-based decision making in open and non-predictive environments. Situational decision making incorporates the determination of an action based on the current situation, represented by the situation model and trained system behavior using deep neural networks. Commonly, the situation modeling is not considered an intermediate step for decision making in situational action selection. This contribution introduces a novel approach for decision making using situation modeling and deep neural networks. It uses an information structuring and representation technique for the generation of situation spectra used as input to deep learning-based decision making. Simulation-based experimental results show the proposed approach's effectiveness and importance.},
author = {Hagele, Georg and Sarkheyli-Hagele, Arezoo},
doi = {10.1109/CogSIMA51574.2021.9475922},
file = {:Users/baharehzarei/Downloads/Situational{\_}Decision{\_}Making{\_}Using{\_}Situation{\_}Modeling{\_}and{\_}Deep{\_}Learning.pdf:pdf},
isbn = {9781728176987},
journal = {Proceedings - 2021 IEEE International Conference on Cognitive and Computational Aspects of Situation Management, CogSIMA 2021},
keywords = {Situation model,safety critical systems,situational risk assessment},
pages = {108--115},
publisher = {IEEE},
title = {{Situational Decision Making Using Situation Modeling and Deep Learning}},
year = {2021}
}
@article{Wei2020,
abstract = {The articles in this special section focus on situation awareness in intelligent human-computer interaction for critical decision making (HCI). HCI is recognized as an ctive field that focuses on the various interactions of human with machines. The HCI has been widely applied in multiple domains, such as artificial intelligence, computer vision, image and multimedia analysis, and cognitive and behavioral sciences. The objective of the HCI is to make the computer smart via receiving enough knowledge about the environment where it is deployed and reduce the human intervention aspect toward decision making. This enables development of high-end computers that are context aware and smart in making decisions with reference to the context. Situation awareness of an intelligent HCI will decide the success and application of the solution across the real world environment. The aim of this special issue is to provide a platform on the topic of situation awareness in intelligent HCI for time critical decision making.},
author = {Wei, Wei and Wu, Jinsong and Zhu, Chunsheng},
doi = {10.1109/MIS.2019.2956692},
file = {:Users/baharehzarei/Downloads/Special{\_}Issue{\_}on{\_}Situation{\_}Awareness{\_}in{\_}Intelligent{\_}Human-Computer{\_}Interaction{\_}for{\_}Time{\_}Critical{\_}Decision{\_}Making.pdf:pdf},
issn = {19411294},
journal = {IEEE Intelligent Systems},
number = {1},
pages = {3--5},
publisher = {IEEE},
title = {{Special Issue on Situation Awareness in Intelligent Human-Computer Interaction for Time Critical Decision Making}},
volume = {35},
year = {2020}
}
@article{Desolda2017a,
abstract = {Research on the Internet of Things (IoT) has devoted many efforts to technological aspects. Little social and practical benefits have emerged so far. IoT devices, so-called smart objects, are becoming even more pervasive and social, leading to the need to provide non-technical users with innovative interaction strategies for controlling their behavior. In other words, the opportunities offered by IoT can be amplified if new approaches are conceived to enable non-technical users to be directly involved in "composing" their smart objects by synchronizing their behavior. To fulfill this goal, this article introduces a model that includes new operators for defining rules combining multiple events and conditions exposed by smart objects, and for defining temporal and spatial constraints on rule activation. The article also presents the results of an elicitation study that was conducted to identify possible visual paradigms for expressing composition rules. Prototypes implementing the resulting visual paradigms were compared during a controlled experiment and the one that resulted most relevant for our goals was used in a study that involved home-automation experts. Finally, the article discusses some design implications that came out from the performed studies and presents the architecture of a platform supporting rule definition and execution.},
author = {Desolda, Giuseppe and Ardito, Carmelo and Matera, Maristella},
doi = {10.1145/3057859},
file = {:Users/baharehzarei/Downloads/3057859.pdf:pdf},
issn = {15577325},
journal = {ACM Transactions on Computer-Human Interaction},
keywords = {Human-centered computings user studies,Information systems mashups,Software and its engineerings integrated and visua,Visual languages},
number = {2},
title = {{Empowering end users to customize their smart environments: Model, composition paradigms, and domain-specific tools}},
volume = {24},
year = {2017}
}
@phdthesis{Lucky2017,
author = {NESALUCKY, MEHERUN},
file = {:Users/baharehzarei/Downloads/phd{\_}unimib{\_}761118.pdf:pdf},
school = {UNIVERSITYOFMILAN-BICOCCA},
title = {{User-Driven Composition of Web APIs: Bridging the Gap between Users' Requirements and Technology Constraints}},
url = {https://boa.unimib.it/handle/10281/141783},
year = {2017}
}
@article{Zhao2019,
abstract = {Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users' feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.},
author = {Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J.},
doi = {10.1145/3340294},
file = {:Users/baharehzarei/Downloads/3340294.pdf:pdf},
issn = {1559114X},
journal = {ACM Transactions on the Web},
keywords = {Design guideline,Empirical studies,End-user service composition,Mapshups,Qualitative studies,Review framework,Service-oriented computing,Systematic review,User studies,Web services},
number = {3},
title = {{User studies on end-user service composition: A literature review and a design framework}},
volume = {13},
year = {2019}
}
@article{Maximilien2007a,
abstract = {Distributed programming has shifted from private networks to the public Internet and from using private and controlled services to increasingly using publicly available heterogeneous Web services (e.g., REST, SOAP, RSS, and Atom). This move enables the creation of innovative end-user-oriented composed services with user interfaces. These services mashups are typically point solutions to specific (specialized) problems; however, what is missing is a programming model that facilitates and accelerates creation and deployment of mashups of diverse services. In this paper we describe a domain-specific language that unifies the most common service models and facilitates service composition and integration into end-user-oriented Web applications. We demonstrate our approach with an implementation that leverages the Ruby on Rails framework. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Maximilien, E. Michael and Wilkinson, Hernan and Desai, Nirmit and Tai, Stefan},
doi = {10.1007/978-3-540-74974-5_2},
file = {:Users/baharehzarei/Downloads/WS{\_}API{\_}Mashups.pdf:pdf},
isbn = {9783540749738},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {13--26},
title = {{A domain-specific language for Web APIs and services mashups}},
volume = {4749 LNCS},
year = {2007}
}
@article{Tang2021,
abstract = {Web service composition has become a prevalent software development method that enables developing powerful Mashups by effectively combining Web services with different functions. However, as the number of Web services increases, it becomes challenging for developers to select appropriate services to develop Web applications that satisfy functional requirements. In order to recommend Web services considering user's preferences, a composition pattern-aware Web service recommendation method called EWACP-DeepFM is proposed, which combines the composition patterns between Web services and Mashups and the co-occurrence and popularity of Web services. By constructing a multi-dimensional feature matrix, which is further trained by the depth factorisation machine (DeepFM) model to learn potential link relationships between Web services and Mashup applications, and recommend Top-N best services for the target Mashup application. Experiments performed using the real datasets from ProgrammableWeb show that the proposed method outperforms others with better recommendation effectiveness.},
author = {Tang, Bing and Tang, Mingdong and Xia, Yanmin and Hsieh, Meng Yen},
doi = {10.1080/09540091.2021.1911933},
file = {:Users/baharehzarei/Downloads/Composition pattern-aware web service recommendation based on depth factorisation machine.pdf:pdf},
issn = {13600494},
journal = {Connection Science},
keywords = {Web service recommendation,composition patterns,depth factorisation machine,mashup,web API},
number = {4},
pages = {870--890},
publisher = {Taylor {\&} Francis},
title = {{Composition pattern-aware web service recommendation based on depth factorisation machine}},
url = {https://doi.org/10.1080/09540091.2021.1911933},
volume = {33},
year = {2021}
}
@book{Rojas2021,
author = {Rojas, Carlos},
booktitle = {Building Native Web Components},
doi = {10.1007/978-1-4842-5905-4},
file = {:Users/baharehzarei/Downloads/978-1-4842-5905-4.pdf:pdf},
isbn = {9781484259047},
title = {{Building Native Web Components}},
year = {2021}
}
@article{Piessens2002,
abstract = {The separation-of-concerns principle is one of the essential principles in software engineering. It says that software should be decomposed in such a way that different “concerns” or aspects of the problem at hand are solved in well-separated modules or parts of the software. Yet, many security experts feel uneasy about trying to isolate security-related con- cerns, because security is such a pervasive property of a piece of software. And in fact, separating security-related concerns such as access control, or defensive input checking, is indeed very hard to achieve with current software engineering techniques. While the authors fully agree with the observation that security is a pervasive property, they argue in this position paper that attempts to separate security aspects from other aspects of an application (even though in many cases not completely successful) are a necessary means to raise the security level of most applications. The two main arguments are: increased flexibility of the security mechanisms (leading to easier adaptation to unanticipated or evolving risks), and better-focused efforts of the few security experts in the development team, leading to fewer security design and implementation errors.},
author = {Piessens, Frank and Verhanneman, Tine and Win, Bart De},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Piessens, Verhanneman, Win - 2002 - On the importance of the separation-of-concerns principle in secure software engineering.pdf:pdf},
journal = {Workshop on the Application of Engineering Principles to System Security Design},
pages = {1--10},
title = {{On the importance of the separation-of-concerns principle in secure software engineering}},
url = {http://www.acsac.org/waepssd/papers/02-piessens.pdf},
year = {2002}
}
@article{Yang2002,
abstract = {Web services are becoming the prominent paradigm for distributed computing and electronic business. This has raised the opportunity for service providers and application developers to develop valueadded services by combining existing web services. Emerging web service standards and web service composition solutions have not addressed the issues of service re-use and extension yet. In this paper we propose the concept of web component that packages together elementary or complex services and presents their interfaces and operations in a consistent and uniform manner in the form of a class definition. Web components are internally synthesized out of reused, specialized, or extended elementary or complex web services. They are published externally as normal web services and can thus be employed by any web-based application.},
author = {Yang, Jian and Papazoglou, Mike P.},
doi = {10.1007/3-540-47961-9_5},
file = {:Users/baharehzarei/Downloads/3-540-47961-9{\_}5.pdf:pdf},
isbn = {9783540437383},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {21--36},
title = {{Web component: A substrate for web service reuse and composition}},
volume = {2348},
year = {2002}
}
@article{Springborg2022,
abstract = {Given the success of IoT platforms, more developers and companies want to include the technology in their portfolio. Code generators for API clients can enhance productivity, but they tend to generate universal purpose code, and on the other hand the networking primitives of IoT devices are platform specific, especially when security mechanisms such as Transport Layer Security are part of the picture. This paper presents cpp-tiny-client, an API client generator developed as a plugin for the OpenAPI Generator project, which can tailor the generated code based on the IoT platform specified by the user. Our work allows to generate correct code for API clients for IoT devices, and thus can empower a developer with more productivity and a faster time-to-market for its own applications. By combining together mainstream technologies only, cpp-tiny-client offers a gentle learning curve. Moreover, experiments show that the generated code has a reasonable footprint, at least with respect to the IoT devices that were used in the validation of the work. The code related to this work is available through the OpenAPI Generator project [31]. The information in this paper is extended in [27].},
archivePrefix = {arXiv},
arxivId = {arXiv:2201.00270v1},
author = {Springborg, Anders Aaen and Andersen, Martin Kaldahl and Hattel, Kaare Holland and Albano, Michele},
doi = {10.1145/3477314.3508387},
eprint = {arXiv:2201.00270v1},
file = {:Users/baharehzarei/Downloads/2201.00270.pdf:pdf},
isbn = {9781450387132},
journal = {Proceedings of the ACM Symposium on Applied Computing},
keywords = {ACM proceedings, LaTeX, text tagging},
number = {1},
pages = {202--205},
publisher = {Association for Computing Machinery},
title = {{cpp-tiny-client: A secure API client generator for IoT devices}},
volume = {1},
year = {2022}
}
@article{Yang2002a,
abstract = {Web services are becoming the prominent paradigm for distributed computing and electronic business. This has raised the opportunity for service providers and application developers to develop valueadded services by combining existing web services. Emerging web service standards and web service composition solutions have not addressed the issues of service re-use and extension yet. In this paper we propose the concept of web component that packages together elementary or complex services and presents their interfaces and operations in a consistent and uniform manner in the form of a class definition. Web components are internally synthesized out of reused, specialized, or extended elementary or complex web services. They are published externally as normal web services and can thus be employed by any web-based application.},
author = {Yang, Jian and Papazoglou, Mike P.},
doi = {10.1007/3-540-47961-9_5/COVER},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Papazoglou - 2002 - Web component A substrate for web service reuse and composition.pdf:pdf},
isbn = {9783540437383},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {21--36},
publisher = {Springer Verlag},
title = {{Web component: A substrate for web service reuse and composition}},
url = {https://link.springer.com/chapter/10.1007/3-540-47961-9{\_}5},
volume = {2348},
year = {2002}
}
@article{Bravo2010,
abstract = {Web service substitution is one of the most advanced tasks that a composite Web service developer must achieve. Substitution occurs when, in a composite scenario, a service operation is replaced to improve the composition performance or fix a disruption caused by a failing service. To move the automation of substitution forward, a set of measures, considering structure and functionality of Web services, are provided. Most of current proposals for the discovery and matchmaking of Web services are based on the semantic perspective, which lacks the precise information that is needed toward Web service substitution. This paper describes a set of similarity measures to support this substitution. Similarity measurement accounts the differences or similarities by the syntax comparison of names and data types, followed by the comparison of input and output parameters values of Web service operations. Calculation of these measures was implemented using a filtering process. To evaluate this approach, a software architecture was implemented, and experimental tests were carried on both private and public available Web services. Additionally, as is discussed, the application of these measures can be extended to other Web services tasks, such as classification, clustering and composition. {\textcopyright} 2010, IGI Global.},
author = {Bravo, Maricela and Alvarado, Matias},
doi = {10.4018/jwsr.2010070101},
file = {:Users/baharehzarei/Downloads/Similarity-Measures-for-Substituting-Web-Services.pdf:pdf},
isbn = {9781466619425},
issn = {15457362},
journal = {International Journal of Web Services Research},
keywords = {Functional Similarity,Level of Substitution,Similarity Measurement,Structural Similarity,Web Service Substitution},
number = {3},
pages = {1--29},
title = {{Parsimilarity measures for substituting web services}},
volume = {7},
year = {2010}
}
@inproceedings{Karavisileiou2021,
author = {Karavisileiou, Aikaterini and Mainas, Nikolaos and Bouraimis, Fotios},
booktitle = {Proceedings of the 2021 Future of Information and Communication Conference (FICC)},
editor = {Arai, Kohei},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Karavisileiou, Mainas, Bouraimis - 2021 - Automated Ontology Instantiation of OpenAPI REST Service Descriptions.pdf:pdf},
isbn = {9783030731007},
keywords = {web service},
pages = {945--962},
publisher = {Springer, Cham},
title = {{Automated Ontology Instantiation of OpenAPI REST Service Descriptions}},
year = {2021}
}
@article{Espinoza-Arias2020,
abstract = {Many organizations maintain knowledge graphs that are organized according to ontologies. However, these ontologies are implemented in languages (e.g. OWL) that are difficult to understand by users who are not familiar with knowledge representation techniques. In particular, this affects web developers who want to develop ontology-based applications but may find challenging accessing ontology-based data in knowledge graphs through SPARQL queries. To address this problem, we propose an accessible layer for ontology-based knowledge graphs through REST APIs. We define a mapping specification between the Web Ontology Language (OWL) and the OpenAPI Specification (OAS) to provide ontology-based API definitions in a language well-known to web developers. Our mapping specification identifies key similarities between OWL and OAS and provides implementation guidelines and examples. We also developed a reference mapping implementation that automatically transforms OWL ontologies into OpenAPI specifications in a matter of seconds.},
author = {Espinoza-Arias, Paola and Garijo, Daniel and Corcho, Oscar},
doi = {10.1007/978-3-030-65847-2_11},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Espinoza-Arias, Garijo, Corcho - 2020 - Mapping the Web Ontology Language to the OpenAPI Specification.pdf:pdf},
isbn = {9783030658465},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {OWL,Ontologies,OpenAPI,REST API},
pages = {117--127},
title = {{Mapping the Web Ontology Language to the OpenAPI Specification}},
volume = {12584 LNCS},
year = {2020}
}
@article{Karavisileiou2020a,
abstract = {OpenAPI Specification (OAS) defines a description format for REST APIs. In order for a machine to understand the meaning of REST services, OpenAPI service descriptions must be unambiguous. In a previous work we analysed the reasons that cause ambiguities in OpenAPI and showed that, in order to eliminate ambiguities, OpenAPI properties must be semantically annotated and mapped to a semantic model. Leveraging latest results for hypermedia-based construction of Web APIs (i.e. Hydra), the present work forwards this approach and proposes a reference ontology for REST services along with a formal procedure for converting OpenAPI service descriptions to instances of this ontology.},
author = {Karavisileiou, Aikaterini and Mainas, Nikolaos and Petrakis, Euripides G.M.},
doi = {10.1109/ICTAI50040.2020.00016},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Karavisileiou, Mainas, Petrakis - 2020 - Ontology for OpenAPI REST Services Descriptions.pdf:pdf},
isbn = {9781728192284},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Hydra,Ontology,OpenAPI,REST,Web service},
pages = {35--40},
title = {{Ontology for OpenAPI REST Services Descriptions}},
volume = {2020-Novem},
year = {2020}
}
@article{Elsayed2020,
abstract = {In this paper, we propose a new mapping technique from the OMG's UML modeling language into the Web Ontology Language (OWL) to serve the Semantic Web. UML (Unified Modeling Language) is widely accepted and used as a standardized modeling language in Object-Oriented Analysis (OOA) and Design (OOD) approach by domain experts to model real-world objects in software development. On the other hand, the conceptualization, which is represented in OWL, is designed to process the content of information rather than just present the information. Therefore, the matter of migrating UML to OWL is becoming an energetic research domain. OWL (Web Ontology Language) is a Semantic Web language designed for defining ontologies on the Web. An ontology is a formal specification naming and definition of shared data. This technique describes how to map UML Models into OWL and allows us to keep semantic of UML sequence diagrams such as messages, the sequence of messages, guard invariant, etc. to make data of UML sequence diagrams machine-readable.},
author = {Elsayed, Mo'men and Elkashef, Nermeen and Hassan, Yasser F.},
doi = {10.14569/IJACSA.2020.0110542},
file = {:Users/baharehzarei/Downloads/Paper{\_}42-Mapping{\_}UML{\_}Sequence{\_}Diagram.pdf:pdf},
issn = {21565570},
journal = {International Journal of Advanced Computer Science and Applications},
keywords = {Mapping,OWL,Ontology,Sequence diagram,UML,Unified modeling language,Web ontology language},
number = {5},
pages = {318--326},
title = {{Mapping UML sequence diagram into the web ontology language OWL}},
volume = {11},
year = {2020}
}
@article{Wafa,
author = {Wafa, Souiou and Nora, Bounour},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wafa, Nora - Unknown - Rules-Based Approach To Convert Class Diagram Operations To Ontology.pdf:pdf},
keywords = {class},
pages = {77--80},
title = {{Rules-Based Approach To Convert Class Diagram Operations To Ontology}}
}
@article{OpenNetworkingFoundation2018,
author = {{Open Networking Foundation}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Open Networking Foundation - 2018 - UML to OpenAPI Mapping Guide lines.pdf:pdf},
pages = {1--34},
title = {{UML to OpenAPI Mapping Guide lines}},
year = {2018}
}
@article{Ed-Douibi2018a,
abstract = {REpresentational State Transfer (REST) has become the prominent architectural style for designing Web APIs. This increasing adoption has triggered the creation of languages to formally describe REST APIs, thus facilitating and promoting their usage. In particular, a consortium of companies has created the OpenAPI Initiative, which aims at creating a vendor neutral, portable, standard and open specification for describing REST APIs. OpenAPI specification has become the choice of reference for describing REST APIs, and its adopters can benefit from a plethora of tools for documenting, developing and integrating REST APIs. However, current documentation tools for OpenAPI only describe REST APIs in HTML pages using text and code samples, thus requiring a considerable effort to visualize and understand what the APIs offer. In this paper, we propose a tool called OpenAPItoUML, which generates UML models from OpenAPI definitions, thus offering a better visualization of the data model and operations of REST APIs.},
author = {Ed-Douibi, Hamza and {C{\'{a}}novas Izquierdo}, Javier Luis and Cabot, Jordi},
doi = {10.1007/978-3-319-91662-0_41},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ed-Douibi, C{\'{a}}novas Izquierdo, Cabot - 2018 - OpenAPItoUML A tool to generate UML models from OpenAPI definitions.pdf:pdf},
isbn = {9783319916613},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {OpenAPI,REST API,UML},
pages = {487--491},
title = {{OpenAPItoUML: A tool to generate UML models from OpenAPI definitions}},
volume = {10845 LNCS},
year = {2018}
}
@inproceedings{Ed-Douibi2018,
abstract = {More and more companies and governmental organizations are publishing data on the Web via REST APIs. The increasing number of REST APIs has promoted the creation of specialized applications aiming to combine and reuse different data sources to generate and deduce new information. However, creating such applications is a tedious and error-prone process since developers must invest much time in discovering the data model behind each candidate REST API, define the composition strategy, and manually implement such strategy. To facilitate this process, we propose an approach to automatically compose and orchestrate data-oriented REST APIs. For an initial set of REST APIs, we discover the data models, identify matching concepts, obtain a global model, and make the latter available on the Web as a global REST API. A prototype tool relying on OpenAPI for describing APIs and on OData for querying them is also provided.},
address = {Como},
author = {Ed-Douibi, Hamza and {C{\'{a}}novas Izquierdo}, Javier Luis and Cabot, Jordi},
booktitle = {7th IFIP WG 2.14 European Conference, ESOCC 2018},
doi = {10.1007/978-3-319-99819-0_12},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ed-Douibi, C{\'{a}}novas Izquierdo, Cabot - 2018 - APIComposer Data-driven composition of REST APIs.pdf:pdf},
isbn = {9783319998183},
issn = {16113349},
keywords = {API composition,Modeling,OData,OpenAPI,REST API},
pages = {161--169},
title = {{APIComposer: Data-driven composition of REST APIs}},
volume = {11116 LNCS},
year = {2018}
}
@article{Serrano2017,
abstract = {Over the last decade, an exponentially increasing number of REST services have been providing a simple and straightforward syntax for accessing rich data resources. To use these services, however, developers have to understand 'information-use contracts' specified in natural language, and, to build applications that benefit from multiple existing services they have to map the underlying resource schemas in their code. This process is difficult and error-prone, especially as the number and overlap of the underlying services increases, and the mappings become opaque, difficult to maintain, and practically impossible to reuse. The more recent advent of the Linked Data formalisms can offer a solution to the challenge. In this paper, we propose a conceptual framework for REST-service integration based on Linked Data models. In this framework, the data exposed by REST services is mapped to Linked Data schemas, based on these descriptions, we have developed a middleware that can automatically compose API calls to respond to data queries (in SPARQL). Furthermore, we have developed a RDF model for characterizing the access-control protocols of these APIs and the quality of the data they expose, so that our middleware can develop 'legal' compositions with desired qualities. We report our experience with the implementation of a prototype that demonstrates the usefulness of our framework in the context of a research-data management application.},
author = {Serrano, Diego and Stroulia, Elen and Lau, Diana and Ng, Tinny},
doi = {10.1109/ICWS.2017.26},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Serrano et al. - 2017 - Linked REST APIs A Middleware for Semantic REST API Integration.pdf:pdf},
isbn = {9781538607527},
journal = {Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017},
keywords = {Linked Data,REST APIs,data integration},
pages = {138--145},
title = {{Linked REST APIs: A Middleware for Semantic REST API Integration}},
year = {2017}
}
@book{Mendes2006,
abstract = {Since its original inception back in 1989 the Web has changed into an environment where Web applications range from small-scale information dissemination applications, often developed by non-IT professionals, to large-scale, commercial, enterprise-planning and scheduling applications, developed by multidisciplinary teams of people with diverse skills and backgrounds and using cutting-edge, diverse technologies. As an engineering discipline, Web engineering must provide principles, methodologies and frameworks to help Web professionals and researchers develop applications and manage projects effectively. Mendes and Mosley have selected experts from numerous areas in Web engineering, who contribute chapters where important concepts are presented and then detailed using real industrial case studies. After an introduction into the discipline itself and its intricacies, the contributions range from Web effort estimation, productivity benchmarking and conceptual and model-based application development methodologies, to other important principles such as usability, reliability, testing, process improvement and quality measurement. This is the first book that looks at Web engineering from a measurement perspective. The result is a self-containing, comprehensive overview detailing the role of measurement and metrics within the context of Web engineering. This book is ideal for professionals and researchers who want to know how to use sound principles for the effective management of Web projects, as well as for courses at an advanced undergraduate or graduate level. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
archivePrefix = {arXiv},
arxivId = {cs/0306108},
author = {Mendes, Emilia and Mosley, Nile},
booktitle = {Web Engineering},
doi = {10.1007/3-540-28218-1},
eprint = {0306108},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mendes, Mosley - 2021 - Web engineering.pdf:pdf},
isbn = {3540281967},
pages = {1--438},
primaryClass = {cs},
title = {{Web engineering}},
year = {2021}
}
@article{Paredes-Valverde2012,
abstract = {The social networks have become in a powerful diffusion media in several fields such as communication, e-commerce and entertainment. However, the development of new applications that combine the functionality of different social networks with the purpose of providing added-value to users is not very common. In this context, a new kind of applications called mashups has emerged. A mashup is a web application that integrates data from multiple web sources in order to provide a unique service. Internal data sources, RSS/Atom feeds, Screen-Scraping and Web Services are some resources used by mashups. Nowadays, most of Web Services provided by social networks use the REST-based architectural style because it offers significant advantages in comparison with other technologies. The contribution of this paper is a review of REST-based APIs for the development of mashups that integrate well known social networks such as Youtube{\textcopyright}, Picasa{\textcopyright}, and Flickr{\textcopyright}, among others. In addition, a set of 4 mashups were developed combining the APIs discussed. Also, this work provides a development guide to perform tasks such as extraction and combination from different data sources, as well as leads to the emergence of new ideas for developing web applications.},
author = {Paredes-Valverde, Mario Andr{\'{e}}s and Alor-Hern{\'{a}}ndez, Giner and Rodr'guez-Gonz{\'{a}}lez, Alejandro and Hern{\'{a}}ndez-Chan, Gandhi},
doi = {10.1016/j.protcy.2012.03.022},
file = {:Users/baharehzarei/Downloads/1-s2.0-S2212017312002514-main.pdf:pdf},
issn = {22120173},
journal = {Procedia Technology},
keywords = {acm,corresponding author,e-mail address,marioparedes,mashup,org,rest,social networks,web services},
pages = {205--213},
title = {{Developing Social Networks Mashups: An Overview of REST-Based APIs}},
volume = {3},
year = {2012}
}
@article{Lee2012,
abstract = {Data mashups enable users to create new applications by combining Web APIs from several data sources. Although data mashups have become very popular over the last few years, there are several challenging issues when combining Web APIs into data mashups, especially when compatible APIs are manually discovered and composed by mash up developers. In this paper, we propose an approach for automatic discovery and composition of Web APIs using their semantic descriptions. This approach can be described as that of generating directed a cyclic graphs (DAGs) that can produce output satisfying a desired goal. We rapidly filter out APIs that are guaranteed not to involve the composition in order to produce the DAGs efficiently. {\textcopyright} 2012 IEEE.},
author = {Lee, Yong Ju and Kim, Jae Soo},
doi = {10.1109/CICN.2012.56},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Kim - 2012 - Automatic web API composition for semantic data mashups.pdf:pdf},
isbn = {9780769548500},
journal = {Proceedings - 4th International Conference on Computational Intelligence and Communication Networks, CICN 2012},
keywords = {Web API,composition,data mashup,discovery,graph-based composition algorithm,semantic matching method},
pages = {953--957},
publisher = {IEEE},
title = {{Automatic web API composition for semantic data mashups}},
year = {2012}
}
@article{Yang2018,
abstract = {Web API specifications are machine-readable descriptions of APIs. These specifications, in combination with related tooling, simplify and support the consumption of APIs. However, despite the increased distribution of web APIs, specifications are rare and their creation and maintenance heavily rely on manual efforts by third parties. In this paper, we propose an automatic approach and an associated tool called D2Spec for extracting significant parts of such specifications from web API documentation pages. Given a seed online documentation page of an API, D2Spec first crawls all documentation pages on the API, and then uses a set of machine-learning techniques to extract the base URL, path templates, and HTTP methods - collectively describing the endpoints of the API. We evaluate whether D2Spec can accurately extract endpoints from documentation on 116 web APIs. The results show that D2Spec achieves a precision of 87.1{\%} in identifying base URLs, a precision of 80.3{\%} and a recall of 80.9{\%} in generating path templates, and a precision of 83.8{\%} and a recall of 77.2{\%} in extracting HTTP methods. In addition, in an evaluation on 64 APIs with pre-existing API specifications, D2Spec revealed many inconsistencies between web API documentation and their corresponding publicly available specifications. API consumers would benefit from D2Spec pointing them to, and allowing them thus to fix, such inconsistencies.},
author = {Yang, Jinqiu and Wittern, Erik and Ying, Annie T.T. and Dolby, Julian and Tan, Lin},
doi = {10.1145/3196398.3196411},
file = {:Users/baharehzarei/Desktop/Towards{\_}Extracting{\_}Web{\_}API{\_}Specifications{\_}from{\_}Documentation.pdf:pdf},
isbn = {9781450357166},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
pages = {454--464},
publisher = {ACM},
title = {{Towards extracting web API specifications from documentation}},
year = {2018}
}
@inproceedings{Koch2012,
author = {Koch, Nora and Brambilla, Marco and Meli{\'{a}}, Santiago},
booktitle = {8th International Workshop on Model-Driven and Agile Engineering for the Web (MDWE 2012)},
doi = {10.1007/978-3-642-35623-0},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Koch, Brambilla, Meli{\'{a}} - 2012 - Extracting models from Web API documentation.pdf:pdf},
isbn = {9783642356223},
issn = {03029743},
number = {July},
title = {{Extracting models from Web API documentation}},
volume = {7703 LNCS},
year = {2012}
}
@article{Karakostas2017,
abstract = {This paper investigates the adequacy of API mashups for supporting the information requirements of travelers, prior, during and after the journey. We analyse the information requirements of travelers at the various stages of a multimodal journey and perform a gap analysis against the actual information supplied by travel APIs. We base the approach on the analysis of three publicly available travel related APIs. Finally, we propose how semantically annotating APIs can support intelligent travel assistants that address the information requirements of travelers.},
author = {Karakostas, Bill and Kalamboukis, Zannis},
doi = {10.1016/j.procs.2017.05.326},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Karakostas, Kalamboukis - 2017 - API mashups How well do they support the travellers' information needs.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {API,API composition (mashup),cognitive travel process,semantic annotation,travel API},
pages = {204--209},
publisher = {Elsevier B.V.},
title = {{API mashups: How well do they support the travellers' information needs?}},
url = {http://dx.doi.org/10.1016/j.procs.2017.05.326},
volume = {109},
year = {2017}
}
@article{Xue2017,
abstract = {Until today, finding the most suitable APIs to use in an application was burdensome, requiring manual and time-consuming searches across a diverse set of websites, in particular regarding how multiple APIs could be combined and worked together (i.e. API mashups). In this paper, we propose a new method to automatically generate API mashups through real-world data collection, text mining and natural language processing (NLP ) techniques. The generated API mashups are further ranked and recommended to developers based on a quantitative indicator of whether the given API mashup is plausible. To evaluate the overall accuracy of the proposed method, we use the generated API mashups to train several machine learning and deep learning models, and then use an independent mashup dataset collected from Github projects for testing. The experimental results show that our proposed method is feasible and accurate for automatic API mashup generation and recommendation.},
annote = {AI planners},
author = {Xue, Qinghan and Liu, Lei and Chen, Weipeng and Chuah, Mooi Choo},
doi = {10.1109/ICMLA.2017.0-169},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Xue et al. - 2017 - Automatic generation and recommendation for API mashups.pdf:pdf},
isbn = {9781538614174},
journal = {Proceedings - 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017},
keywords = {AP Imashups,NLP,deep learning,machine learning},
pages = {119--124},
title = {{Automatic generation and recommendation for API mashups}},
volume = {2017-Decem},
year = {2017}
}
@article{Serrano2020,
abstract = {Web APIs have been adopted as the de facto standard for exchanging data on the Web. However, engineering applications that orchestrate the invocation of multiple APIs and the data flow among them are still mostly manual and labor intensive. In fact, as the number of the potentially relevant APIs increases, compositions become opaque, difficult to maintain, and practically impossible to reuse. The recent advances around linked data formalisms have the potential to provide "usable" semantics, to enable automatic API composition methods. In this paper, we formalize a simplified description model, based on SPARQL graph patterns, for capturing the semantics of Web APIs. Based on this model, we propose a methodology for a fully automated process that produces semantically valid composition chains, using iterative subgraph isomorphism. We have validated the usefulness and accuracy of our approach, using a collection of publicly available Web APIs relevant to a real-world use cases.},
author = {Serrano, Diego and Stroulia, Eleni},
doi = {10.1007/s11761-020-00301-1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Serrano, Stroulia - 2020 - Semantics-based API discovery, matching and composition with linked metadata.pdf:pdf},
journal = {Service Oriented Computing and Applications},
keywords = {Automatic service composition,Linked data,RDF,REST,SPARQL,SWAGGER,Semantic Web services},
pages = {283--296},
title = {{Semantics-based API discovery, matching and composition with linked metadata}},
url = {https://doi.org/10.1007/s11761-020-00301-1},
volume = {14},
year = {2020}
}
@article{Lee2013,
author = {Lee, Yong-ju},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lee - 2013 - Algorithm for Automatic Web API Composition.pdf:pdf},
isbn = {9781612082486},
journal = {WEB 2013, The First International Conference on Building and Exploring Web Based Environments},
keywords = {-automatic composition algorithm,mashup,ontology learning method,semantic data,web api},
number = {c},
pages = {57--62},
title = {{Algorithm for Automatic Web API Composition}},
year = {2013}
}
@article{Wu2021,
abstract = {As the number of Web APIs ever increases, choosing the appropriate APIs for mashup creations becomes more difficult. To tackle this problem, various methods have been proposed to recommend APIs to match requirements of mashups and achieved much success. However, there existed some challenges with feature fusion and utilization, textual requirement understanding, utilization of Mashup categories and compatibility evaluation. Therefore, we propose a neural framework (MTFM) based on multi-model fusion and multi-task learning for Mashup-oriented Web API recommendation. MTFM exploits a semantic component to generate representations of requirements and introduces a feature interaction component to model the feature interaction between mashups and Web APIs. Output features of both components are further fused to predict the candidate APIs, and this enables us to have both the advantages of content-based and collaborative filtering methods. We further introduce mashup category judgment as an auxiliary task, where both tasks are viewed as a multi-label learning problem and jointly optimized with multi-task learning. Also, we have extended MTFM to MTFM++ to take advantage of the metadata and quality features of APIs, and proposed a metric for compatibility evaluation. Experimental results on the ProgrammableWeb dataset show that our methods outperform most popular state-of-the-art methods.},
author = {Wu, Hao and Duan, Yunhao and Yue, Kun and Zhang, Lei},
doi = {10.1109/TSC.2021.3098756},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2021 - Mashup-Oriented Web API Recommendation via Multi-Model Fusion and Multi-Task Learning.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Blogs,Collaboration,Feature extraction,Mashup,Mashups,Semantics,Social networking (online),Task analysis,Web API recommendation,multi-model fusion,multi-task learning,neural network},
number = {c},
pages = {1--14},
publisher = {IEEE},
title = {{Mashup-Oriented Web API Recommendation via Multi-Model Fusion and Multi-Task Learning}},
volume = {1374},
year = {2021}
}
@article{,
title = {{IEEE Xplore Full-Text PDF:}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=9492754}
}
@article{Osrof2012,
author = {Osrof, Maha and Zaghal, R},
file = {:Users/baharehzarei/Downloads/MT{\_}1970{\_}20714018{\_}8070.pdf:pdf},
journal = {Acit2K.Org},
keywords = {backtracking,business rules categories,mapping,ontology concepts},
title = {{Ontology Based Business Rules Extraction Model and Algorithm (OBBREMA)}},
url = {http://www.acit2k.org/ACIT/2012Proceedings/524.pdf},
year = {2012}
}
@article{Englund2021,
abstract = {Smart cities and communities (SCC) constitute a new paradigm in urban development. SCC ideate a data-centered society aimed at improving efficiency by automating and optimizing activities and utilities. Information and communication technology along with Internet of Things enables data collection and with the help of artificial intelligence (AI) situation awareness can be obtained to feed the SCC actors with enriched knowledge. This paper describes AI perspectives in SCC and gives an overview of AI-based technologies used in traffic to enable road vehicle automation and smart traffic control. Perception, smart traffic control and driver modeling are described along with open research challenges and standardization to help introduce advanced driver assistance systems and automated vehicle functionality in traffic. To fully realize the potential of SCC, to create a holistic view on a city level, availability of data from different stakeholders is necessary. Further, though AI technologies provide accurate predictions and classifications, there is an ambiguity regarding the correctness of their outputs. This can make it difficult for the human operator to trust the system. Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model. Another challenge related to trust is explainability: models can have difficulty explaining how they came to certain conclusions, so it is difficult for humans to trust them.},
author = {Englund, Cristofer and Aksoy, Eren Erdal and Alonso-Fernandez, Fernando and Cooney, Martin Daniel and Pashami, Sepideh and {\AA}strand, Bj{\"{o}}rn},
doi = {10.3390/smartcities4020040},
file = {:Users/baharehzarei/Downloads/AI{\_}Perspectives{\_}in{\_}Smart{\_}Cities{\_}and{\_}Communities{\_}to.pdf:pdf},
isbn = {4670856022},
issn = {26246511},
journal = {Smart Cities},
keywords = {Artificial intelligence,Driver modeling,Perception,Smart cities,Smart traffic control},
number = {2},
pages = {783--802},
title = {{Ai perspectives in smart cities and communities to enable road vehicle automation and smart traffic control}},
volume = {4},
year = {2021}
}
@article{Komninos2021,
abstract = {The paper is a follow-up of a previous investigation and effort to develop the ontology of the smart city (Komninos, N., Bratsas, C., Kakderi, C., and Tsarchopoulos, P. "Smart city ontologies: Improving the effectiveness of smart city applications". Journal of Smart Cities, vol. 1(1), 1-17. https://www.komninos.eu/wp-content/uploads/2015/07/2015-Smart-City-Ontologies-Published.pdf). Since the publication of this article in 2015, research and literature on smart cities have evolved significantly, as have the technologies for digital spaces and applications that support city functions. These developments are reflected in the present form of the smart city ontology 2.0 we propose. It depicts the building blocks of the smart city ontology (technologies, structure, function, planning), and the object properties and data properties that connect structural blocks and classes. The aim of the SCO 2.0 is to provide a better understanding and description of the smart/intelligent city landscape; identify the main components and processes, the terms used to describe them, their definition and meaning; clarify key processes related to the integration of the different dimensions of the smart city, mainly the physical, social, and digital dimensions. The paper is accompanied by an owl file, developing the ontology through the editor Prot{\'{e}}g{\'{e}}.},
author = {Komninos, Nicos and Panori, Anastasia and Kakderi, Christina},
doi = {10.20944/preprints202108.0101.v1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Komninos, Panori, Kakderi - 2021 - The Smart City Ontology 2.0 Assessing the components and interdependencies of city smartness.pdf:pdf},
keywords = {city smartness,intelligent city,ontology,smart city,smart ecosystem},
number = {August},
pages = {1--35},
title = {{The Smart City Ontology 2.0: Assessing the components and interdependencies of city smartness}},
url = {https://basic-formal-ontology.org/},
year = {2021}
}
@article{Komninos2016,
abstract = {This paper addresses the problem of low impact of smart city applications observed in the fields of energy and transport, which constitute high-priority domains for the development of smart cities. However, these are not the only fields where the impact of smart cities has been limited. The paper provides an explanation for the low impact of various individual applications of smart cities and discusses ways of improving their effectiveness. We argue that the impact of applications depends primarily on their ontology, and secondarily on smart technology and programming features. Consequently, we start by creating an overall ontology for the smart city, defining the building blocks of this ontology with respect to the most cited definitions of smart cities, and structuring this ontology with the Prot{\'{e}}g{\'{e}} 5.0 editor, defining entities, class hierarchy, object properties, and data type properties. We then analyze how the ontologies of a sample of smart city applications fit into the overall Smart City Ontology, the consistency between digital spaces, knowledge processes, city domains targeted by the applications, and the types of innovation that determine their impact. In conclusion, we underline the relationships between innovation and ontology, and discuss how we can improve the effectiveness of smart city applications, combining expert and user-driven ontology design with the integration and or-chestration of applications over platforms and larger city entities such as neighborhoods, districts, clusters, and sectors of city activities.},
author = {Komninos, Nicos and Bratsas, Charalampos and Kakderi, Christina and Tsarchopoulos, Panagiotis},
doi = {10.18063/jsc.2015.01.001},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Komninos et al. - 2016 - Smart City Ontologies Improving the effectiveness of smart city applications.pdf:pdf},
issn = {2382641X},
journal = {Journal of Smart Cities},
keywords = {54124,application,aristotle,correspondence to,department of urban and,design,efficiency,email,greece,impact,innovation,intelligent city,komninos,nicos komninos,ontology,org,regional development and planning,thessaloniki,university of thessaloniki,urenio,urenio research},
number = {1},
title = {{Smart City Ontologies: Improving the effectiveness of smart city applications}},
volume = {1},
year = {2016}
}
@misc{,
title = {{IEEE Xplore Full-Text PDF:}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=9492754},
urldate = {2022-10-22}
}
@article{Buitelaar,
abstract = {In this paper we describe a plug-in (OntoLT) for the widely used Prot{\'{e}}g{\'{e}} ontol-ogy development tool that supports the interactive extraction and/or extension of ontologies from text. The OntoLT approach provides an environment for the integration of linguistic analysis in ontology engineering through the definition of mapping rules that map linguistic entities in annotated text collections to concept and attribute candidates (i.e. Prot{\'{e}}g{\'{e}} classes and slots). The paper explains this approach in more detail and discusses some initial experiments on deriving a shallow ontology for the neurology domain from a corresponding collection of neurological scientific abstracts.},
author = {Buitelaar, Paul and Olejnik, Daniel and Sintek, Michael},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Buitelaar, Olejnik, Sintek - Unknown - A Prot{\'{e}}g{\'{e}} Plug-In for Ontology Extraction from Text Based on Linguistic Analysis.pdf:pdf},
title = {{A Prot{\'{e}}g{\'{e}} Plug-In for Ontology Extraction from Text Based on Linguistic Analysis}}
}
@article{Patrascu,
abstract = {By their operations variety, insurance companies are faced with a large amount of data, taking into account that an insurance type allows to cover several risks types. Also, the same risk type can be the subject to several insurance types. The risk notion is specific to the insurance domain and the insurance risk significances are varied. Ontology is a formal, explicit specification of the terms and defines a common vocabulary for researchers who need to communicate information in a certain field. This paper aims to create an OWL ontology for modelling the activity of an insurance company, providing a solid basis to apply knowledge in the general insurances domain, by defining a set of representation terms for risks, in order for the big insurance companies to discover new information relating to the interaction between risks, to form an overview of the external and internal risk factors to which an insurer is subjected to.},
author = {Pătraşcu, Aurelia},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pătraşcu - Unknown - Ontology based Approach for an Insurance Company Activity Modelling.pdf:pdf},
keywords = {C81,OWL ontology,Prot{\'{e}}g{\'{e}} ontology editor,insurance policy,types of risk JEL Clasification: C63},
title = {{Ontology based Approach for an Insurance Company Activity Modelling}}
}
@book{Duarte2021,
author = {Duarte, Alejandro},
booktitle = {Practical Vaadin},
doi = {10.1007/978-1-4842-7179-7},
file = {:Users/baharehzarei/Desktop/Duarte2021{\_}Book{\_}PracticalVaadin.pdf:pdf},
isbn = {9781484271780},
title = {{Practical Vaadin}},
year = {2021}
}
@article{Gaedke2000,
abstract = {From a software engineering perspective the World Wide Web is a new application platform. The implementation model that the Web is based on makes it difficult to apply classic process models to the development and even more the evolution of Web-applications. Component-based software development seems to be a promising approach for addressing key requirements of the very dynamic field of Web-application development and evolution. But such an approach requires dedicated support. The WebComposition Process Model addresses this requirement by describing the component-based development of Web-applications. It uses an XML-based markup language to seamlessly integrate with existing Web-standards. For the coordination of components the concept of an open process model with an explicit support for reuse is introduced. By describing application domains using domain-components the process model addresses the need for a controlled evolution of Web applications.},
author = {Gaedke, M and Graef, G},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gaedke, Graef - 2000 - Development and Evolution of Web-Applications using the WebComposition Process Model.pdf:pdf},
title = {{Development and Evolution of Web-Applications using the WebComposition Process Model}},
year = {2000}
}
@article{Boufrida2022,
abstract = {Due to the considerable increase in freely available data (especially on the Web), extracting relevant information from textual content is a critical challenge. Most of the available data is embedded in unstructured texts and is not linked to formalized knowledge structures such as ontologies or rules. A potential solution to this problem is to acquire such knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring complex relationships from texts and coding these in the form of rules. Our approach begins with existing domain knowledge represented as an OWL ontology, and applies NLP tools and text matching techniques to deduce different atoms, such as classes, properties and literals, to capture deductive knowledge in the form of new rules. For the reason, to enrich the existing domain ontology by these rules, in order to obtain higher relational expressiveness, make reasoning and produce new facts. The approach was tested using medical reports, specifically, in the specialty of gynecology. It reports an F-measure of 95.83{\%} on test our corpus.},
author = {Boufrida, Amina and Boufaida, Zizette},
doi = {10.1016/J.JKSUCI.2020.05.008},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Boufrida, Boufaida - 2022 - Rule extraction from scientific texts Evaluation in the specialty of gynecology.pdf:pdf},
issn = {22131248},
journal = {Journal of King Saud University - Computer and Information Sciences},
keywords = {Knowledge extraction,Natural language processing,Ontology Web Language (OWL) ontology,Rule acquisition,Semantic Web Rule Language (SWRL) rules,Text mining},
month = {apr},
number = {4},
pages = {1150--1160},
publisher = {King Saud bin Abdulaziz University},
title = {{Rule extraction from scientific texts: Evaluation in the specialty of gynecology}},
volume = {34},
year = {2022}
}
@misc{,
title = {{A Small Matter of Programming | Guide books}},
url = {https://dl.acm.org/doi/book/10.5555/529410},
urldate = {2022-03-29}
}
@article{Huang2013,
abstract = {Configurable process models can be used to provide information on business processes for different user groups in an appropriate and efficient manner. It promotes the reuse of proven practices by providing analysts with a generic modeling artifact from which to derive individual process models. Before a configurable business process being configured into a concrete business process model, the variability points of the configurable business process need to be identified. The decision on how to bind these variation points boils down to the users' requirements and needs. Given the specified requirements of the users, the configurable business process can be configured. In the paper, we propose a framework for carrying out automatic service-based business process configuration by using SWRL based business rules. We design and implement a variation point ontology, in which the guidelines of variable points are presented by SWRL rules. We also summarize a set of domain-specific business rules too, thus we can use these domain-specific rules to get the specific rules needed to meet users' requirements. We exploit domain ontology as knowledge base and rules as guideline to configure business process, for the purpose of individual configuration. Then we employ a configuration algorithm to configure a configurable business process depending on the reference result we obtain. The approach is validated by a case study from the domain of the urban logistics distribution. {\textcopyright} 2013 IEEE.},
author = {Huang, Ying and Feng, Zaiwen and He, Keqing and Huang, Yiwang},
doi = {10.1109/SCC.2013.59},
file = {:Users/baharehzarei/Downloads/Ontology-Based{\_}Configuration{\_}for{\_}Service-Based{\_}Business{\_}Process{\_}Model.pdf:pdf},
isbn = {9780768550268},
journal = {Proceedings - IEEE 10th International Conference on Services Computing, SCC 2013},
keywords = {Business process,Business rules,Configuration,Domain Ontology,Variation points},
pages = {296--303},
publisher = {IEEE},
title = {{Ontology-based configuration for service-based business process model}},
year = {2013}
}
@article{Bryant2010,
abstract = {This paper projects that an important future direction in software engineering is domain-specific software engineering (DSE). From requirements specification to design, and then implementation, a tighter coupling between the description of a software system with its application domain has the potential to improve both the correctness and reliability of the software system, and also lead to greater opportunities for software automation. In this position paper, we explore the impact of this emerging paradigm on requirements specification, design modeling, and implementation, as well as challenge areas benefiting from the new paradigm. Copyright 2010 ACM.},
author = {Bryant, Barrett R. and Gray, Jeff and Mernik, Marjan},
doi = {10.1145/1882362.1882376},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bryant, Gray, Mernik - 2010 - Domain-specific software engineering.pdf:pdf},
isbn = {9781450304276},
journal = {Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010},
keywords = {Domain-specific languages,Domain-specific modeling,Requirements specification},
number = {June 2014},
pages = {65--68},
title = {{Domain-specific software engineering}},
year = {2010}
}
@article{Sosa2011,
abstract = {To a larger extent than in other software applications, embedded systems commonly require the participation of a mixture of engineers that collaboratively produce a piece of software. This makes this area particularly prone to Domain Specific Languages (DSLs). By raising the abstraction level, DSLs facilitate the understanding of a DSL specification by engineers with different backgrounds. By being domain-specific, DSLs makes possible the separation of concerns that are not possible to separate at code level, and this in turn, facilitates the collaborative specification of DSL expressions. However, "these DSL views" are rarely orthogonal, and dependencies commonly exist among them. In some cases, task serialization along those dependencies might be a solution but at the cost of reducing task parallelization. Rather, this paper introduces "an assertive approach": all DSL view developments are launched from the start, and engineers can request from their mates, who are working on a different view, to prioritize some tasks so that they can continue. Realizing this vision implies: (1) explicitly stating DSL dependencies and (2), the existence of view-aware editors that interpret such dependencies during the collaborative specification of DSL expressions. This approach is borne out by MUVIE, a view-aware DSL editor implemented on top of GMF. {\textcopyright} 2011 IEEE.},
author = {Sosa, Josune De and D{\'{i}}az, Oscar and Trujillo, Salvador},
doi = {10.1109/SEAA.2011.41},
file = {:Users/baharehzarei/Downloads/Defining{\_}DSL{\_}Expressions{\_}Collaboratively{\_}in{\_}Multidisciplinary{\_}Embedded{\_}Engineering.pdf:pdf},
isbn = {9780769544885},
journal = {Proceedings - 37th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2011},
keywords = {DSL,collaborative,embedded engineering,multidisciplinary,views},
pages = {217--220},
publisher = {IEEE},
title = {{Defining DSL expressions collaboratively in multidisciplinary embedded engineering}},
year = {2011}
}
@article{Fehrenbach2013,
abstract = {Domain-specific languages (DSLs) can improve software maintainability due to less verbose syntax, avoidance of boilerplate code, more accurate static analysis, and domain-specific tool support. However, most existing applications cannot capitalise on these benefits because they were not designed to use DSLs, and rewriting large existing applications from scratch is infeasible. We propose a process for evolving existing software to use embedded DSLs based on modular definitions and applications of syntactic sugar as provided by the extensible programming language SugarJ. Our process is incremental along two dimensions: A developer can add support for another DSL as library, and a developer can refactor more code to use the syntax, static analysis, and tooling of a DSL. Importantly, the application remains executable at all times and no complete rewrite is necessary. We evaluate our process by incrementally evolving the Java Pet Store and a deliberately small part of the Eclipse IDE to use language support for field-accessors, JPQL, XML, and XML Schema. To help maintainers to locate Java code that would benefit from using DSLs, we developed a tool that analyses the definition of a DSL to derive patterns of Java code that could be represented with a high-level abstraction of the DSL instead. {\textcopyright} 2013 Springer International Publishing.},
author = {Fehrenbach, Stefan and Erdweg, Sebastian and Ostermann, Klaus},
doi = {10.1007/978-3-319-02654-1_6},
file = {:Users/baharehzarei/Downloads/SLE13FehrenbachEO.pdf:pdf},
isbn = {9783319026534},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {96--116},
title = {{Software evolution to domain-specific languages}},
volume = {8225 LNCS},
year = {2013}
}
@article{Insfran2012,
abstract = {Mashups are new-generation of Web applications aimed at reutilizing contents and services provided by third-party components. In Web applications, usability is considered one of the most important quality factors. However, usability evaluation of mashups is usually relied upon their individual components. Therefore, there is a need for specific approaches in order to evaluate the usability of mashups by considering their specific characteristics. This paper proposes a Usability Model for Mashups that can be used for evaluating the usability of mashups applications. This Usability Model decomposes the usability sub-characteristic from the ISO/IEC 25010 standard into other sub-characteristics and attributes. Finally, metrics are associated to these attributes to quantify and detect usability problems. In order to illustrate the feasibility of the approach, we present the evaluation of a mashup application using this usability model. {\textcopyright} 2012 IEEE.},
author = {Insfran, Emilio and Cedillo, Priscila and Fern{\'{a}}ndez, Adri{\'{a}}n and Abrah{\~{a}}o, Silvia and Matera, Maristella},
doi = {10.1109/QUATIC.2012.28},
file = {:Users/baharehzarei/Downloads/14.QUATIC12-EvaluatingtheUsabilityofMashupsApplications.pdf:pdf},
isbn = {9780769547770},
journal = {Proceedings - 2012 8th International Conference on the Quality of Information and Communications Technology, QUATIC 2012},
keywords = {Mashups,Metrics,SQuaRE,Usability Evaluation,Usability Model},
number = {June},
pages = {323--326},
title = {{Evaluating the usability of mashups applications}},
year = {2012}
}
@article{Lafage2019,
abstract = {This paper describes ONERA's on-going effort to develop a collaborative environment to support multidisciplinary design analysis and optimization in the context of overall vehicle design team activities. This environment, namely WhatsOpt, is a web application allowing the ONERA experts to define collaboratively aircraft multi-disciplinary analyses in terms of disciplines and data exchanges. From that high-level definition, users can generate the source code skeleton required to plug the implementations of disciplines and get an actual executable model of the concept under study. Then, they are able to run numerical methods such as sensibility analysis and design of experiments in order to finally tune the model, define and run optimizations regarding given design target requirements.},
author = {Lafage, R{\'{e}}mi and Defoort, Sebastien and Lef{\`{e}}bvre, Thierry},
doi = {10.2514/6.2019-2990},
file = {:Users/baharehzarei/Downloads/whatsopt-aiaa-1.1.1.pdf:pdf},
isbn = {9781624105890},
journal = {AIAA Aviation 2019 Forum},
number = {June},
pages = {1--20},
title = {{Whatsopt: A web application for multidisciplinary design analysis and optimization}},
year = {2019}
}
@article{Pascalau2009,
abstract = {This paper shows how business rules and particularly how JSON Rules can be used to model mashups together with underlining the advantages of this solution compared to traditional techniques. To achieve this, a concrete use case combining Monster Job Search and Google Maps is developed. In addition, we study the similarities between the conceptual models of mashup and Software as Service and argue towards a common sense by using their common root: the services choreography. {\textcopyright} IFIP International Federation for Information Processing 2009.},
author = {Pascalau, Emilian and Giurca, Adrian},
doi = {10.1007/978-3-642-04280-5_8},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pascalau, Giurca - 2009 - A Rule-Based Approach of Creating and Executing Mashups.pdf:pdf},
isbn = {9783642042799},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {Business rules,JSON Rules,Mashup,Software as Service,Web 2.0 applications},
pages = {82--95},
title = {{A Rule-Based Approach of Creating and Executing Mashups}},
volume = {305},
year = {2009}
}
@inproceedings{Li2020,
abstract = {The extraction of regulatory information is a prerequisite for automated code compliance checking. Although a number of machine learning models have been explored for extracting computer-understandable engineering constraints from code clauses written in natural language, most are inadequate to address the complexity of the semantic relations between named entities. In particular, the existence of two or more overlapping relations involving the same entity greatly exacerbates the difficulty of information extraction. In this paper, a joint extraction model is proposed to extract the relations among entities in the form of triplets. In the proposed model, a hybrid deep learning algorithm combined with a decomposition strategy is applied. First, all candidate subject entities are identified, and then, the associated object entities and predicate relations are simultaneously detected. In this way, multiple relations, especially overlapping relations, can be extracted. Furthermore, nonrelated pairs are excluded through the judicious recognition of subject entities. Moreover, a collection of domain-specific entity and relation types is investigated for model implementation. The experimental results indicate that the proposed model is promising for extracting multiple relations and entities from building codes.},
author = {Li, Fulin and Song, Yuanbin and Shan, Yongwei},
booktitle = {Appl. Sci.},
doi = {10.3390/app10207103},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Li, Song, Shan - Unknown - Joint Extraction of Multiple Relations and Entities from Building Code Clauses.pdf:pdf},
keywords = {automated code compliance checking,building code,deep learning,regulation information extraction},
title = {{Joint Extraction of Multiple Relations and Entities from Building Code Clauses}},
url = {www.mdpi.com/journal/applsci},
year = {2020}
}
@article{Zamanirad2017,
abstract = {At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code - all made possible by APIs. Yet, developers today are not as empowered to program bots in much the same way. To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. Our solution is two faceted: Firstly, we construct an API knowledge graph to encode and evolve APIs; secondly, leveraging the above we apply techniques in NLP, ML and Entity Recognition to perform the required synthesis from natural language user expressions into API calls.},
archivePrefix = {arXiv},
arxivId = {1711.05410},
author = {Zamanirad, Shayan and Benatallah, Boualem and Barukh, Moshe Chai and Casati, Fabio and Rodriguez, Carlos},
doi = {10.1109/ASE.2017.8115694},
eprint = {1711.05410},
file = {:Users/baharehzarei/Downloads/Programming{\_}Bots{\_}by{\_}Synthesizing{\_}Natural{\_}Language{\_}.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zamanirad et al. - 2017 - Programming bots by synthesizing natural language expressions into API invocations.pdf:pdf},
isbn = {9781538626849},
journal = {ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
number = {August 2019},
pages = {832--837},
title = {{Programming bots by synthesizing natural language expressions into API invocations}},
year = {2017}
}
@article{Nuruzzaman2018,
abstract = {Nowadays it is the era of intelligent machine. With the advancement of artificial intelligent, machine learning and deep learning, machines have started to impersonate as human. Conversational software agents activated by natural language processing is known as chatbot, are an excellent example of such machine. This paper presents a survey on existing chatbots and techniques applied into it. It discusses the similarities, differences and limitations of the existing chatbots. We compared 11 most popular chatbot application systems along with functionalities and technical specifications. Research showed that nearly 75{\%} of customers have experienced poor customer service and generation of meaningful, long and informative responses remains a challenging task. In the past, methods for developing chatbots have relied on hand-written rules and templates. With the rise of deep learning these models were quickly replaced by end-to-end neural networks. More specifically, Deep Neural Networks is a powerful generative-based model to solve the conversational response generation problems. This paper conducted an in-depth survey of recent literature, examining over 70 publications related to chatbots published in the last 5 years. Based on literature review, this study made a comparison from selected papers according to method adopted. This paper also presented why current chatbot models fails to take into account when generating responses and how this affects the quality conversation.},
author = {Nuruzzaman, Mohammad and Hussain, Omar Khadeer},
doi = {10.1109/ICEBE.2018.00019},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Nuruzzaman, Hussain - 2018 - A Survey on Chatbot Implementation in Customer Service Industry through Deep Neural Networks.pdf:pdf},
isbn = {9781538679920},
journal = {Proceedings - 2018 IEEE 15th International Conference on e-Business Engineering, ICEBE 2018},
keywords = {Chatbot,Deep learning,Dialogue system,Natural language processing,Neural network},
pages = {54--61},
publisher = {IEEE},
title = {{A Survey on Chatbot Implementation in Customer Service Industry through Deep Neural Networks}},
year = {2018}
}
@article{Directorate-general2019,
author = {Directorate-general, Digit and Isa, Informatics},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Directorate-general, Isa - 2019 - Architecture for public service chatbots.pdf:pdf},
title = {{Architecture for public service chatbots}},
year = {2019}
}
@article{Jung2013a,
abstract = {With the growing number of Web services available on the Web, finding services that match user's query becomes more difficult. To deal with the problem, numerous approaches employed semantic techniques in terms of annotation (or tagging) and service discovery algorithms. However, the problem of semantic service discovery based on the identification of fine-grained goals of users and services is still challenging due to the lack of semantic information in Web services. To enable the goal-driven semantic service discovery, we propose an automatic functional-goals tagging approach which employs a set of natural language processing (NLP) procedures based on the descriptions of Web services. In addition, to check the effectiveness of the functional-goals tagged, we designed a goal-driven semantic service discovery algorithm and compared it with other approaches: keyword-based, ontology-based, and topic-based service discovery. As results, our proposed goal-driven semantic service discovery achieved 75.7{\%} of precision and 59.6{\%} of recall (finally, F1=62{\%}) that outperform other discovery approaches. {\textcopyright} 2013 IEEE.},
author = {Jung, Yuchul and Cho, Yoonsung and Park, Yoo Mi and Lee, Taedong},
doi = {10.1109/ICSC.2013.45},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jung et al. - 2013 - Automatic tagging of functional-goals for goal-driven semantic service discovery(2).pdf:pdf},
isbn = {9780769551197},
journal = {Proceedings - 2013 IEEE 7th International Conference on Semantic Computing, ICSC 2013},
keywords = {Automatic tagging,Functional-goal,Goal-driven semantic service discovery,Web service},
pages = {212--219},
publisher = {IEEE},
title = {{Automatic tagging of functional-goals for goal-driven semantic service discovery}},
year = {2013}
}
@article{Adam2020,
abstract = {Communicating with customers through live chat interfaces has become an increasingly popular means to provide real-time customer service in many e-commerce settings. Today, human chat service agents are frequently replaced by conversational software agents or chatbots, which are systems designed to communicate with human users by means of natural language often based on artificial intelligence (AI). Though cost- and time-saving opportunities triggered a widespread implementation of AI-based chatbots, they still frequently fail to meet customer expectations, potentially resulting in users being less inclined to comply with requests made by the chatbot. Drawing on social response and commitment-consistency theory, we empirically examine through a randomized online experiment how verbal anthropomorphic design cues and the foot-in-the-door technique affect user request compliance. Our results demonstrate that both anthropomorphism as well as the need to stay consistent significantly increase the likelihood that users comply with a chatbot's request for service feedback. Moreover, the results show that social presence mediates the effect of anthropomorphic design cues on user compliance.},
author = {Adam, Martin and Wessel, Michael and Benlian, Alexander},
doi = {10.1007/s12525-020-00414-7},
file = {:Users/baharehzarei/Downloads/Adam2020{\_}Article{\_}AI-basedChatbotsInCustomerServ.pdf:pdf},
issn = {14228890},
journal = {Electronic Markets},
keywords = {Anthropomorphism,Artificial intelligence,Chatbot,Compliance,Customer service,Social presence},
number = {March},
publisher = {Electronic Markets},
title = {{AI-based chatbots in customer service and their effects on user compliance}},
year = {2020}
}
@article{Lampe2010,
abstract = {Matchmaking - i.e., the task of finding functionally suitable service offers based on a service request - has only been addressed in the context of WS-*Web services. However, RESTful services are gaining increasing attraction and have been adopted by major companies, thus increasing the need for suitable matchmaking solutions. This paper introduces XAM4SWS, an adaptive matchmaker for semantic Web services that supports multiple service description formats, including hRESTS and MicroWSMO for RESTful services. XAM4SWS adapts existing methodologies from WS-*matchmaking and extends them through the inclusion of REST-specific service features. A prototypical implementation of the matchmaker is evaluated with respect to multiple information retrieval metrics using an adapted semantic Web service test collection. {\textcopyright} 2010 ACM.},
author = {Lampe, Ulrich and Schulte, Stefan and Siebenhaar, Melanie and Schuller, Dieter and Steinmetz, Ralf},
doi = {10.1145/1883133.1883136},
file = {:Users/baharehzarei/Downloads/Adaptive{\_}matchmaking{\_}for{\_}RESTful{\_}services{\_}based{\_}on.pdf:pdf},
isbn = {9781450302388},
journal = {Proceedings of the 5th International Workshop on Enhanced Web Service Technologies, WEWST 2010},
keywords = {MicroWSMO,REST,matchmaking,semantic,service},
number = {January},
pages = {10--17},
title = {{Adaptive matchmaking for RESTful services based on hRESTS and MicroWSMO}},
year = {2010}
}
@article{Wang2013,
abstract = {Faced with the increasing services and users' personalized requirements, it remains a big challenge for users to effectively and accurately discover and reuse interested services. Goal oriented requirements modeling has attracted more and more attentions in services discovery and modeling, but little work has focused on extracting intentional goals from service descriptions. In this paper, based on the ranked domain keywords, we investigate how to extract domain-specific service goals from service descriptions, which can contribute to services discovery and recommendation. Programmable Web, a publicly accessible service repository, is selected as the testbed. Experiments show the feasibility of the proposed approach. {\textcopyright} 2013 IEEE.},
author = {Wang, Jian and Zhang, Neng and Zeng, Cheng and Li, Zheng and He, Keqing},
doi = {10.1109/SCC.2013.16},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2013 - Towards services discovery based on service goal extraction and recommendation.pdf:pdf},
isbn = {9780768550268},
journal = {Proceedings - IEEE 10th International Conference on Services Computing, SCC 2013},
keywords = {servcies discovery,service goal extraction,service goal recommendation},
pages = {65--72},
publisher = {IEEE},
title = {{Towards services discovery based on service goal extraction and recommendation}},
year = {2013}
}
@article{Jenkins2007,
abstract = {Service oriented chatbot systems are designed to help users access information from a website more easily. The system uses natural language responses to deliver the relevant information, acting like a customer service representative. In order to understand what users expect from such a system and how they interact with it we carried out two experiments which highlighted different aspects of interaction. We observed the communication between humans and the chatbots, and then between humans, applying the same methods in both cases. These findings have enabled us to focus on aspects of the system which directly affect the user, meaning that we can further develop a realistic and helpful chatbot. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Jenkins, Marie Claire and Churchill, Richard and Cox, Stephen and Smith, Dan},
doi = {10.1007/978-3-540-73110-8_9},
file = {:Users/baharehzarei/Downloads/Analysis-of-User-Interaction.pdf:pdf},
isbn = {9783540731085},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Chatbot,Communication,Dialogue,Human-computer interaction,Intelligent system,Natural language,Question-answering},
number = {PART 3},
pages = {76--83},
title = {{Analysis of user interaction with service oriented chatbot systems}},
volume = {4552 LNCS},
year = {2007}
}
@article{Aman2018,
abstract = {Automatic key concept extraction from text is the main challenging task in information extraction, information retrieval and digital libraries, ontology learning, and text analysis. The statistical frequency and topical graph-based ranking are the two kinds of potentially powerful and leading unsupervised approaches in this area, devised to address the problem. To utilize the potential of these approaches and improve key concept identification, a comprehensive performance analysis of these approaches on datasets from different domains is needed. The objective of the study presented in this paper is to perform a comprehensive empirical analysis of selected frequency and topical graph-based algorithms for key concept extraction on three different datasets, to identify the major sources of error in these approaches. For experimental analysis, we have selected TF-IDF, KP-Miner and TopicRank. Three major sources of error, i.e., frequency errors, syntactical errors and semantical errors, and the factors that contribute to these errors are identified. Analysis of the results reveals that performance of the selected approaches is significantly degraded by these errors. These findings can help us develop an intelligent solution for key concept extraction in the future.},
author = {Aman, Muhammad and Said, Abas bin Md and Kadir, Said Jadid Abdul and Ullah, Israr},
doi = {10.3390/info9050128},
file = {:Users/baharehzarei/Downloads/information-09-00128-v2.pdf:pdf},
issn = {20782489},
journal = {Information (Switzerland)},
keywords = {Empirical analysis,Information retrieval,Key concept extraction,Keyphrase extraction,Text mining},
number = {5},
title = {{Key concept identification: A comprehensive analysis of frequency and topical graph-based approaches}},
volume = {9},
year = {2018}
}
@article{Shams2010,
abstract = {The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM) Prototype. The prototype is domain-specific, the purpose of which is to map instructional text onto a knowledge domain. The context of the knowledge domain is DC electrical circuit. During development, the prototype has been tested with a limited data set from the domain. The prototype reached a stage where it needs to be evaluated with a representative linguistic data set called corpus. A corpus is a collection of text drawn from typical sources which can be used as a test data set to evaluate NLP systems. As there is no available corpus for the domain, we developed and annotated a representative corpus. The evaluation of the prototype considers two of its major components-lexical components and knowledge model. Evaluation on lexical components enriches the lexical resources of the prototype like vocabulary and grammar structures. This leads the prototype to parse a reasonable amount of sentences in the corpus. While dealing with the lexicon was straight forward, the identification and extraction of appropriate semantic relations was much more involved. It was necessary, therefore, to manually develop a conceptual structure for the domain to formulate a domain-specific framework of semantic relations. The framework of semantic relations-that has resulted from this study consisted of 55 relations, out of which 42 have inverse relations. We also conducted rhetorical analysis on the corpus to prove its representativeness in conveying semantic. Finally, we conducted a topical and discourse analysis on the corpus to analyze the coverage of discourse by the prototype. {\textcopyright} 2010 ACADEMY PUBLISHER.},
author = {Shams, Rushdi and Elsayed, Adel and Akter, Quazi Mah Zereen},
doi = {10.4304/jcp.5.1.69-80},
file = {:Users/baharehzarei/Downloads/1204.6364.pdf:pdf},
issn = {1796203X},
journal = {Journal of Computers},
keywords = {Conceptual structure,Corpus,Discourse analysis,Knowledge model,Knowledge representation,Lexical components,Ontology,Semantic relations,Topical analysis},
number = {1},
pages = {69--80},
title = {{A corpus-based evaluation of a domain-specific text to knowledge mapping prototype}},
volume = {5},
year = {2010}
}
@article{Mihalcea2004,
author = {Mihalcea, Rada and Tarau, Paul},
file = {:Users/baharehzarei/Downloads/TextRank{\_}Bringing{\_}Order{\_}into{\_}Text.pdf:pdf},
number = {July},
title = {{TextRank : Bringing Order into Texts 132547658 {\$} 9 @ 2BADC EF8 " GHEPI RTSVU ¨ Q WFX ` Y acb e fhgpi d XqYsrtb e 1u254 R 8 v v}},
year = {2004}
}
@article{Hulth2003,
abstract = {In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), ...},
author = {Hulth, Anette},
doi = {10.3115/1119355.1119383},
file = {:Users/baharehzarei/Downloads/Improved{\_}Automatic{\_}Keyword{\_}Extraction{\_}Given{\_}More{\_}L.pdf:pdf},
number = {April},
pages = {216--223},
title = {{Improved automatic keyword extraction given more linguistic knowledge}},
year = {2003}
}
@article{Hasan2010,
abstract = {State-of-the-art approaches for unsupervised keyphrase extraction are typically evaluated on a single dataset with a single parameter setting. Consequently, it is unclear how effective these approaches are on a new dataset from a different domain, and how sensitive they are to changes in parameter settings. To gain a better understanding of state-of-the-art unsupervised keyphrase extraction algorithms, we conduct a systematic evaluation and analysis of these algorithms on a variety of standard evaluation datasets.},
author = {Hasan, Kazi Saidul and Ng, Vincent},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hasan, Ng - 2010 - Conundrums in unsupervised keyphrase extraction Making sense of the state-of-the-art.pdf:pdf},
journal = {Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference},
number = {August},
pages = {365--373},
title = {{Conundrums in unsupervised keyphrase extraction: Making sense of the state-of-the-art}},
volume = {2},
year = {2010}
}
@article{We2004,
file = {:Users/baharehzarei/Downloads/sturation.pdf:pdf},
journal = {University of Lancaster.},
pages = {1--8},
title = {{Unit 2 Representativeness , balance and sampling 2}},
url = {http://www.lancaster.ac.uk/fass/projects/corpus/ZJU/xCBLS/chapters/A02.pdf},
year = {2009}
}
@article{Simon2018,
abstract = {Extracting key terms from technical documents allows us to write effective documentation that is specific and clear, with minimum ambiguity and confusion caused by nearly synonymous but different terms. For instance, in order to avoid confusion, the same object should not be referred to by two different names (e.g. “hydraulic oil filter”). In the modern world of commerce, clear terminology is the hallmark of successful RFPs (Requests for Proposal) and is therefore a key to the growth of competitive organizations. While Automatic Term Extraction (ATE) is a well-developed area of study, its applications in the technical domain have been sparse and constrained to certain narrow areas such as the biomedical research domain. We present a method for Automatic Term Extraction (ATE) for the technical domain based on the use of part-of-speech features and common words information. The method is evaluated on a C programming language reference manual as well as a manual of aircraft maintenance guidelines, and has shown comparable or better results to the reported state of the art results.},
author = {Simon, Nisha Ingrid and Ke{\v{s}}elj, Vlado},
doi = {10.1145/3209280.3229100},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Simon, Ke{\v{s}}elj - 2018 - Automatic term extraction in technical domain using part-of-speech and common-word features.pdf:pdf},
isbn = {9781450357692},
journal = {Proceedings of the ACM Symposium on Document Engineering 2018, DocEng 2018},
keywords = {Natural language processing,POS tagging,Terminology extraction,Text mining},
number = {May},
title = {{Automatic term extraction in technical domain using part-of-speech and common-word features}},
year = {2018}
}
@article{Chuang2012,
abstract = {Keyphrases aid the exploration of text collections by communicating salient aspects of documents and are often used to create effective visualizations of text. While prior work in HCI and visualization has proposed a variety of ways of presenting keyphrases, less attention has been paid to selecting the best descriptive terms. In this article, we investigate the statistical and linguistic properties of keyphrases chosen by human judges and determine which features are most predictive of high-quality descriptive phrases. Based on 5,611 responses from 69 graduate students describing a corpus of dissertation abstracts, we analyze characteristics of human-generated keyphrases, including phrase length, commonness, position, and part of speech. Next, we systematically assess the contribution of each feature within statistical models of keyphrase quality. We then introduce a method for grouping similar terms and varying the specificity of displayed phrases so that applications can select phrases dynamically based on the available screen space and current context of interaction. Precision-recall measures find that our technique generates keyphrases that match those selected by human judges. Crowdsourced ratings of tag cloud visualizations rank our approach above other automatic techniques. Finally, we discuss the role of HCI methods in developing new algorithmic techniques suitable for user-facing applications. {\textcopyright} 2012 ACM.},
author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
doi = {10.1145/2362364.2362367},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chuang, Manning, Heer - 2012 - Without the clutter of unimportant words Descriptive keyphrases for text visualization.pdf:pdf},
issn = {10730516},
journal = {ACM Transactions on Computer-Human Interaction},
keywords = {Interaction,Keyphrases,Visualization},
number = {3},
title = {{"Without the clutter of unimportant words": Descriptive keyphrases for text visualization}},
volume = {19},
year = {2012}
}
@misc{Fadhil2019,
abstract = {Building conversational agents have many technical, design and linguistic challenges. Other more complex elements include using emotionally intelligent conversational agent to build trust with the individuals. In this chapter, we introduce the nature of conversational user interfaces (CUIs) for health and describe UX design principles informed by a systematic literature review of relevant research works. We analyze scientific literature in conversational interfaces and chatterbots, providing a survey of major studies and describing UX design principles and interaction patterns.},
archivePrefix = {arXiv},
arxivId = {1902.09022},
author = {Fadhil, Ahmed and Schiavo, Gianluca},
booktitle = {arXiv},
eprint = {1902.09022},
issn = {23318422},
keywords = {Bots,Conversational UIs,Design Patterns,Dialogue Systems,Health,Wellbeing},
month = {feb},
publisher = {arXiv},
title = {{Designing for Health Chatbots}},
year = {2019}
}
@misc{,
title = {{Chatbots as conversational healthcare services{\_}Final | Enhanced Reader}},
url = {chrome-extension://dagcmkpagjlhakfdhnbomgmjdpkdklff/enhanced-reader.html?openApp{\&}pdf=https{\%}3A{\%}2F{\%}2Farxiv.org{\%}2Fpdf{\%}2F2011.03969.pdf},
urldate = {2021-03-31}
}
@article{Zhang2020,
author = {Zhang, Neng and Wang, Jian and Ma, Yutao},
file = {:Users/baharehzarei/Downloads/07896653 (1).pdf:pdf},
number = {3},
pages = {488--502},
publisher = {IEEE},
title = {{Mining Domain Knowledge on Service Goals from Textual Service Descriptions}},
volume = {13},
year = {2020}
}
@inproceedings{Rodr,
author = {Rodr, Rolando and Espinosa, Roberto and Bianchini, Devis and Garrig, Irene and Maz, Jose-norberto},
booktitle = {ICWE 2012 Workshops},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rodr et al. - 2012 - Extracting Models from Web API Documentation.pdf:pdf},
keywords = {discovery process,openapi,repository,rest web apis},
pages = {134--145},
publisher = {Springer-Verlag Berlin Heidelberg 2012},
title = {{Extracting Models from Web API Documentation}},
year = {2012}
}
@article{Pan2013,
author = {Pan, Odsd Jeff and Staab, Steffen and A{\ss}mann, Uwe and Ebert, J{\"{u}}rgen},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pan et al. - 2013 - Current Trends and Perspectives in Ontology-Driven Software Development ( ODSD ).pdf:pdf},
pages = {1--75},
title = {{Current Trends and Perspectives in Ontology-Driven Software Development ( ODSD )}},
year = {2013}
}
@article{Zhang2019,
author = {Zhang, Neng and Wang, Jian and He, Keqing and Li, Zheng and Huang, Yiwang},
doi = {10.1007/s10115-018-1171-4},
file = {:Users/baharehzarei/Downloads/Mining{\_}and{\_}clustering{\_}service{\_}goals{\_}for{\_}RESTful{\_}se.pdf:pdf},
issn = {0219-3116},
journal = {Knowledge and Information Systems},
keywords = {Service discovery,RESTful service,Service goal,Top},
number = {3},
pages = {669--700},
publisher = {Springer London},
title = {{Mining and clustering service goals for RESTful service discovery}},
url = {https://doi.org/10.1007/s10115-018-1171-4},
volume = {58},
year = {2019}
}
@article{Dubielewicz2015,
abstract = {The domain knowledge represented by ontology should be widely used in the design process of information system. The aim of the paper is to outline a systematic approach of developing a CIM model (domain model, precisely) on the basis of a selected domain ontology. There are presented some hints how ontology concepts can be expressed in domain model. Elaborated example realizes some difficulties in proposed approach, e.g. the domain knowledge is spread over many ontologies, some facts are defined at very general level (their interpretation is more difficult), ontology may contain many irrelevant elements. Nevertheless, we are believed that applying ontology in conscious way can help to achieve higher compliance of the domain model with the application domain.},
author = {Dubielewicz, Iwona and Hnatkowska, Bogumi{\l}a and Huzar, Zbigniew and Tuzinkiewicz, Lech},
doi = {10.1515/fcds-2015-0001},
file = {:Users/baharehzarei/Downloads/Domain{\_}Modeling{\_}in{\_}the{\_}Context{\_}of{\_}Ontology.pdf:pdf},
issn = {08676356},
journal = {Foundations of Computing and Decision Sciences},
keywords = {CIM,MDA,SUMO,UML,domain modelling,ontology},
number = {1},
pages = {3--15},
title = {{Domain modeling in the context of ontology}},
volume = {40},
year = {2015}
}
@article{Library2012b,
author = {Library, W P Widget},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Library - 2012 - Ict omelette(3).pdf:pdf},
pages = {1--15},
title = {{Ict omelette}},
year = {2012}
}
@article{Packagec,
author = {Package, Work and Report, Standardization and Mms, T-systems and Activities, Communication and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package et al. - Unknown - ICT OMELETTE Report D8 . 5 – Final Dissemination and Standardization Report.pdf:pdf},
pages = {1--51},
title = {{ICT OMELETTE Report D8 . 5 – Final Dissemination and Standardization Report}}
}
@article{Packaged,
author = {Package, Work and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Architecture - Unknown - Report(2).pdf:pdf},
pages = {1--29},
title = {{Report}}
}
@article{Packagea,
author = {Package, Work},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package - Unknown - Report Telco Mashup Architecture.pdf:pdf},
pages = {1--49},
title = {{Report Telco Mashup Architecture}}
}
@article{Packagee,
author = {Package, Work and Plan, Standardisation and Cooperation, Technological Platform and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package et al. - Unknown - ICT OMELETTE Report D1 . 1 Collaboration and Standardisation Plan.pdf:pdf},
pages = {1--25},
title = {{ICT OMELETTE Report D1 . 1 Collaboration and Standardisation Plan}}
}
@article{Package2013,
author = {Package, Work and Plan, Exploitation and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Plan, Architecture - 2013 - ICT OMELETTE Report D8 . 3 - Dissemination plan and initial Market Analysis.pdf:pdf},
number = {June},
pages = {1--32},
title = {{ICT OMELETTE Report D8 . 3 - Dissemination plan and initial Market Analysis}},
year = {2013}
}
@article{Package,
author = {Package, Work and Mms, T-systems},
file = {:Users/baharehzarei/Downloads/Deliverable 2.1.pdf:pdf},
journal = {Architecture},
pages = {1--84},
title = {{ICT OMELETTE Report D2 . 1 – State-of-the-art in the field of Mashup concepts}}
}
@article{Package2011,
author = {Package, Work},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package - 2011 - Report.pdf:pdf},
pages = {1--15},
title = {{Report}},
year = {2011}
}
@article{Library2012,
author = {Library, W P Widget},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Library - 2012 - Ict omelette.pdf:pdf},
pages = {1--15},
title = {{Ict omelette}},
year = {2012}
}
@article{Platform,
author = {Platform, W P Mashup},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Platform - Unknown - Report.pdf:pdf},
title = {{Report}}
}
@article{Packagef,
author = {Package, Work and Mms, T-systems and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Mms, Architecture - Unknown - ICT OMELETTE Report D5 . 2 - Initial discovery and composition report.pdf:pdf},
pages = {1--37},
title = {{ICT OMELETTE Report D5 . 2 - Initial discovery and composition report}}
}
@article{Packageb,
author = {Package, Work},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package - Unknown - ICT OMELETTE Final Specification of Mashup Description Language and Telco Mashup Architecture.pdf:pdf},
pages = {1--69},
title = {{ICT OMELETTE Final Specification of Mashup Description Language and Telco Mashup Architecture}}
}
@article{Packageg,
author = {Package, Work and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Architecture - Unknown - Report.pdf:pdf},
pages = {1--43},
title = {{Report}}
}
@article{Omelette,
author = {Omelette, I C T},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Omelette - Unknown - ICT OMELETTE Report D7.4 Evaluations of demonstrators reports Page 1 of 106.pdf:pdf},
pages = {1--106},
title = {{ICT OMELETTE Report D7.4 Evaluations of demonstrators reports Page 1 of 106}}
}
@article{Packageh,
author = {Package, Work and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Architecture - Unknown - ICT OMELETTE Report D5 . 3 – Final discovery and composition report.pdf:pdf},
pages = {1--56},
title = {{ICT OMELETTE Report D5 . 3 – Final discovery and composition report}}
}
@article{Library2012a,
author = {Library, W P Widget},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Library - 2012 - Ict omelette(2).pdf:pdf},
pages = {1--15},
title = {{Ict omelette}},
year = {2012}
}
@article{Packagei,
author = {Package, Work and Environments, Development and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Environments, Architecture - Unknown - ICT OMELETTE Report D4 . 1 – Initial technical and user Requirements and Architecture.pdf:pdf},
pages = {1--34},
title = {{ICT OMELETTE Report D4 . 1 – Initial technical and user Requirements and Architecture for Telco Mashup Development Environments}}
}
@article{Packagej,
author = {Package, Work and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package, Architecture - Unknown - Prototype.pdf:pdf},
pages = {1--43},
title = {{Prototype}}
}
@article{Packagek,
author = {Package, Work and Bolton, Uni and Studies, Case and Architecture, Software},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Package et al. - Unknown - ICT OMELETTE Demonstrator D7 . 1 – Definition of Selected Use Cases.pdf:pdf},
pages = {1--28},
title = {{ICT OMELETTE Demonstrator D7 . 1 – Definition of Selected Use Cases}}
}
@article{Liu2007a,
abstract = {Mashup is a hallmark of Web 2.0 and attracts both industry and academia recently. It refers to an ad hoc composition technology of Web applications that allows users to draw upon content retrieved from external data sources to create entirely new services. Compared to traditional "developer- centric" composition technologies, e.g., BPEL and WSCI, mashup provides a flexible and easy-of-use way for service composition on web. It makes the consumers free to compose services as they wish as well as simplifies the composition task. This paper makes two contributions. Firstly, we propose the mashup architecture, extend current SOA model with mashup and analyze how it facilitates service composition. Secondly, we propose a mashup component model to help developers leverage to create their own composite services. A case study is given to illustrate how to do service composition by mashup. This paper also discusses about some interesting topics about mashup. {\textcopyright} 2007 IEEE.},
author = {Liu, Xuanzhe and Hui, Yi and Sun, Wei and Liang, Haiqi},
doi = {10.1109/SERVICES.2007.67},
file = {:Users/baharehzarei/Downloads/Towards{\_}Service{\_}Composition{\_}Based{\_}on{\_}Mashup.pdf:pdf},
isbn = {0769529267},
issn = {1361-6528},
journal = {Proceedings - 2007 IEEE Congress on Services, SERVICES 2007},
number = {July},
pages = {332--339},
title = {{Towards service composition based on mashup}},
year = {2007}
}
@article{Lee2015,
abstract = {With the growing popularity of data mashups, the number of Web APIs has increased significantly. As a result, finding and composing the right APIs has become an increasingly complex task. Although several tools such as Yahoo's Pipes, IBM's Lotus Mashup, and Intel's Mashmaker have been developed to enable users to create data mashups without programming skills, there are several challenging issues when combining a large number of APIs into the data mashup. This paper proposes novel algorithms for the automatic discovery and composition of Web APIs. Our discovery algorithm adopts strategies that rapidly prune APIs that are guaranteed not to match the query. Our composition algorithm consists of constructing a composable similarity graph (CSG) and searching composition candidates. The CSG presents the semantic functional dependency between the inputs and the outputs of the Web APIs. Using this graph, we generate directed acyclic graphs (DAGs) that can produce the output satisfying the desired goal. We evaluate the algorithms on a real-world dataset from ProgrammableWeb.com, and show that they can produce the results satisfying the user's desired output.},
author = {Lee, Yong Ju},
doi = {10.6688/JISE.2015.31.4.5},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lee - 2015 - Semantic-based web API composition for data mashups.pdf:pdf},
issn = {10162364},
journal = {Journal of Information Science and Engineering},
keywords = {Composition,Data mashup,Discovery,Graph-based algorithm,Ontology learning,Web API},
number = {4},
pages = {1233--1248},
title = {{Semantic-based web API composition for data mashups}},
volume = {31},
year = {2015}
}
@book{Verborgh2016,
abstract = {Machine clients are increasingly making use of the Web to perform tasks. While Web services traditionally mimic remote procedure calling interfaces, a new generation of so-called hypermedia APIs works through hyperlinks and forms, in a way similar to how people browse the Web. This means that existing composition techniques, which determine a procedural plan upfront, are not sufficient to consume hypermedia APIs, which need to be navigated at runtime. Clients instead need a more dynamic plan that allows them to follow hyperlinks and use forms with a preset goal. Therefore, in this paper, we show how compositions of hypermedia APIs can be created by generic Semantic Web reasoners. This is achieved through the generation of a proof based on semantic descriptions of the APIs' functionality. To pragmatically verify the applicability of compositions, we introduce the notion of pre-execution and post-execution proofs. The runtime interaction between a client and a server is guided by proofs but driven by hypermedia, allowing the client to react to the application's actual state indicated by the server's response. We describe how to generate compositions from descriptions, discuss a computer-assisted process to generate descriptions, and verify reasoner performance on various composition tasks using a benchmark suite. The experimental results lead to the conclusion that proof-based consumption of hypermedia APIs is a feasible strategy at Web scale.},
archivePrefix = {arXiv},
arxivId = {1512.07780},
author = {Verborgh, Ruben and Arndt, D{\"{o}}rthe and {Van Hoecke}, Sofie and {De Roo}, Jos and Mels, Giovanni and Steiner, Thomas and Gabarro, Joaquim},
booktitle = {Theory and Practice of Logic Programming},
doi = {10.1017/S1471068416000016},
eprint = {1512.07780},
file = {:Users/baharehzarei/Downloads/the-pragmatic-proof-hypermedia-api-composition-and-execution.pdf:pdf},
isbn = {1471068416000},
issn = {14753081},
keywords = {Semantic Web,Web APIs,composition,hypermedia APIs,proof,reasoning},
number = {1},
pages = {1--48},
title = {{The pragmatic proof: Hypermedia API composition and execution}},
volume = {17},
year = {2016}
}
@article{Klusch2014,
author = {Klusch, Matthias},
doi = {10.1007/978-1-4614-6170-8_121},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Klusch - 2014 - Service Discovery.pdf:pdf},
journal = {Encyclopedia of Social Network Analysis and Mining},
pages = {1707--1717},
title = {{Service Discovery}},
year = {2014}
}
@article{Selvaretnam2012,
abstract = {The availability of an abundance of knowledge sources has spurred a large amount of effort in the development and enhancement of Information Retrieval techniques. Users' information needs are expressed in natural language and successful retrieval is very much dependent on the effective communication of the intended purpose. Natural language queries consist of multiple linguistic features which serve to represent the intended search goal. Linguistic characteristics that cause semantic ambiguity and misinterpretation of queries as well as additional factors such as the lack of familiarity with the search environment affect the users' ability to accurately represent their information needs, coined by the concept "intention gap". The latter directly affects the relevance of the returned search results which may not be to the users' satisfaction and therefore is a major issue impacting the effectiveness of information retrieval systems. Central to our discussion is the identification of the significant constituents that characterize the query intent and their enrichment through the addition of meaningful terms, phrases or even latent representations, either manually or automatically to capture their intended meaning. Specifically, we discuss techniques to achieve the enrichment and in particular those utilizing the information gathered from statistical processing of term dependencies within a document corpus or from external knowledge sources such as ontologies. We lay down the anatomy of a generic linguistic based query expansion framework and propose its module-based decomposition, covering topical issues from query processing, information retrieval, computational linguistics and ontology engineering. For each of the modules we review state-of-the-art solutions in the literature categorized and analyzed under the light of the techniques used. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Selvaretnam, Bhawani and Belkhatir, Mohammed},
doi = {10.1007/s10844-011-0174-3},
file = {:Users/baharehzarei/Downloads/Natural{\_}language{\_}technology{\_}and{\_}query{\_}ex.pdf:pdf},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Knowledge based processing,Linguistic analysis,Query expansion systems,Term dependency},
number = {3},
pages = {709--740},
title = {{Natural language technology and query expansion: Issues, state-of-the-art and perspectives}},
volume = {38},
year = {2012}
}
@article{doi:10.1162/coli\_a\_00368,
abstract = { This article describes the development of Microsoft XiaoIce, the most popular social chatbot in the world. XiaoIce is uniquely designed as an artifical intelligence companion with an emotional connection to satisfy the human need for communication, affection, and social belonging. We take into account both intelligent quotient and emotional quotient in system design, cast human–machine social chat as decision-making over Markov Decision Processes, and optimize XiaoIce for long-term user engagement, measured in expected Conversation-turns Per Session (CPS). We detail the system architecture and key components, including dialogue manager, core chat, skills, and an empathetic computing module. We show how XiaoIce dynamically recognizes human feelings and states, understands user intent, and responds to user needs throughout long conversations. Since the release in 2014, XiaoIce has communicated with over 660 million active users and succeeded in establishing long-term relationships with many of them. Analysis of large-scale online logs shows that XiaoIce has achieved an average CPS of 23, which is significantly higher than that of other chatbots and even human conversations. },
author = {Zhou, Li and Gao, Jianfeng and Li, Di and Shum, Heung-Yeung},
doi = {10.1162/coli_a_00368},
journal = {Computational Linguistics},
number = {1},
pages = {53--93},
title = {{The Design and Implementation of XiaoIce, an Empathetic Social Chatbot}},
url = {https://doi.org/10.1162/coli{\_}a{\_}00368},
volume = {46},
year = {2020}
}
@article{Candello2017,
abstract = {Multi-person conversations are an integral part of everyday life interactions, in meetings, family dinners, chats in bars, and in almost every collaborative or competitive environment including hospitals, offices, etc. The ability of human beings to organize, manage, and (mostly) make productive such complex interactive structures are nothing less than remarkable. The advent of social media platforms and messaging systems such as WhatsApp in the first 15 years of the 21st century expanded our ability as a society to also have asynchronous conversations in text form, from family and friends chat groups to national conversing in a highly distributed form in social media [3]. In this context, many technological advances in the early 2010s in natural language processing (spearheaded by the IBM Watson's victory in Jeopardy) spurred the availability in the early 2010s of text-based chatbots in websites and apps and spoken speech interfaces such as Siri and Cortana. However, the absolute majority of those chatbot deployments were in contexts of dyadic dialog, that is, a conversation between a single chatbot with a single user. Human-computer interaction, in practice, has also been mostly about dyadic interaction since the dawn of computer systems in the 1950s. Notice that the two dominant interaction paradigms, command-line and point-and-click, are neither well suited for either multi-user interaction (one application with more than one user engaged in the same activity); nor multi-app interaction (one user interacting seamlessly with more than one application); and even less for generic multiparty applications (many users and many bots simultaneously). Notably, exceptions are surface interaction and multi-user games, but the mainstream of human-computer interaction remains one-to-one.In this sense, conversational interfaces powered by chatbots are an important breakthrough from the past of computer interaction because they naturally enable multiparty applications. By exploiting the many social protocols human had developed for multi-person conversations since the advent of language, conversation-based interfaces may finally break from the dyadic paradigm in computer interaction. In this presentation, we consider a scenario of wealth management where advice is provided by multiple chatbots. We highlight a preliminary study undertaken with potential users with limited financial knowledge to understand their experience when using a multi-bot conversational system. APPROACH ADOPTED},
author = {Candello, Heloisa and Brazil, S{\~{a}}o Paulo and Vasconcelos, Marisa and Paulo, S{\~{a}}o and Pinhanez, Claudio and Paulo, S{\~{a}}o},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Candello et al. - 2017 - Evaluating the conversation flow and content quality of a multi-bot conversational system.pdf:pdf},
isbn = {9788576694069},
journal = {Extended Proceedings of the 16th Brazilian Symposium on Human Factors in Computing Systems.},
number = {October},
pages = {60--61},
title = {{Evaluating the conversation flow and content quality of a multi-bot conversational system}},
volume = {9},
year = {2017}
}
@article{Jin2018,
abstract = {In order to solve the problem of poor universality and the absence of contextual information in word similarity calculation based on dictionary, this paper proposes a semantic similarity computation method based on Word2vec. This method improves HowNet and Tongyici Cilin, and also adds the word vector model as a weighing parameter to calculate the word similarity, after compares the similarity of the words by assigning different weights to the three methods. Through experimental comparison, the Pearson coefficient of the algorithm and the artificial value is 0. 892, and the method can cover most words so that it can effectively solve the problem of the similarity of the word calculation in the dictionary.},
author = {Jin, Xiaolin and Zhang, Shuwu and Liu, Jie},
doi = {10.1109/ICCAIS.2018.8570612},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jin, Zhang, Liu - 2018 - Word Semantic Similarity Calculation Based on Word2vec.pdf:pdf},
isbn = {9781538660201},
journal = {ICCAIS 2018 - 7th International Conference on Control, Automation and Information Sciences},
keywords = {HowNet,Tongyici Cilin,Word similarity calculation,Word2vec},
pages = {12--16},
publisher = {IEEE},
title = {{Word Semantic Similarity Calculation Based on Word2vec}},
year = {2018}
}
@article{Bapat2018,
abstract = {Chatbots are text-based conversational agents. Natural Language Understanding (NLU) models are used to extract meaning and intention from user messages sent to chatbots. The user experience of chatbots largely depends on the performance of the NLU model, which itself largely depends on the initial dataset the model is trained with. The training data should cover the diversity of real user requests the chatbot will receive. Obtaining such data is a challenging task even for big corporations. We introduce a generic approach to generate training data with the help of crowd workers, we discuss the approach workflow and the design of crowdsourcing tasks assuring high quality. We evaluate the approach by running an experiment collecting data for 9 different intents. We use the collected training data to train a natural language understanding model. We analyse the performance of the model under different training set sizes for each intent. We provide recommendations on selecting an optimal confidence threshold for predicting intents, based on the cost model of incorrect and unknown predictions.},
author = {Bapat, Rucha and Kucherbaev, Pavel and Bozzon, Alessandro},
doi = {10.1007/978-3-319-91662-0_8},
file = {:Users/baharehzarei/Downloads/paper61{\_}ICWE2018{\_}NLU.pdf:pdf},
isbn = {9783319916613},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Conversational agents,Crowdsourcing,Natural language understanding},
pages = {114--128},
title = {{Effective crowdsourced generation of training data for chatbots natural language understanding}},
volume = {10845 LNCS},
year = {2018}
}
@inproceedings{Bapat2018a,
abstract = {Chatbots are text-based conversational agents. Natural Language Understanding (NLU) models are used to extract meaning and intention from user messages sent to chatbots. The user experience of chatbots largely depends on the performance of the NLU model, which itself largely depends on the initial dataset the model is trained with. The training data should cover the diversity of real user requests the chatbot will receive. Obtaining such data is a challenging task even for big corporations. We introduce a generic approach to generate training data with the help of crowd workers, we discuss the approach workflow and the design of crowdsourcing tasks assuring high quality. We evaluate the approach by running an experiment collecting data for 9 different intents. We use the collected training data to train a natural language understanding model. We analyse the performance of the model under different training set sizes for each intent. We provide recommendations on selecting an optimal confidence threshold for predicting intents, based on the cost model of incorrect and unknown predictions.},
author = {Bapat, Rucha and Kucherbaev, Pavel and Bozzon, Alessandro},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-91662-0_8},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bapat, Kucherbaev, Bozzon - 2018 - Effective crowdsourced generation of training data for chatbots natural language understanding.pdf:pdf},
isbn = {9783319916613},
issn = {16113349},
keywords = {Conversational agents,Crowdsourcing,Natural language understanding},
pages = {114--128},
publisher = {Springer Verlag},
title = {{Effective crowdsourced generation of training data for chatbots natural language understanding}},
url = {https://doi.org/10.1007/978-3-319-91662-0{\_}8},
volume = {10845 LNCS},
year = {2018}
}
@article{Bano2014,
abstract = {The focus of Service Oriented Software Development (SOSD) is to develop software by integrating reusable services to lower the required cost, time and effort of development and increase reusability, agility, quality and customer satisfaction. It has been recognised in the literature that SOSD faces various challenges especially in requirements engineering (RE). The objective of this study is to investigate these challenges of Service Oriented RE (SORE) from practitioners' perspectives in order to gain a deeper understanding of the related issues and to reveal potential gaps between research and practice in SORE. They present a qualitative study of the challenges and issues in SORE. The data were collected by conducting interviews with practitioners working in IT companies in Sydney, who have had substantial experience with service oriented software projects. The authors findings reveal that most of the challenges of SORE are similar to those that are faced during RE in traditional or component-based software development. According to the practitioners, the research and practice has made some advances in the technical direction but the human related issues in SORE have not been addressed adequately. {\textcopyright} The Institution of Engineering and Technology 2014.},
author = {Bano, Muneera and Zowghi, Didar and Ikram, Naveed and Niazi, Mahmood},
doi = {10.1049/iet-sen.2013.0131},
file = {:Users/baharehzarei/Downloads/IETSoftware2014.pdf:pdf},
issn = {17518806},
journal = {IET Software},
number = {4},
pages = {154--160},
title = {{What makes service oriented requirements engineering challenging? A qualitative study}},
volume = {8},
year = {2014}
}
@article{He2017,
abstract = {With the fast growth of applications of service-oriented architecture (SOA) in software engineering, there has been a rapid increase in demand for building service-based systems (SBSs) by composing existing Web services. Finding appropriate component services to compose is a key step in the SBS engineering process. Existing approaches require that system engineers have detailed knowledge of SOA techniques which is often too demanding. To address this issue, we propose Keyword Search for Service-based Systems (KS3), a novel approach that integrates and automates the system planning, service discovery and service selection operations for building SBSs based on keyword search. KS3 assists system engineers without detailed knowledge of SOA techniques in searching for component services to build SBSs by typing a few keywords that represent the tasks of the SBSs with quality constraints and optimisation goals for system quality, e.g., reliability, throughput and cost. KS3 offers a new paradigm for SBS engineering that can significantly save the time and effort during the system engineering process. We conducted large-scale experiments using two real-world Web service datasets to demonstrate the practicality, effectiveness and efficiency of KS3.},
author = {He, Qiang and Zhou, Rui and Zhang, Xuyun and Wang, Yanchun and Ye, Dayong and Chen, Feifei and Grundy, John C. and Yang, Yun},
doi = {10.1109/TSE.2016.2624293},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - Keyword Search for Building Service-Based Systems.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Service-based system,cloud computing,keyword search,quality of service,service composition,web service},
number = {7},
pages = {658--674},
publisher = {IEEE},
title = {{Keyword Search for Building Service-Based Systems}},
volume = {43},
year = {2017}
}
@article{Jonquet2006,
abstract = {The goal of this position paper is to reflect about the concept of service in Informatics. In particular, we present in this paper the concept of dynamic service generation as a different way to provide services in computermediated contexts: services are dynamically constructed and provided (generated) by agents (human or artificial) within a community, by means of a conversation. This process allows services to be more accurate, precise, customized and personalized to satisfy a non predetermined need or wish. The paper presents an overview of the concept of service from philosophy to computer science (service oriented computing). A strict comparison with the current popular approach, called product delivery, is done. The main result emerging by these reflections is a list of �characteristics� of dynamic service generation, in order to promote a progressive transition from product delivery to dynamic service generation systems by transforming one-by-one the outlined characteristics into requirements and specifications. More specifically, two major characteristics are precisely described in the paper as they imply 80{\%} of the other ones. They promote a substitution of an agent oriented kernel to the current object oriented kernel of services as well as the Grid as the service oriented architecture and infrastructure for service exchanges between agents.},
author = {Jonquet, Clement and Cerri, Stefano A},
file = {:Users/baharehzarei/Downloads/Characterization{\_}of{\_}the{\_}Dynamic{\_}Service{\_}Generation.pdf:pdf},
keywords = {Agent,Dynamic Service Generation,Grid Service,Service,Service Oriented Computing/Architecture,Web service},
number = {06007},
title = {{Characterization of the Dynamic Service Generation concept}},
url = {http://www.lirmm.fr/{~}jonquet/publications/documents/RR-LIRMM-06007-Jonquet-feb2006.pdf},
year = {2006}
}
@inproceedings{Benlakhal2012,
abstract = {With the wide adoption of service-oriented architectures, many service-based systems (SBSs) and mashups composed of Web services have been developed. Meanwhile, web service discovery, as a crucial component to guarantee the quality of Web service composition in SBS, gained increasing attention. Web service discovery aims to select the set of best service candidates according to the service requester's requirements. Existing syntactic and semantic-based approaches are proven to have low recall and precision or expecting high level of expertise from the service requesters. In this paper, we propose an approach for Web service discovery based on chatbots which obtains the best suited set of services to address the goals of service requesters. Chatbot-based interfaces are the emerging trend to provide a more natural human-device interaction. Compared to the existing web service discovery techniques, our approach alleviates the need for service requesters to have prior knowledge about the service location or its structure and enables them to identify services by leveraging natural language. We evaluated the approach through the collection of 56 queries from the test subject group on our repository of existing web service descriptions and report on quality metrics. The results shows high precision and recall.},
author = {Zarei, Bahareh and Noura, Mahda and Gaedke, Martin},
booktitle = {19th International Conference on WWW/Internet 2020 A},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zarei, Noura, Gaedke - 2020 - A BOT-ENABLED APPROACH FOR WEB SERVICE DISCOVERY.pdf:pdf},
isbn = {9789898704238},
issn = {18770509},
keywords = {Chat-Bot,Conversational interfaces,Natural Language Processing,Service-Based Systems,Web Service Discovery},
pages = {3--10},
title = {{A BOT-ENABLED APPROACH FOR WEB SERVICE DISCOVERY}},
year = {2020}
}
@article{Zhang2020a,
abstract = {With the rapid development of service-oriented computing, a large number of software applications have been developed based on the services computing framework. It is well known that software engineering is a knowledge-intensive activity, and thus the effective management of service-related knowledge facilitates service-oriented software development. Although many methodologies have been proposed for service-oriented knowledge management, little attention has been paid to mining knowledge (especially domain-specific functionalities) from service resources. To address this issue, we propose an approach to mine domain knowledge on service goals (i.e., service functionalities) from textual descriptions of services. The approach consists of two components: service goal extraction from textual service descriptions based on linguistic analysis and domain service goal construction that merges semantically similar service goals within a domain. The effectiveness of the proposed approach is validated by a series of experiments conducted on a real-world dataset crawled from the ProgrammableWeb.},
author = {Zhang, Neng and Wang, Jian and Ma, Yutao},
doi = {10.1109/TSC.2017.2693147},
file = {:Users/baharehzarei/Downloads/07896653.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Service-oriented software development,domain service goal,knowledge mining},
number = {3},
pages = {488--502},
publisher = {IEEE},
title = {{Mining Domain Knowledge on Service Goals from Textual Service Descriptions}},
volume = {13},
year = {2020}
}
@article{Chowdhury,
author = {Chowdhury, Soudip Roy and Birukou, Aliaksandr and Daniel, Florian and Casati, Fabio},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chowdhury et al. - Unknown - Composition Patterns in Data Flow Based Mashups.pdf:pdf},
keywords = {composition pattern,data mining,end-user development,mashup},
title = {{Composition Patterns in Data Flow Based Mashups}}
}
@article{Wang2016,
abstract = {As research topics become increasingly complex, large scale interdisciplinary research projects are commonly established to foster cross-disciplinary cooperation and to utilize potential synergies. In the case of the Collaborative Research Center (CRC) 1026, 19 individual projects from different disciplines are brought together to investigate perspectives and solutions for sustainable manufacturing. Beside overheads regarding the coordination of activities and communication, such interdisciplinary projects are also facing challengs regarding data management. For exchange and combination of research results, data from individual projects have to be stored systematically, categorized, and linked according to the logical interrelations of the involved disciplinary knowledge domains. In the CRC 1026, the project for information infrastructure observed and analysed collaboration practices and developed IT-supported solutions to facilitate and foster research collaboration. Data management measures in this period were mainly focused on building a shared conceptual framework, and the organization of task related data. For the former aspect, an ontology basesd apporach was developed and prototypically implemented. For the latter aspect, a message board integrated task management system was developed and applied.},
author = {Wang, Wei Min and G{\"{o}}pfert, Tobias and Stark, Rainer},
doi = {10.3390/ijgi5040041},
file = {:Users/baharehzarei/Downloads/Data{\_}Management{\_}in{\_}Collaborative{\_}Interdisciplinary.pdf:pdf},
issn = {22209964},
journal = {ISPRS International Journal of Geo-Information},
keywords = {Collaborative research,Data management,Interdisciplinary research,Ontology,Semantic matching},
number = {4},
title = {{Data management in collaborative interdisciplinary research projects-conclusions from the digitalization of research in sustainable manufacturing}},
volume = {5},
year = {2016}
}
@article{Syu2014,
abstract = {In recent years, developing needed software applications via the technique Web Service Composition (WSC) has been more and more popular. Moreover, benefit from the Semantic Web Services (SWSs) technology, it is possible to even automatically conduct WSC, i.e. the Automated Web Service composition (AWSC). Currently the AWSC is a well-studied research subject and which means the existence of a large number of related research efforts. To existing AWSC researches, our goal is to make complete and referable surveys of them, and in this paper we adopt a strategy which may be more efficient than directly reviewing original research papers that we inspect and focus on the already-published AWSC surveys, trying to take advantages of them. With an AWSC survey framework proposed by us previously, we present a modest review on the selected AWSC surveys which inspected the AWSC researches that largely benefit from the SWSs technology. For each selected AWSC survey, in this review we indicate what AWSC research concerns defined by us in the survey framework are covered by it and precisely describe its contents. With this review, the reader can easily and quickly find out proper AWSC surveys for more advanced information and reading. {\textcopyright} 2014 IEEE.},
author = {Syu, Yang and Fanjiang, Yong Yi and Kuo, Jong Yih and Ma, Shang Pin},
doi = {10.1109/ICSC.2014.41},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Syu et al. - 2014 - A review of the automatic web service composition surveys.pdf:pdf},
isbn = {9781479940028},
journal = {Proceedings - 2014 IEEE International Conference on Semantic Computing, ICSC 2014},
keywords = {Aspect-Oriented Paradigm,Automatic Service Composition,Semantic Web Service,Service-Oriented Architecture},
pages = {199--202},
publisher = {IEEE},
title = {{A review of the automatic web service composition surveys}},
year = {2014}
}
@article{Ma2013,
abstract = {Web service discovery is the process of locating web services to satisfy the requirements of service requesters, and as such, plays an important role to realize business-to-business interoperability. Today, many service discovery mechanisms are available, roughly classified into two categories: text-based service retrieval and ontology-based service matching. However, both kinds of service discovery approaches have their limitations in finding out suitable web services for users. Text-based approaches cannot reflect the semantics of the service capability. Ontology-based approaches are likely infeasible and non-scalable since service providers need to spend considerable efforts to annotate the service descriptions. This study proposes an approach to searching for web services, called LS3 (Lexical and Semantic Service Search) architecture. This proposed approach includes two main sub-processes: query expansion and service ranking to enable the retrieval of relevant web services during the discovery process by considering lexical similarity and semantic similarity. {\textcopyright} 2013 IEEE.},
author = {Ma, Shang Pin and Li, Chia Hsueh and Tsai, Yao Yu and Lan, Ci Wei},
doi = {10.1109/ICEBE.2013.65},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ma et al. - 2013 - Web service discovery using lexical and semantic query expansion.pdf:pdf},
isbn = {9780769551111},
journal = {Proceedings - 2013 IEEE 10th International Conference on e-Business Engineering, ICEBE 2013},
keywords = {Lexical and semantic query expansion,Ontology mapping,Service discovery,Service search},
pages = {423--428},
publisher = {IEEE},
title = {{Web service discovery using lexical and semantic query expansion}},
year = {2013}
}
@article{Kaufer2006,
abstract = {In this paper, we present an approach to hybrid semantic web service matching based on both logic programming, and syntactic similarity measurement. The implemented matchmaker, called WSMO-MX, applies different matching filters to retrieve WSMO-oriented service descriptions that are semantically relevant to a given query with respect to seven degrees of hybrid matching. These degrees are recursively computed by aggregated valuations of ontology based type matching, logical constraint and relation matching, and syntactic similarity as well. {\textcopyright} 2006 IEEE.},
author = {Kaufer, Frank and Klusch, Matthias},
doi = {10.1109/ECOWS.2006.39},
file = {:Users/baharehzarei/Downloads/04031160.pdf:pdf},
isbn = {076952737X},
journal = {Proceedings of ECOWS 2006: Fourth European Conference on Web Services},
pages = {161--170},
title = {{WSMO-MX: A logic programming based hybrid service matchmaker}},
year = {2006}
}
@inbook{Klusch2018,
address = {New York, NY},
author = {Klusch, Matthias},
booktitle = {Encyclopedia of Social Network Analysis and Mining},
doi = {10.1007/978-1-4939-7131-2_121},
editor = {Alhajj, Reda and Rokne, Jon},
isbn = {978-1-4939-7131-2},
pages = {2474--2484},
publisher = {Springer New York},
title = {{Service Discovery}},
url = {https://doi.org/10.1007/978-1-4939-7131-2{\_}121},
year = {2018}
}
@article{Su2017,
abstract = {As the Web evolves towards a service-oriented architecture, application program interfaces (APIs) are becoming an increasingly important way to provide access to data, services, and devices. We study the problem of natural language interface to APIs (NL2APIs), with a focus on web APIs for web services. Such NL2APIs have many potential beneits, for example, facilitating the integration of web services into virtual assistants. We propose the irst end-to-end framework to build an NL2API for a given web API. A key challenge is to collect training data, i.e., NL command-API call pairs, from which an NL2API can learn the semantic mapping from ambiguous, informal NL commands to formal API calls. We propose a novel approach to collect training data for NL2API via crowdsourcing, where crowd workers are employed to generate diversiied NL commands. We optimize the crowdsourcing process to further reduce the cost. More speciically, we propose a novel hierarchical probabilistic model for the crowdsourcing process, which guides us to allocate budget to those API calls that have a high value for training NL2APIs. We apply our framework to real-world APIs, and show that it can collect high-quality training data at a low cost, and build NL2APIs with good performance from scratch. We also show that our modeling of the crowdsourcing process can improve its efectiveness, such that the training data collected via our approach leads to better performance of NL2APIs than a strong baseline.},
author = {Su, Yu and Awadallah, Ahmed Hassan and Khabsa, Madian and Pantel, Patrick and Gamon, Michael and Encarnacion, Mark},
doi = {10.1145/3132847.3133009},
file = {:Users/baharehzarei/Downloads/cikm17{\_}nl2api.pdf:pdf},
isbn = {9781450349185},
journal = {International Conference on Information and Knowledge Management, Proceedings},
keywords = {Crowdsourcing,Hierarchical probabilistic model,Natural language interface,Web API},
pages = {177--186},
title = {{Building natural language interfaces to Web APIs}},
volume = {Part F1318},
year = {2017}
}
@inproceedings{Mohebbi2010,
abstract = {Currently, most enterprises deploy their services on the Web. This augments the request for tools to perform discovery, selection, composition and invocation of Web services. Among them, Web service discovery should be considered more important. Along with the growing number of available Web services, there is a need for tools not only to perform discovery, but also to realize them in an efficient and effective manner. A number of approaches to Web service discovery have been proposed. In this paper, we provide a taxonomy which categorizes Web service discovery systems from different points of view. Moreover, current approaches to Semantic Web service discovery are classified and described. In addition, we compare the approaches with respect to some criteria from different aspects of view. The results of this study can help researchers in both academia and industry to implement a new or to select the most appropriate existing approach for Semantic Web service discovery with the aid of different criteria. Copyright 2010 ACM.},
address = {Paris, France},
author = {Mohebbi, Keyvan and Ibrahim, Suhaimi and Khezrian, Mojtaba and Munusamy, Kanmani and Tabatabaei, Sayed Gholam Hassan},
booktitle = {iiWAS2010 - 12th International Conference on Information Integration and Web-Based Applications and Services},
doi = {10.1145/1967486.1967496},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mohebbi et al. - 2010 - A comparative evaluation of Semantic Web service discovery approaches.pdf:pdf},
isbn = {9781450304214},
keywords = {Semantic Web,Semantic Web services,Web service discovery,Web services},
pages = {33--39},
title = {{A comparative evaluation of Semantic Web service discovery approaches}},
year = {2010}
}
@article{Qi,
author = {Qi, Peng and Dozat, Timothy and Zhang, Yuhao and Manning, Christopher D},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Qi et al. - Unknown - Universal Dependency Parsing from Scratch.pdf:pdf},
title = {{Universal Dependency Parsing from Scratch}}
}
@article{Stollberg2007,
author = {Stollberg, Michael and Keller, Uwe and Lausen, Holger and Heymans, Stijn},
doi = {10.1007/978-3-540-72667-8},
file = {:Users/baharehzarei/Downloads/Two-Phase{\_}Web{\_}Service{\_}Discovery{\_}Based{\_}on{\_}Rich{\_}Func.pdf:pdf},
isbn = {9783540726678},
journal = {4th European Semantic Web Conference},
number = {May},
title = {{4th European Semantic Web Conference}},
volume = {4519},
year = {2007}
}
@inproceedings{Baez2020,
abstract = {This paper lays the foundation for a new delivery paradigm for web-accessible content and functionality, i.e., conversational interaction. Instead of asking users to read text, click through links and type on the keyboard, the vision is to enable users to “speak to a website” and to obtain natural language, spoken feedback. The paper describes how state-of-the-art chatbot technology can enable a dialog between the user and the website, proposes a reference architecture for the automated inference of site-specific chatbots able to mediate between the user and the website, and discusses open challenges and research questions. The envisioned, bidirectional dialog paradigm advances current screen reader technology and aims to benefit both regular users in eyes-free usage scenarios as well as visually impaired users in everyday scenarios.},
author = {Baez, Marcos and Daniel, Florian and Casati, Fabio},
booktitle = {The 3rd International Workshop on Chatbot Research (Conversations 2019)},
doi = {10.1007/978-3-030-39540-7_7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Baez, Daniel - 2019 - Conversational Web Interaction Proposal of a Dialog-Based Natural Language Interaction Paradigm for the Web Conve.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Baez, Daniel - 2019 - Conversational Web Interaction Proposal of a Dialog-Based Natural Language Interaction Paradigm for the Web Con(2).pdf:pdf;:Users/baharehzarei/Downloads/baez-conversations2019.pdf:pdf},
isbn = {9783030395391},
issn = {16113349},
keywords = {Conversational agents,Non-visual browsing,Screen readers},
number = {October},
pages = {94--110},
publisher = {Springer, Cham},
title = {{Conversational Web Interaction: Proposal of a Dialog-Based Natural Language Interaction Paradigm for the Web}},
volume = {11970 LNCS},
year = {2020}
}
@techreport{DavidYoffie2018,
author = {{David Yoffie}, Professor B and Wu, Liang and Sweitzer, Jodie and Eden, Denzil and Ahuja, Karan},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/David Yoffie et al. - 2018 - (No Title).pdf:pdf},
institution = {Harvard Business school},
title = {{Voice War: Hey Google vs. Alexa vs. Siri}},
url = {www.hbsp.harvard.edu.},
year = {2018}
}
@article{Aznag2013,
abstract = {This paper shows that the problem of web services representation is crucial and analyzes the various factors that influence on it. It presents the traditional representation of web services considering traditional textual descriptions based on the information contained in WSDL files. Unfortunately, textual web services descriptions are dirty and need significant cleaning to keep only useful information. To deal with this problem, we introduce rules based text tagging method, which allows filtering web service description to keep only significant information. A new representation based on such filtered data is then introduced. Many web services have empty descriptions. Also, we consider web services representations based on the WSDL file structure (types, attributes, etc.). Alternatively, we introduce a new representation called symbolic reputation, which is computed from relationships between web services. The impact of the use of these representations on web service discovery and recommendation is studied and discussed in the experimentation using real world web services.},
author = {Aznag, Mustapha and Quafafou, Mohamed and Durand, Nicolas and Jarir, Zahi},
doi = {10.5121/ijwsc.2013.4101},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aznag et al. - 2013 - WEB SERVICES DISCOVERY AND RECOMMENDATION BASED ON INFORMATION EXTRACTION AND SYMBOLIC REPUTATION.pdf:pdf},
journal = {International Journal on Web Service Computing (IJWSC)},
keywords = {WSDL file,Web services,data representation,discovery and recommendation,information extraction,semantic tagging,symbolic reputation},
number = {1},
title = {{WEB SERVICES DISCOVERY AND RECOMMENDATION BASED ON INFORMATION EXTRACTION AND SYMBOLIC REPUTATION}},
url = {http://axis.apache.org/axis/},
volume = {4},
year = {2013}
}
@article{Arunachalam2013,
abstract = {Enterprise service architecture creates an IT environment where standardized components can aggregate and work together to reduce complexity. Service Discovery mechanism is used for discovering one or more documents that describes a particular service. Since there is no clear understanding regarding the behavior of service, ontology is used for providing appropriate service to the end user. Ontology is a generic knowledge that represents agreed domain semantics that can be reused by different kinds of applications or tasks. This is an efficient method to provide clear notion of service. This paper proposes a generic service discovery framework for an enterprise using ontology and a knowledge based agent to get a clear view about the service provided and its functionality.},
author = {Arunachalam, N and Swarnalakshmi, R and Sangeetha, R and Pradheepa, B},
file = {:Users/baharehzarei/Downloads/AN{\_}ONTOLOGY-BASED{\_}SERVICE{\_}DISCOVERY{\_}FRAM.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Arunachalam et al. - 2013 - an Ontology-Based Service Discovery Framework for an Enterprise.pdf:pdf},
issn = {2321-8363},
journal = {International Journal of Computer Science and Mobile Applications},
keywords = {Ontology,Semantic web and Service Description,Service Discovery,– Enterprise Services},
number = {5},
pages = {65--75},
title = {{an Ontology-Based Service Discovery Framework for an Enterprise}},
volume = {1},
year = {2013}
}
@techreport{DeMarneffe2008,
abstract = {and the discussion of the enhanced and enhanced++ UD representations by Schuster and Manning (2016).},
author = {{De Marneffe}, Marie-Catherine and Manning, Christopher D},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/De Marneffe, Manning - 2008 - Stanford typed dependencies manual.pdf:pdf},
title = {{Stanford typed dependencies manual}},
url = {http:/www.universaldependencies.org},
year = {2008}
}
@article{Abdul-Kader2015,
author = {Abdul-Kader, Sameera A and Woods, John},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Abdul-Kader, Woods - 2015 - Survey on Chatbot Design Techniques in Speech Conversation Systems.pdf:pdf},
journal = {(IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {AIML,Chatbot,Loebner Prize,NLP,NLTK,SQL,Turing Test},
number = {7},
pages = {72--80},
title = {{Survey on Chatbot Design Techniques in Speech Conversation Systems}},
url = {www.ijacsa.thesai.org},
volume = {6},
year = {2015}
}
@techreport{EzeizaAlvarez2017,
author = {{Ezeiza Alvarez}, Jon and Bast, Hannah},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ezeiza Alvarez, Bast - 2017 - A review of word embedding and document similarity algorithms applied to academic text.pdf:pdf},
title = {{A review of word embedding and document similarity algorithms applied to academic text}},
year = {2017}
}
@techreport{Le,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
author = {Le, Quoc and Mikolov, Tomas},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Le, Mikolov - Unknown - Distributed Representations of Sentences and Documents.pdf:pdf},
title = {{Distributed Representations of Sentences and Documents}}
}
@inproceedings{Fuckner2013,
address = {Aachen},
author = {Fuckner, M{\'{a}}rcio and Barth{\`{e}}s, Jean-Paul and Scalabrin, Edson Emilio},
booktitle = {WEBIST 2013: Web Information Systems and Technologies},
doi = {10.1007/978-3-662-44300-2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fuckner, Barth{\`{e}}s, Scalabrin - 2013 - Web Service Discovery and Execution Using a Dialog-Based Approach.pdf:pdf},
isbn = {9783662442999},
issn = {18651348},
keywords = {web services},
pages = {103--118},
title = {{Web Service Discovery and Execution Using a Dialog-Based Approach}},
volume = {189},
year = {2013}
}
@article{Krempels,
abstract = {Several Semantic Web techniques applied in Service-oriented architectures enable explicit representation and reasoning on service operations. Those techniques applied to the automatic discovery, selection and execution of services are promising but still too complex to allow a large-scale adoption. Consequently, the most widespread approach for upgrading existing services to a semantic level is the usage of traditional software engineering practices, resulting in complex and non-intuitive interfaces in some cases. We propose an approach to leverage the service discovery, selection and execution processes for Web services without semantic annotations, using only the WSDL descriptor as a source. In order to do so, we first identify candidates based on linguistic cues extracted from the Web service descriptor. The generated proof-of-concept allows users to select and execute service operations through a personal assistant using restricted requests in natural language.},
author = {Krempels, Karl Heinz and Stocker, Alexander},
doi = {10.1007/978-3-662-44300-2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fuckner, Barth{\`{e}}s, Scalabrin - Unknown - Web Service Discovery and Execution Using a Dialog-Based Approach.pdf:pdf},
keywords = {Natural language {\textperiodcentered},Personal assistants,Web services {\textperiodcentered}},
title = {{Web Information Systems and Technologies}}
}
@inproceedings{Raghuvanshi2018,
abstract = {We demonstrate an end-to-end approach for building conversational interfaces from prototype to production that has proven to work well for a number of applications across diverse verticals. Our architecture improves on the standard domain-intent-entity classification hierarchy and dialogue management architecture by leveraging shallow semantic parsing. We observe that NLU systems for industry applications often require more struc-tured representations of entity relations than provided by the standard hierarchy, yet without requiring full semantic parses which are often inaccurate on real-world conversational data. We distinguish two kinds of semantic properties that can be provided through shallow semantic parsing: entity groups and entity roles. We also provide live demos of conversational apps built for two different use cases: food ordering and meeting control.},
address = {Brussels},
author = {Raghuvanshi, Arushi and Carroll, Lucien and Raghunathan, Karthik},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Raghuvanshi, Carroll, Raghunathan - Unknown - Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing.pdf:pdf},
pages = {157--162},
title = {{Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing}},
year = {2018}
}
@article{Zhang2018,
author = {Zhang, Neng and Wang, Jian and Ma, Yutao and He, Keqing and Li, Zheng and Frank, Xiaoqing},
doi = {10.1016/j.jss.2018.04.046},
file = {:Users/baharehzarei/Downloads/1-s2.0-S0164121218300748-main.pdf:pdf},
issn = {0164-1212},
journal = {The Journal of Systems {\&} Software},
keywords = {Service discovery,Service-based system,Web service},
pages = {73--91},
publisher = {Elsevier Inc.},
title = {{Web service discovery based on goal-oriented query expansion}},
url = {https://doi.org/10.1016/j.jss.2018.04.046},
volume = {142},
year = {2018}
}
@article{Young,
archivePrefix = {arXiv},
arxivId = {arXiv:1708.02709v8},
author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
eprint = {arXiv:1708.02709v8},
file = {:Users/baharehzarei/Downloads/1708.02709.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Young et al. - Unknown - Recent Trends in Deep Learning Based Natural Language Processing.pdf:pdf},
keywords = {and engineering,ek laboratories,nanyang technological university,school of computer science,singapore},
pages = {1--32},
title = {{Recent Trends in Deep Learning Based Natural Language Processing}}
}
@article{Science2005,
author = {Science, Computer},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Science - 2005 - Master ' s Thesis.pdf:pdf},
title = {{Master ' s Thesis}},
year = {2005}
}
@phdthesis{Formulation2010,
author = {Formulation, Query and Matchmaking, Adaptive},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Formulation, Matchmaking - 2010 - Web Service Discovery Based on Semantic Information.pdf:pdf},
title = {{Web Service Discovery Based on Semantic Information}},
year = {2010}
}
@article{Dumas2016,
author = {Dumas, Marlon},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Dumas - 2016 - Service Discovery Master ' s Thesis ( 30 EAP ).pdf:pdf},
title = {{Service Discovery Master ' s Thesis ( 30 EAP )}},
year = {2016}
}
@article{Lightweight2012,
author = {Lightweight, A and Approach, Integrated},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lightweight, Approach - 2012 - I MPROVING S EMANTIC W EB S ERVICES D ISCOVERY AND R ANKING A LIGHTWEIGHT , INTEGRATED APPROACH J OS{\'{E}}.pdf:pdf},
title = {{I MPROVING S EMANTIC W EB S ERVICES D ISCOVERY AND R ANKING A LIGHTWEIGHT , INTEGRATED APPROACH J OS{\'{E}} M AR{\'{I}}A G ARC{\'{I}}A}},
year = {2012}
}
@article{Wu2014,
author = {Wu, Jian and Chen, Liang and Zheng, Zibin},
doi = {10.1007/s10115-013-0623-0},
file = {:Users/baharehzarei/Downloads/2014-KAIS-LiangChen.pdf:pdf},
journal = {Knowledge and information systems},
number = {1},
pages = {207--229},
title = {{Clustering Web services to facilitate service discovery}},
volume = {38},
year = {2014}
}
@inproceedings{Gao2014,
address = {Berlin, Heidelberg,},
author = {Gao, Huiying and Wang, Susu and Sun, Lily and Nian, Fuxing},
booktitle = {International Conference on Informatics and Semiotics in Organisations},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gao et al. - 2014 - Hierarchical Clustering based Web Service Discovery.pdf:pdf},
keywords = {hierarchical clustering,semantic analysis,service matching,vector space model,web service description,web service discovery},
pages = {281--291},
publisher = {Springer Berlin Heidelberg},
title = {{Hierarchical Clustering based Web Service Discovery}},
year = {2014}
}
@inproceedings{Sagayaraj2017,
author = {Sagayaraj, S and Santhoshkumar, M},
booktitle = {4th International Conference on Electronics and Communication Systems (ICECS)},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sagayaraj, Santhoshkumar - 2017 - A Survey on clustering methods in Web Service Discovery.pdf:pdf},
keywords = {- classification,clustering,clustering technology,method},
pages = {189--194},
publisher = {IEEE},
title = {{A Survey on clustering methods in Web Service Discovery}},
year = {2017}
}
@inproceedings{Fariss2018,
author = {Fariss, Mourad and Allali, Naoufal El and Asaidi, Hakima},
booktitle = {International Conference on Advanced Information Technology, Services and Systems},
doi = {10.1007/978-3-030-11914-0},
editor = {Springer, Cham},
file = {:Users/baharehzarei/Downloads/10.1007978-3-030-11914-08.pdf:pdf},
isbn = {9783030119140},
keywords = {Ontolo,Semantics,Web service,Web service discovery},
number = {October},
pages = {78--87},
publisher = {Springer International Publishing},
title = {{Review of Ontology Based Approaches for Web Service Discovery : Methods and Protocols Review of Ontology Based Approaches for Web Service Discovery}},
url = {http://dx.doi.org/10.1007/978-3-030-11914-0{\_}8},
year = {2018}
}
@article{Bhardwaj2015,
author = {Bhardwaj, Kailash Chander},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bhardwaj - 2015 - MACHINE LEARNING IN EFFICIENT AND EFFECTIVE WEB SERVICE DISCOVERY.pdf:pdf},
journal = {Journal of Web Engineering},
keywords = {b,bizer,c,communicated by,fuzzy logic,neural networks,ontology,quality of services,semantics,services modeling language,web,web ontology language,web service description language,web service modeling ontology,white},
number = {3},
pages = {196--214},
title = {{MACHINE LEARNING IN EFFICIENT AND EFFECTIVE WEB SERVICE DISCOVERY}},
volume = {14},
year = {2015}
}
@article{Song2013,
author = {Song, Seheon and Lee, Seok-won},
doi = {10.1016/j.mcm.2012.08.007},
file = {:Users/baharehzarei/Downloads/1-s2.0-S0895717712002154-main.pdf:pdf},
issn = {0895-7177},
journal = {Mathematical and Computer Modelling},
number = {1-2},
pages = {261--273},
publisher = {Elsevier Ltd},
title = {{A goal-driven approach for adaptive service composition using planning}},
url = {http://dx.doi.org/10.1016/j.mcm.2012.08.007},
volume = {58},
year = {2013}
}
@techreport{Sallehuddin2016,
abstract = {Nowadays, semantic web services are published and updated with growing demand for cloud computing. Since a single service is not capable of processing the increase of data and user's demand the improvement is necessary to match and rank semantic web service to achieve the user's goal. In the semantic web service framework, users' request is the input to the system and output is ranking of semantic web service. It has become a limitation to match between requests with the semantic web service description. This paper proposes a new framework for matching and ranking semantic web service based on OWLS. The proposed new framework can match the keyword in each task and ranking service. This framework is done by using performance ontology-based indexing. The result is obtained and the performance of the services for multiple requests has been measured.},
author = {Sallehuddin, Roselina and Mohamad, Radziah and Kaewpruksapimon, Chatchada},
booktitle = {Article in International Journal of Advances in Soft Computing and its Applications},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sallehuddin, Mohamad, Kaewpruksapimon - 2016 - The Format of the IJOPCM, first submission.pdf:pdf},
keywords = {Big data,Cloud Computing,Selection Service,Semantic Web Service},
number = {2},
title = {{The Format of the IJOPCM, first submission}},
url = {https://www.researchgate.net/publication/308718360},
volume = {8},
year = {2016}
}
@article{Bhardwaj2016,
abstract = {Web service discovery has been a challenging activity since a long time and is still a problem area to find the right service as per user's criteria. Web service discovery process entails finding the most relevant service according to the user's requirements. With the proliferation of web services, demand for automatic web service discovery framework has increased to provide services highly relevant to user requirements. Discovery of web services is an important step from user's perspective, since it is the first activity in service consumption. If web services cannot be discovered, they will become useless. Though this area has been the attention of research community since long, still the web services discovery remains a challenge for organizations using SOA (service oriented architecture). In this paper, we focus on various discovery processes of the web services and its related challenges. Human intervention is often needed to interpret the meaning in order to select and invoke web services. This leads to error-prone and time consuming processes. So, the process of service selection is not fully automated and requires researcher's attention to address the problem. There exist various approaches to address this problem with majority of the approaches using syntax based techniques. With the maturity of semantic technology, current trend is towards applying semantic based approaches. These approaches uses variant of techniques and ontology is one of the prominent techniques. There have been many literature surveys covering both semantic and syntactic based approaches. It has been observed that there is lack of study on ontology based approaches for the web services discovery. This paper provides an extensive survey on purely ontology based techniques, highlighting the state-of-the-art approaches. The objective of this survey is to help new researchers, who are interested to study and contribute into this research domain.},
author = {Bhardwaj, Kailash Chander and Sharma, R K},
doi = {10.14257/ijeic.2016.7.6.01},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bhardwaj, Sharma - 2016 - Ontologies A Review of Web Service Discovery Techniques.pdf:pdf},
issn = {2093-9655},
journal = {International Journal of Energy, Information and Communications},
keywords = {classification,ontology,quality of services,semantic matchmaking,semantics,web service},
pages = {1--12},
title = {{Ontologies: A Review of Web Service Discovery Techniques}},
url = {http://dx.},
volume = {7},
year = {2016}
}
@article{Jalali2014,
abstract = {Web Services are independent software systems which offer machine-to-machine interactions over the Internet to achieve well-described operations. With the advent of Service-Oriented Architecture (SOA), Web Services have gained tremendous popularity. As the number of Web Services is increased, finding the best service according to users requirements becomes a challenge. The Semantic Web Service discovery is the process of finding the most suitable service that satisfies the user request. A number of approaches to Web Service discovery have been proposed. In this paper, we classify them and determine the advantages and disadvantages of each group, to help researchers to implement a new or to select the most appropriate existing approach for Semantic Web Service discovery. We, also, provide a taxonomy which categorizes Web Service discovery systems from different points of view. There are three different views, namely, architectural view, automation view and matchmaking view. We focus on the matchmaking view which is further divided into semantic-based, syntax-based and context-aware. We explain each subgroup of it in detail, and then subsequently compare the subgroups in terms of their merits and drawbacks.},
author = {Jalali, Mehrdad and Pakari, Soodeh and Kheirkhah, Esmaeel},
doi = {10.5121/ijcseit.2014.4101},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jalali, Pakari, Kheirkhah - 2014 - Web Service Discovery Methods and Techniques A Review Re-Routing Based on QoS Requirements Changing i.pdf:pdf},
journal = {International Journal of Computer Science, Engineering and Information Technology (IJCSEIT)},
keywords = {Context-aware,Ontology,Semantic Web Service,Service Discovery},
number = {1},
title = {{Web Service Discovery Methods and Techniques: A Review Re-Routing Based on QoS Requirements Changing in WSN View project Information Dissemination in Social Networks View project WEB SERVICE DISCOVERY METHODS AND TECHNIQUES: A REVIEW}},
url = {https://www.researchgate.net/publication/262732243},
volume = {4},
year = {2014}
}
@article{Adala2011,
abstract = {As a greater number of Web Services are made available today, automatic discovery is recognized as an important task. To promote the automation of service discovery, different semantic languages have been created that allow describing the functionality of services in a machine interpretable form using Semantic Web technologies. The problem is that users do not have intimate knowledge about semantic Web service languages and related toolkits. In this paper, we propose a discovery framework that enables semantic Web service discovery based on keywords written in natural language. We describe a novel approach for automatic discovery of semantic Web services which employs Natural Language Processing techniques to match a user request, expressed in natural language, with a semantic Web service description. Additionally, we present an efficient semantic matching technique to compute the semantic distance between ontological concepts.},
author = {Adala, Asma and Tabbane, Nabil and Tabbane, Sami},
doi = {10.1155/2011/238683},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Adala, Tabbane, Tabbane - 2011 - A Framework for Automatic Web Service Discovery Based on Semantics and NLP Techniques.pdf:pdf},
journal = {Advances in Multimedia},
publisher = {Hindawi Publishing Corporation},
title = {{A Framework for Automatic Web Service Discovery Based on Semantics and NLP Techniques}},
volume = {2011},
year = {2011}
}
@techreport{Garcia2012a,
author = {Garc{\'{i}}a, Jos{\'{e}} Mar{\'{i}}a and Ruiz, David and Ruiz-Cort{\'{e}}s, Antonio},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Garc{\'{i}}a, Ruiz, Ruiz-Cort{\'{e}}s - 2012 - IMPROVING SEMANTIC WEB SERVICES DISCOVERY AND RANKING A LIGHTWEIGHT, INTEGRATED APPROACH.pdf:pdf},
title = {{IMPROVING SEMANTIC WEB SERVICES DISCOVERY AND RANKING A LIGHTWEIGHT, INTEGRATED APPROACH}},
url = {http://www.isa.us.es/},
year = {2012}
}
@phdthesis{Blackler2008,
author = {Blackler, Alethea L},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Blackler - 2008 - Intuitive interaction with complex artefacts empirically-based research.pdf:pdf},
school = {Saarbr{\"{u}}cken},
title = {{Intuitive interaction with complex artefacts : empirically-based research}},
url = {https://eprints.qut.edu.au/19122/3/c19122.pdf},
year = {2008}
}
@article{Lethrech2016,
abstract = {{\textcopyright} 2015 IEEE. Although the DSM approach was already present in the industrial world since the 90s, these last years it knows a growing interest from the research community. Domain Specific Modeling approach is essentially characterized by raising the level of abstraction and full code generation. In fact, a Domain Specific Language is semantically strong since it directly uses concepts and rules from a specific problem domain. In addition, concentrating on a narrow domain increases considerably the ability to automate the generation of the final solution from the domain-specific models. Throughout our research we have noticed a formalism lack of Domain Specific Software Development (DSSD) process. We propose in this paper a DSSD process formalism to guide the development of domain specific solutions. This process defines the phases, artifacts and roles required to specify and implement domain specific solutions.},
author = {Lethrech, Mohammed and Kenzi, Adil and Elmagrouni, Issam and Nassar, Mahmoud and Kriouile, Abdelaziz},
doi = {10.1109/ICoCS.2015.7483261},
file = {:Users/baharehzarei/Downloads/07483261.pdf:pdf},
isbn = {9781467396691},
journal = {Proceedings of 2015 IEEE World Conference on Complex Systems, WCCS 2015},
keywords = {Development Process,Domain Specific Language (DSL),Domain Specific Modeling (DSM),Domain Specific Software Development (DSSD),Model Driven Engineering (MDE)},
pages = {1--7},
publisher = {IEEE},
title = {{A process definition for domain specific software development}},
year = {2016}
}
@article{Casati2012,
author = {Casati, Fabio and Daniel, Florian and Angeli, Antonella De and Imran, Muhammad and Soi, Stefano and Wilkinson, Chritopher R and Marchese, Maurizio},
file = {:Users/baharehzarei/Downloads/DSM{\_}IJNGC2012.pdf:pdf},
journal = {International Journal of Next-Generation Computing},
number = {2},
title = {{Developing Mashup Tools for End-Users: On the Importance of the Application Domain}},
volume = {3},
year = {2012}
}
@inproceedings{Imran2012b,
abstract = {Web Engineering is the application of systematic, disciplined and quantifiable approaches to development, operation, and maintenance of Web-based applications. It is both a pro-active approach and a growing collection of theoretical and empirical research in Web application development. This paper gives an overview of Web Engineering by addressing the questions: a) why is it needed? b) what is its domain of operation? c) how does it help and what should it do to improve Web application development? and d) how should it be incorporated in education and training? The paper discusses the significant differences that exist between Web applications and conventional software, the taxonomy of Web applications, the progress made so far and the research issues and experience of creating a specialisation at the master's level. The paper reaches a conclusion that Web Engineering at this stage is a moving target since Web technologies are constantly evolving, making new types of applications possible, which in turn may require innovations in how they are built, deployed and maintained.},
author = {Imran, Muhammad and Soi, Stefano and Daniel, Florian and Marchese, Maurizio},
booktitle = {Web Engineering},
doi = {10.1007/3-540-28218-1},
file = {:Users/baharehzarei/Downloads/dsm{\_}icwe2012.pdf:pdf},
isbn = {3540281967},
number = {July},
pages = {1--438},
title = {{On the Systematic Development of Domain-Specific Mashup Tools for End Users}},
year = {2012}
}
@article{Soi2014,
abstract = {Despite the common claim by mashup platforms that they enable end-users to develop their own software, in practice end-users still don't develop their own mashups, as the highly technical or inexistent user bases of today's mashup platforms testify. The key shortcoming of current platforms is their general-purpose nature, that privileges expressive power over intuitiveness. In our prior work, we have demonstrated that a domain-specific mashup approach, which privileges intuitiveness over expressive power, has much more potential to enable end-user development (EUD). The problem is that developing mashup platforms—domain-specific or not—is complex and time consuming. In addition, domain-specific mashup platforms by their very nature target only a small user basis, that is, the experts of the target domain, which makes their development not sustainable if it is not adequately supported and automated.$\backslash$r$\backslash$n$\backslash$r$\backslash$nWith this article, we aim to make the development of custom, domain-specific mashup platforms cost-effective. We describe a mashup tool development kit (MDK) that is able to automatically generate a mashup platform (comprising custom mashup and component description languages and design-time and runtime environments) from a conceptual design and to provision it as a service. We equip the kit with a dedicated development methodology and demonstrate the applicability and viability of the approach with the help of two case studies.},
author = {Soi, Stefano and Daniel, Florian and Casati, Fabio},
doi = {10.1145/2628439},
file = {:Users/baharehzarei/Downloads/SoiTWEB2014.pdf:pdf},
issn = {15591131},
journal = {ACM Transactions on the Web},
number = {3},
pages = {1--35},
title = {{Conceptual Development of Custom, Domain-Specific Mashup Platforms}},
volume = {8},
year = {2014}
}
@article{Soi2014a,
abstract = {Despite the common claim by mashup platforms that they enable end-users to develop their own software, in practice end-users still don't develop their own mashups, as the highly technical or inexistent user bases of today's mashup platforms testify. The key shortcoming of current platforms is their general-purpose nature, that privileges expressive power over intuitiveness. In our prior work, we have demonstrated that a domain-specific mashup approach, which privileges intuitiveness over expressive power, has much more potential to enable end-user development (EUD). The problem is that developing mashup platforms-domain-specific or not-is complex and time consuming. In addition, domain-specific mashup platforms by their very nature target only a small user basis, that is, the experts of the target domain, which makes their development not sustainable if it is not adequately supported and automated. With this article, we aim to make the development of custom, domain-specific mashup platforms cost-effective. We describe a mashup tool development kit (MDK) that is able to automatically generate a mashup platform (comprising custom mashup and component description languages and design-time and runtime environments) from a conceptual design and to provision it as a service. We equip the kit with a dedicated development methodology and demonstrate the applicability and viability of the approach with the help of two case studies.},
author = {Soi, Stefano and Daniel, Florian and Casati, Fabio},
doi = {10.1145/2628439},
file = {:Users/baharehzarei/Downloads/SoiTWEB2014.pdf:pdf},
journal = {ACM Trans. Web},
title = {{Conceptual Development of Custom, Domain-Specific Mashup Platforms}},
url = {http://dx.doi.org/10.1145/2628439},
volume = {8},
year = {2014}
}
@article{Chemnitz2017,
author = {Chemnitz, Universitaetsbibliothek and Trial, I E L and Xplore, Ieee},
file = {:Users/baharehzarei/Downloads/Unknown - 2017 - ISOIECIEEE International Standard - Systems and software engineering--Vocabulary.pdf:pdf},
title = {{INTERNATIONAL STANDARD ISO / IEC / IEEE}},
volume = {2017},
year = {2017}
}
@inproceedings{Ceh2010,
abstract = {Domain-specific languages (DSL) are programming languages devoted to solve problems in a specific domain. Development of a DSL includes the following phases: decision, analysis, design, implementation and deployment. The least known and examined are analysis and design. Although various formal methodologies exist, the domain analysis is still done informally, most of the time. A common reason why formal methodologies are not used as often as they could be is that they are very demanding. Instead of developing a new, less complex methodology, we propose that domain analysis could be replaced with a previously existing analysis in some other form. A particularly suitable form for such is ontology. This paper focuses on ontology based domain analysis and how it can be incorporated into the DSL design phase. We present preliminary results of the Ontology2DSL framework, which can be used to help transform ontology to DSL grammar.},
author = {{\v{C}}eh, Ines and {\v{C}}repin{\v{s}}ek, Matej and Kosar, Toma{\v{z}} and Mernik, Marjan},
booktitle = {INForum},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/{\v{C}}eh et al. - 2010 - Using ontology in the development of domain-specific languages.pdf:pdf},
pages = {185--196},
title = {{Using ontology in the development of domain-specific languages}},
url = {https://pdfs.semanticscholar.org/5a6f/9f6b4820ad263a00a3aaa12b9b257a25fb0f.pdf},
year = {2010}
}
@techreport{Frank2010,
address = {Essen},
author = {Frank, Ulrich},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Frank - 2010 - Outline of a Method for Designing Domain-Specific Modelling Languages.pdf:pdf},
institution = {Universit{\"{a}}t Duisburg},
title = {{Outline of a Method for Designing Domain-Specific Modelling Languages}},
url = {https://www.econstor.eu/obitstream/10419/58163/1/716089785.pdf},
year = {2010}
}
@misc{Kokemuller,
author = {Kokemuller, Neil},
title = {{Multidisciplinary Teams {\&} the Importance of Teamwork | Chron.com}}
}
@article{Roche2003,
abstract = {During the last decade, the explosive growth of information technologies led to a shift in the market and economic view of the society: communication and knowledge sharing became the new economic stakes. But everyone speaks his own language, with his own terms and meanings. Ontologies seem to be one of the most suitable solutions faced with this problem and have become a very popular research topic in knowledge representation. But several problems remain which claim for clarification. The main objective of this survey is to make explicit the main questions about ontology and to draw some guidelines about the possible answers.},
author = {Roche, Christophe},
doi = {10.1016/S1474-6670(17)37715-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Roche - 2003 - Ontology A Survey.pdf:pdf},
journal = {IFAC Proceedings Volumes},
month = {sep},
number = {22},
pages = {187--192},
publisher = {Elsevier},
title = {{Ontology: A Survey}},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017377157},
volume = {36},
year = {2003}
}
@article{Maurer2016,
abstract = {The agile approach exists not only as a collection of individual techniques (principles, practices, roles, artifacts) but also in the form of entire agile methods, each of which is defined as a particular compendium of agile techniques. Chapter 9 presents the four principal agile methods: Extreme Programming (XP), Lean Software, Crystal and Clear. It discusses both the strong points and limitations of each.},
author = {Maurer, Frank and Melnik, Grigori},
doi = {10.1145/1134285.1134503},
file = {:Users/baharehzarei/Downloads/WBMA{\_}{\_}{\_}2016{\_}{\_}{\_}Craftsmanship1.pdf:pdf},
isbn = {1595933751},
journal = {Proceeding of the 28th international conference on Software engineering - ICSE '06},
number = {November},
pages = {1057},
title = {{IBM Design Thinking Software Development Framework}},
url = {http://portal.acm.org/citation.cfm?doid=1134285.1134503},
year = {2016}
}
@article{Plattner2009,
abstract = {In Design Thinking Peter Rowe provides a systematic account of the process of designing in architecture and urban planning. He examines multiple and often dissimilar theoretical positions whether they prescribe forms or simply provide procedures for solving problems - as particular manifestations of an underlying structure of inquiry common to all designing. Over 100 illustrations and a number of detailed observations of designers in action support Rowe's thesis.},
author = {Plattner, Hasso},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking(7).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking(2).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking(3).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking(4).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking(5).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Plattner - 2009 - Design thinking(6).pdf:pdf},
isbn = {9780262680677},
issn = {00340006},
journal = {Harvard Business Review},
number = {June},
pages = {220--251},
title = {{Design thinking}},
url = {http://bert.lib.indiana.edu:2048/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=crh{\&}AN=32108052},
volume = {16},
year = {2009}
}
@article{Cerejo2012,
abstract = {The difficult task of innovation is a key facet of Research {\&} Development (R{\&}D) institutions. Innovation is also closely related with processes oriented to achieve solutions in design. By addressing this topic, we propose to research new emerging design methods and provide an overview of design thinking tools that can be applied in an early stage of the R{\&}D research process in order to produce meaningful results. This research presents a set of experimental guidelines and an analysis method for the application of these tools. In particular, it is taken into account that design, through design thinking, extends to the experience that costumers/users have with products, services, spaces or even multidimensional experiences, which is a relevant input for R{\&}D innovation development. It is well known that design has changed its scope beyond the conception of artifacts: Design thinking methodologies have resulted from the advancement of current design paradigms. However, design thinking is not a new design discipline. It is a fresh multi-disciplinary platform that utilizes conceptual tools in order to provide steps towards innovation. In fact, the establishment of coherent guidelines for the design thinking process is a very complex task, due to its interdisciplinary requirements, that convey many diverse mindsets. The main focus of this study is creating an analysis toolkit that enables non-specialist and specialist users to perform high-quality design production. This methodological tool is being applied in the framework of two research centers, namely, the Research center for Science and Technology of the Arts and the Telecommunication Institute of Oporto University. These trials provide valuable pilot studies in order to determine the efficiency of this ‘mind map' in the R{\&}D innovation process. KEYWORDS},
author = {Cerejo, Joana and Barbosa, {\'{A}}lvaro},
file = {:Users/baharehzarei/Downloads/ICEM-2012.pdf:pdf},
journal = {62nd Anual Conference of International Council for Education Media on Design Thinking in Education, Media and Society},
keywords = {creative industries,d organization,design management,design thinking,design thinking tools,innovation,r},
number = {September},
pages = {26--29},
title = {{The Application of Design Thinking Methodology on Research Practices: a Mind-Map of Tools and Method}},
year = {2012}
}
@article{Gulliksen1995,
abstract = {The use of graphical user interfaces in a computerized work environment is often considered to substantially improve the work situation. The outcome can, however, often be the opposite. Inappropriate use of windowing techniques, scrolling, and colors can result in tedious and confusing interaction with the computer. Today's standards and style guides define basic design principles but are insufficient for design of interfaces to end?user applications. Here detailed domain knowledge is indeed essential. A domain?specific style guide (DSSG) is an extension of today's standard with domain?specific primitives, interface elements, and forms, together with domain?specific guidelines. Careful dedicated analysis of information utilization in a domain is the development basis for a DSSG. The development is performed with an object?oriented approach to facilitate the reuse of interface components and to support consistency and structure. Using a DSSG, the development of applications can be performed with a simplified information analysis. Therefore a more effective design process is possible, one in which end users can participate in the design using their own familiar domain?related terminology. Time and costs for the development process can be drastically reduced if domain?specific style guides, design guidelines, and development tools are used.$\backslash$nThe use of graphical user interfaces in a computerized work environment is often considered to substantially improve the work situation. The outcome can, however, often be the opposite. Inappropriate use of windowing techniques, scrolling, and colors can result in tedious and confusing interaction with the computer. Today's standards and style guides define basic design principles but are insufficient for design of interfaces to end?user applications. Here detailed domain knowledge is indeed essential. A domain?specific style guide (DSSG) is an extension of today's standard with domain?specific primitives, interface elements, and forms, together with domain?specific guidelines. Careful dedicated analysis of information utilization in a domain is the development basis for a DSSG. The development is performed with an object?oriented approach to facilitate the reuse of interface components and to support consistency and structure. Using a DSSG, the development of applications can be performed with a simplified information analysis. Therefore a more effective design process is possible, one in which end users can participate in the design using their own familiar domain?related terminology. Time and costs for the development process can be drastically reduced if domain?specific style guides, design guidelines, and development tools are used.},
author = {Gulliksen, Jan and Sandblad, Bengt},
doi = {10.1080/10447319509526116},
file = {:Users/baharehzarei/Downloads/6336c7a4aa994a8eeb4aec28e1af03b18468.pdf:pdf},
issn = {15327590},
journal = {International Journal of Human-Computer Interaction},
keywords = {analysis of information utilisation,domain specific design,interface elements,skilled,style guide,workers,workspaces},
number = {2},
pages = {135--151},
title = {{Domain-Specific Design of User Interfaces}},
volume = {7},
year = {1995}
}
@article{Krsticev2016,
author = {Krsti{\'{c}}ev, Danijela Boberi{\'{c}} and Te{\v{s}}endi{\'{c}}, Danijela and Jovi{\'{c}}, Milan and Baji{\'{c}}, {\v{Z}}eljko},
file = {:Users/baharehzarei/Downloads/icist2016{\_}33.pdf:pdf},
pages = {174--178},
title = {{DSL for web application development}},
year = {2016}
}
@article{Cadavid2009,
abstract = {Nowadays building a web application is still a complex process that requires a big effort to get several tasks done. This article presents a domain specific language aimed to simplify web application development by using it within a MDSD generation process, based on the construction and transformation of high level models from a domain structure viewpoint. this leads software development to focus on understanding the problem and doing a better business analysis, leaving the construction of the solution to a code generation process, thus reducing time and cost.},
author = {Cadavid, Juan Jos{\'{e}} and Lopez, David Esteban and Hincapi{\'{e}}, Jes{\'{u}}s Andr{\'{e}}s and Bernardo, Juan},
file = {:Users/baharehzarei/Downloads/A{\_}Domain{\_}Specific{\_}Language{\_}to{\_}Generate{\_}Web{\_}Applica.pdf:pdf},
journal = {emorias de la XII Conferencia Iberoamericana de Software Engineering (CIbSE 2009)},
keywords = {development,domain specific languages,mda,mdsd,metamodels,model engineering,model transformations,uml profiles,web,web applications,web development tools},
number = {January},
pages = {139--144},
title = {{A Domain Specific Language to Generate Web Applications}},
year = {2009}
}
@article{Happel2006,
abstract = {The emerging field of semantic web technologies promises new stimulus for Software Engineering research. However, since the underlying concepts of the semantic web have a long tradition in the knowledge engineering field, it is sometimes hard for software engineers to overlook the variety of ontology-enabled approaches to Software Engineering. In this paper we therefore present some examples of ontology applications throughout the Software Engineering lifecycle. We discuss the advantages of ontologies in each case and provide a framework for classifying the usage of ontologies in Software Engineering.},
author = {Happel, Hans-j{\"{o}}rg and Seedorf, Stefan},
doi = {10.1111/j.1463-1326.2004.00392.x},
file = {:Users/baharehzarei/Downloads/happel{\_}full.pdf:pdf},
isbn = {9783642102936},
issn = {1462-8902},
journal = {Proceedings of Workshop on Semantic Web Enabled Sofware Engineering (SWESE) on the ISWC},
pages = {5--9},
pmid = {15955116},
title = {{Applications of Ontologies in Software Engineering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.5733{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@article{Rahm,
author = {Rahm, Erhard and Arnold, Patrick and Raunich, Salvatore},
file = {:Users/baharehzarei/Downloads/paris-Octob2014.pdf:pdf},
title = {{PPT - Semantic ontology mappings How to determine and use them}}
}
@article{Paper2014,
abstract = {Recently, Social Network Service market is getting bigger and bigger. Then there are many security threats by malicious users. In addition, because sensitive data is concentrated on the central server, privacy can be exposed to SNS provider as well as malicious users. To overcome this problem, many previous researches suggest decentralized systems for SNS. In these systems, sensitive data may not be stored in central server. When a user transmits a message, the server does not interfere with the process. Thus, the user who transmits a message needs way to manage the keys that are used for message encryption scheme. In this paper, we suggest the efficient key management scheme using Dynamic Identity-Based Broadcast Encryption. Using this scheme, it is possible to communicate securely between users in decentralized social network. {\textcopyright} 2014 Springer-Verlag Berlin Heidelberg.},
author = {Paper, Conference and Yew, Kwang Hooi and Hassan, Mohd Fadzil and Integration, Knowledge-driven},
doi = {10.1007/978-3-642-41674-3},
file = {:Users/baharehzarei/Downloads/2013-SECS-SurveyOfOntoMap-Springer-rev2.pdf:pdf},
isbn = {978-3-642-41673-6},
number = {December},
title = {{Advances in Computer Science and its Applications}},
url = {http://link.springer.com/10.1007/978-3-642-41674-3},
volume = {279},
year = {2014}
}
@article{Hu2017,
author = {Hu, Jiming and Zhang, Yin},
doi = {10.1007/s11192-017-2383-1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hu, Zhang - 2017 - Discovering the interdisciplinary nature of Big Data research through social network analysis and visualization.pdf:pdf},
issn = {0138-9130},
journal = {Scientometrics},
month = {jul},
number = {1},
pages = {91--109},
publisher = {Springer Netherlands},
title = {{Discovering the interdisciplinary nature of Big Data research through social network analysis and visualization}},
url = {http://link.springer.com/10.1007/s11192-017-2383-1},
volume = {112},
year = {2017}
}
@article{Fielder2012,
author = {Fielder, J and Foltz, P W},
file = {:Users/baharehzarei/Downloads/Benefits and Challenges of Multidisciplinary Project Teams{\_} Less.pdf:pdf},
journal = {The ITEA Journal (International Test and Evaluation Association},
number = {1},
pages = {33},
title = {{Benefits and Challenges of Multidisciplinary Project Teams: "Lessons Learned" for Researchers and Practitioners}},
url = {https://commons.erau.edu/cgi/viewcontent.cgi?article=1116{\&}context=publication},
volume = {33},
year = {2012}
}
@article{Zhu2011,
author = {Zhu, Li},
doi = {10.1007/978-3-642-21530-8_51},
file = {:Users/baharehzarei/Downloads/10.1007{\_}978-3-642-21530-8.pdf:pdf},
isbn = {9783642215292},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Boundary Objects,Co-evolution,End User Development,HMS model,Habitable Environment,Meta-design},
pages = {403--406},
title = {{A meta-design framework to support multidisciplinary teams' online collaboration}},
volume = {6654 LNCS},
year = {2011}
}
@article{ScienceEurope2016,
abstract = {Research in the life sciences has changed over the past two decades to increasingly make use of ‘big science' approaches. This new type of research requires the involvement of numerous scientific disciplines, including biology, chemistry, physics, informatics, mathematics, and engineering; there is a shift from traditional scientific progress by individual research groups, towards larger, multidisciplinary projects and approaches. In this Opinion Paper, the Science Europe Scientific Committee for the Life, Environmental and Geo Sciences wishes to alert academic employers, promotion and appointment Committees and European and national research funding organisations to the lack of clear evaluation metrics for scientists working in multidisciplinary teams. The absence of such metrics already has a negative impact on career paths, as many scientists hesitate to participate in multidisciplinary research. Therefore, the Committee has devised concrete recommendations to contribute to the elaboration of an appropriate evaluation framework.},
author = {{Science Europe}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Science Europe - 2016 - Career Pathways in Multidisciplinary Research How to Assess the Contributions of Single Authors in Large Teams.pdf:pdf},
number = {December},
pages = {1--27},
title = {{Career Pathways in Multidisciplinary Research: How to Assess the Contributions of Single Authors in Large Teams}},
url = {http://www.scienceeurope.org/uploads/PublicDocumentsAndSpeeches/SCsPublicDocs/SE{\_}LEGS{\_}Careers{\_}OpinionPaper{\_}FIN.pdf{\%}5Cnhttp://www.scienceeurope.org/uploads/PublicDocumentsAndSpeeches/SCsPublicDocs/SE{\_}LEGS{\_}Careerpaths{\_}Workshop{\_}Report.PDF},
year = {2016}
}
@book{Web2011,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Web, Semantic},
doi = {10.1007/978-3-540-92913-0},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Web - 2011 - Handbook of Semantic Web Technologies.pdf:pdf},
isbn = {978-3-540-92912-3},
title = {{Handbook of Semantic Web Technologies}},
url = {http://link.springer.com/10.1007/978-3-540-92913-0},
year = {2011}
}
@article{Malki2012a,
abstract = {Mashups allowed a significant advance in the automation of interac- tions between applications and Web resources. In particular, the combination of Web APIs is seen as a strength, which can meet the complex needs by combin- ing the functionality and data from multiple services within a single Mashup application. Automating the process of building Mashup based mainly on the Semantics Web APIs facilitate to the developer their selection and matching. In this paper, we propose SAWADL (Semantic Annotation for Web Application Description Language), an extension of the WADL language that allows the semantization of the REST Web Service. We introduce a reference architecture with five layers representing the main functional blocks for annotating and combining web APIs, and therefore make the engineering process of Mashup applications more agile and more flexible.},
author = {Malki, Abdelhamid and Benslimane, Sidi Mohammed},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Malki, Benslimane - 2012 - Building semantic mashup(2).pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {API,Matching,REST,SAWADL,SAWSDL,SOAP,Semantic mashup},
number = {November},
pages = {40--49},
title = {{Building semantic mashup}},
volume = {867},
year = {2012}
}
@article{Casey2003,
abstract = {Component-based software engineering on the Web differs from traditional component and software engineering. We investigate Web component engineering activites that are crucial for the development, composition, and deployment of components on the Web. The current Web Services and Semantic Web initiatives strongly influence our work. Focussing on Web component composition we develop description and reasoning techniques that support a component developer in the composition activities, focussing here on matching. We show how a component model can be integrated into a Semantic Web-style ontology for component development. {\textcopyright}2003 Published by Elsevier Science B. V.},
author = {Casey, M{\'{a}}ire and Pahl, Claus},
doi = {10.1016/S1571-0661(04)80741-2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Casey, Pahl - 2003 - Web components and the semantic web.pdf:pdf},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
number = {5},
pages = {156--163},
title = {{Web components and the semantic web}},
volume = {82},
year = {2003}
}
@article{Hou2010,
author = {Hou, Jun and Zhang, Jinglan and Nayak, Richi and Bose, Aishwarya},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hou et al. - 2010 - Semantics-Based Web Service Discovery Using Information Retrieval Techniques.pdf:pdf},
journal = {International Workshop of the Initiative for the Evaluation of XML Retrieval},
keywords = {latent semantic analysis,semantics,web service discovery},
pages = {336--346},
title = {{Semantics-Based Web Service Discovery Using Information Retrieval Techniques}},
year = {2010}
}
@article{Balaji2013,
abstract = {Semantic web services focuses on providing automation and dynamics to existing web service technologies. A large amount of effort and money has been invested in the field of semantic web service discovery. In traditional web service discovery techniques, users need to select relevant web services manually from an extensive textual list. This paper provides an in-depth analysis on the existing approaches available for the discovery of semantic web services.},
author = {Balaji, Saravana and Gunasri, Priyadharshini G R and Saravana, Balaji B},
doi = {10.5120/14158-1759},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Balaji, Gunasri, Saravana - 2013 - A Survey on Semantic Web Service Discovery Methods.pdf:pdf},
journal = {A Survey on Semantic Web Service Discovery Methods Article in International Journal of Computer Applications},
keywords = {Semantics,UDDI,WSDL,Web service discovery,ontology},
number = {11},
pages = {975--8887},
title = {{A Survey on Semantic Web Service Discovery Methods}},
url = {https://www.researchgate.net/publication/272171105},
volume = {82},
year = {2013}
}
@article{Elsayed2016,
abstract = {{\textcopyright} 2015 IEEE. Semantically annotating web services is gaining more attention as an important aspect to support the automatic matchmaking of web services. Hence, in this paper, we follow a systematic survey related to the Semantic Web Services discovery to answer a research questions about how retrieve the semantic web services that match with a user query. We examined and analyzed 19 papers from the literature, and summarized the results.},
author = {Elsayed, Doaa Hany and Salah, Akram},
doi = {10.1109/ICENCO.2015.7416337},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Elsayed, Salah - 2016 - Semantic Web Service discovery A systematic survey.pdf:pdf},
isbn = {9781509002757},
journal = {2015 11th International Computer Engineering Conference: Today Information Society What's Next?, ICENCO 2015},
keywords = {OWL-S,matchmaking,semantic web service,service discovery},
pages = {131--136},
publisher = {IEEE},
title = {{Semantic Web Service discovery: A systematic survey}},
year = {2016}
}
@article{Jung2013,
abstract = {With the growing number of Web services available on the Web, finding services that match user's query becomes more difficult. To deal with the problem, numerous approaches employed semantic techniques in terms of annotation (or tagging) and service discovery algorithms. However, the problem of semantic service discovery based on the identification of fine-grained goals of users and services is still challenging due to the lack of semantic information in Web services. To enable the goal-driven semantic service discovery, we propose an automatic functional-goals tagging approach which employs a set of natural language processing (NLP) procedures based on the descriptions of Web services. In addition, to check the effectiveness of the functional-goals tagged, we designed a goal-driven semantic service discovery algorithm and compared it with other approaches: keyword-based, ontology-based, and topic-based service discovery. As results, our proposed goal-driven semantic service discovery achieved 75.7{\%} of precision and 59.6{\%} of recall (finally, F1=62{\%}) that outperform other discovery approaches. {\textcopyright} 2013 IEEE.},
author = {Jung, Yuchul and Cho, Yoonsung and Park, Yoo Mi and Lee, Taedong},
doi = {10.1109/ICSC.2013.45},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jung et al. - 2013 - Automatic tagging of functional-goals for goal-driven semantic service discovery.pdf:pdf},
isbn = {9780769551197},
journal = {Proceedings - 2013 IEEE 7th International Conference on Semantic Computing, ICSC 2013},
keywords = {Automatic tagging,Functional-goal,Goal-driven semantic service discovery,Web service},
pages = {212--219},
publisher = {IEEE},
title = {{Automatic tagging of functional-goals for goal-driven semantic service discovery}},
year = {2013}
}
@article{Park2015,
abstract = {A service mashup goes through several processes, which it takes much time and efforts for developers to mashup of many heterogeneous web services. To mitigate the complexity of a service mashup and automate the mashup process, the present paper proposes semantic service discovery and matching technologies. The semantic service discovery technology is capable of finding out more appropriate and ranked services with a given query, and the semantic service matching technology enables searching for compatible and interoperable services automatically across a number of heterogeneous web services. The semantic service discovery and matching technologies are based on the service ontology and service metadata that play important roles in relieving the semantic gap between a user's natural query and the technical service description. To verify the usability and effectiveness of the proposed technologies on this environment, experiments and simple use cases are shown. The results indicate that the proposed technologies help developers create new mashup applications more effectively and conveniently.},
author = {Park, Yoo Mi and Yoo, Hyunkyung and Hur, Cinyoung and Bae, Hyunjoo and Jung, Yuchul},
doi = {10.1109/ICOSC.2015.7050830},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2015 - Semantic service discovery and matching for semi-automatic service mashup.pdf:pdf},
isbn = {9781479979356},
journal = {Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing, IEEE ICSC 2015},
keywords = {Service mashup,semantic service discovery,semantic service matching,service composition},
pages = {332--337},
title = {{Semantic service discovery and matching for semi-automatic service mashup}},
year = {2015}
}
@article{Sangers2013b,
author = {Sangers, Jordy and Frasincar, Flavius and Hogenboom, Frederik and Chepegin, Vadim},
doi = {10.1016/j.eswa.2013.02.011},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sangers et al. - 2013 - Expert Systems with Applications Semantic Web service discovery using natural language processing techniques(2).pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {natural language processing,semantic web services},
number = {11},
pages = {4660--4671},
title = {{Expert Systems with Applications Semantic Web service discovery using natural language processing techniques}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.02.011},
volume = {40},
year = {2013}
}
@article{Pakari2014,
abstract = {Web Services are independent software systems which offer machine-to-machine interactions over the Internet to achieve well-described operations. With the advent of Service-Oriented Architecture (SOA), Web Services have gained tremendous popularity. As the number of Web Services is increased, finding the best service according to users requirements becomes a challenge. The Semantic Web Service discovery is the process of finding the most suitable service that satisfies the user request. A number of approaches to Web Service discovery have been proposed. In this paper, we classify them and determine the advantages and disadvantages of each group, to help researchers to implement a new or to select the most appropriate existing approach for Semantic Web Service discovery. We, also, provide a taxonomy which categorizes Web Service discovery systems from different points of view. There are three different views, namely, architectural view, automation view and matchmaking view. We focus on the matchmaking view which is further divided into semantic-based, syntax-based and context-aware. We explain each sub-group of it in detail, and then subsequently compare the sub-groups in terms of their merits and drawbacks.},
author = {Pakari, Soodeh and Kheirkhah, Esmaeel and Jalali, Mehrdad},
doi = {10.5121/ijcseit.2014.4101},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pakari, Kheirkhah, Jalali - 2014 - Web Service Discovery Methods and Techniques A Review.pdf:pdf},
issn = {22313605},
journal = {International Journal of Computer Science, Engineering and Information Technology},
number = {1},
pages = {01--14},
title = {{Web Service Discovery Methods and Techniques: A Review}},
volume = {4},
year = {2014}
}
@article{Sapkota2006,
abstract = {In this paper, we present a distributed Web service discovery architecture that is designed to be reliable, flexible and scalable. The architecture is based on the concept of distributed shared space and intelligent search among a subset of spaces. It allows the publishing of Web service descriptions as well as to submit requests to discover the Web service of user{\&}{\#}146;s interests. The Web service capabilities and the user requests (goal) are described using a Resource Description Framework (RDF) data model. The architecture supports integration of applications running on different resource specific devices. An application scenario is presented to illustrate the functionality of the proposed architecture.},
author = {Sapkota, Brahmananda and Roman, Dumitru and Kruk, Sebastian Ryszard and Fensel, Dieter},
doi = {10.1109/AICT-ICIW.2006.85},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sapkota et al. - 2006 - Distributed Web service discovery architecture.pdf:pdf},
isbn = {0769525229},
journal = {Proceedings of the Advanced International Conference on Telecommunications and International Conference on Internet and Web Applications and Services, AICT/ICIW'06},
number = {Fp6 004617},
pages = {136},
title = {{Distributed Web service discovery architecture}},
volume = {2006},
year = {2006}
}
@article{Schmidt1999a,
author = {Schmidt, Douglas C},
journal = {C++ Report magazine,},
title = {{Why Software Reuse has Failed and How to Make It Work for You}},
url = {https://www.dre.vanderbilt.edu/{~}schmidt/reuse-lessons.html},
year = {1999}
}
@techreport{,
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - A Framework for Constructing End User Oriented Service Mashups.pdf:pdf},
title = {{A Framework for Constructing End User Oriented Service Mashups}},
url = {https://www.research.manchester.ac.uk/portal/files/60768059/FULL{\_}TEXT.PDF}
}
@book{Liu2011a,
author = {Liu, Qiang},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Liu - 2011 - Link{\"{o}}ping Studies in Science and Technology Dealing with Missing Mappings and Structure in a Network of Ontologies.pdf:pdf},
isbn = {9789173932332},
title = {{Link{\"{o}}ping Studies in Science and Technology Dealing with Missing Mappings and Structure in a Network of Ontologies}},
url = {http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-64281},
year = {2011}
}
@article{Brugha2000,
author = {Brugha, Ruair{\'{i}} and {Zsuzsa Varvasovszky}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Brugha, Zsuzsa Varvasovszky - 2000 - Stakeholder analysis a review.pdf:pdf},
journal = {Health policy and planning },
number = {3},
pages = {239--246},
title = {{Stakeholder analysis: a review.}},
url = {https://watermark.silverchair.com/150239.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAAmUwggJhBgkqhkiG9w0BBwagggJSMIICTgIBADCCAkcGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMRNIdg-18VAvFJDMpAgEQgIICGFfCtwik9J61hy2ZS3X{\_}xfDhYWB0WJe8KmtdnMxkeGRnnHfZ},
volume = {15},
year = {2000}
}
@article{OECD2013,
abstract = {• Nearly 4.3 million students are enrolled in university-level education outside their home country. Australia, the United Kingdom, Switzerland, New Zealand and Austria have, in descending order, the highest percentage of international students. • Asian students represent 53{\%} of foreign students enrolled worldwide. The largest numbers of foreign students are from China, India and Korea. • OECD countries receive more international students than they send abroad for tertiary education. Almost three times as many foreign students are enrolled in tertiary education in OECD countries as there are OECD citizens studying abroad. • Some 83{\%} of all foreign students are enrolled in G20 countries, while 77{\%} are enrolled in OECD countries. These proportions have remained stable during the past decade. Significance This section looks at the extent to which students are studying abroad and their preferred destinations. Pursuing higher-level education in a foreign country allows students to expand their knowledge of other societies and languages , and thus improve their prospects in globalised sectors of the labour market. Beyond its social and educational effects, studying abroad has a considerable economic impact. For host countries, enrolling international students can not only help raise revenues from higher education, but can also be part of a broader strategy to recruit highly skilled immigrants. Findings OECD countries attract three out of four students studying abroad, with Australia, Canada, France, Germany, the United Kingdom and the United States together receiving more than 50{\%} of all foreign students worldwide. In terms of geographical area, Europe is the top destination for tertiary level students enrolled outside their country of origin, hosting 48{\%} of these students, followed by North America, which hosts 21{\%} of all international students. The number of international students in Oceania has tripled since 2000, although this region hosts less than 10{\%} of all foreign students. Other regions, such as Asia, Latin America and the Caribbean, are also seeing growing numbers of international students, reflecting the internationalisation of universities in an increasing number of countries. International students from OECD countries mainly come from},
author = {OECD},
doi = {10.1787/888932315602},
file = {:Users/baharehzarei/Downloads/eag{\_}highlights-2013-12-en.pdf:pdf},
title = {{OECD (2013), “How many students study abroad and where do they go?”, in Education at a Glance 2013: Highlights, OECD Publishing, Paris}},
url = {http://dx.doi.org/10.1787/888932315602.},
volume = {2013},
year = {2013}
}
@article{Castellion2013,
author = {Castellion, George and Markham, Stephen K.},
doi = {10.1111/j.1540-5885.2012.01009.x},
journal = {Journal of Product Innovation Management},
month = {sep},
number = {5},
pages = {976--979},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Perspective: New Product Failure Rates:  Influence of Argumentum ad Populum and Self-Interest}},
url = {http://doi.wiley.com/10.1111/j.1540-5885.2012.01009.x},
volume = {30},
year = {2013}
}
@article{Dobrica2008,
abstract = {The content of this paper addresses the issues regarding the reference architecture design for different domains in the context of a system of systems that is specific to today's embedded systems. The reference architecture contains core services of the domains included in abstract features package. The appropriate architectural style is provided by a knowledge base through service taxonomy. Services, commonality, variability management and rules for product derivation and configuration are the main issues considered in the architectural design process. Our contribution is the integrated vision based on our experiences and studies of the recent publications.},
author = {Dobrica, Liliana and Niemela, Eila},
file = {:Users/baharehzarei/Downloads/cc25ff8abcf8a7dba0bba2e6faae52be85b4.pdf:pdf},
journal = {Proceedings of the 2008 International Conference on Software Engineering Research and Practice, SERP 2008},
keywords = {Computer software selection and evaluation;Embedde},
number = {July 2017},
pages = {287--293},
title = {{An approach to reference architecture design for different domains of embedded systems}},
year = {2008}
}
@book{Endres-niggemeyer2013b,
abstract = {The web is growing quickly, substructures are coming up: a {\{}social, semantic, etc.{\}} web, or the {\{}business, services, etc.{\}} ecosystem which includes all resources of a specific web habitat. In the mashup ecosystem, developers are in intense scientific activity, what is easily measured by the number of their recent papers. Since mashups inherit an opportunistic (participatory) attitude, a main point of research is enabling users to create situation-specific mashups with little effort. After an overview, the chapter highlights areas of intensive discussion one by one: mashup description and modeling, semantic mashups, media mashups, ubiquitous mashups and end-user related development. Information is organized in two levels: right under the headings, a block of topic-related references may pop up. It is addressed to readers with deeper interest. After that, the text for everybody explains and illustrates innovative approaches. The chapter ends with an almost fail-safe outlook: given the growth of the web, the ecosystem of mashups will keep branching out. Core mashup features such as reuse of resources, user orientation, and versatile coordination (loose coupling) of components will propagate.},
author = {Endres-niggemeyer, Brigitte},
booktitle = {Semantic Mashups},
doi = {10.1007/978-3-642-36403-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Endres-niggemeyer - 2013 - Semantic Mashups(2).pdf:pdf},
isbn = {9783642364020},
publisher = {Springer Science {\&} Business Media},
title = {{Semantic Mashups}},
year = {2013}
}
@inproceedings{Imran2012,
abstract = {In this demonstration, we present ResEval Mash, a mashup platform for research evaluation, i.e., for the assessment of the productivity or quality of researchers, teams, institutions, journals, and the like - a topic most of us are acquainted with. The platform is specifically tailored to the need of sourcing data about scientific publications and researchers from the Web, aggregating them, computing metrics (also complex and ad-hoc ones), and visualizing them. ResEval Mash is a hosted mashup platform with a client-side editor and runtime engine, both running inside a common web browser. It supports the processing of also large amounts of data, a feature that is achieved via the sensible distribution of the respective computation steps over client and server. Our preliminary user study shows that ResEval Mash indeed has the power to enable domain experts to develop own mashups (research evaluation metrics); other mashup platforms rather support skilled developers. The reason for this success is ResEval Mash's domain specificity.},
author = {Imran, Muhammad and Kling, Felix and Soi, Stefano and Daniel, Florian},
booktitle = {WWW12 Companion},
doi = {10.1145/2187980.2188049},
isbn = {9781450312301},
keywords = {domain-specific mashup,end-user development,mashup},
pages = {361--364},
title = {{ResEval mash: a mashup tool for advanced research evaluation}},
year = {2012}
}
@article{Trinh2015,
abstract = {Despite an abundance of data available on the web today, satisfying users' complex information needs intelligently by automatically integrating and processing data from various sources remains challenging. In recent years, a large stream of research into mashups as a paradigm of end user development has emerged. These mashups foster combination and reuse of data and services and thereby allow end users to create novel applications. Developing such mashups efficiently and effectively, however, is still difficult for users that lack technical expertise. To address this issue, we extend a mashup platform with automatic mashup composition mechanisms and an agent that assists users in mashup design. To this end, we leverage semantics to simplify the mashup composition process on multiple levels. We associate each widget (i.e., mashup component) with a semantic model of inputs and outputs. These semantic models are helpful for identifying appropriate widgets in a given context and allow us to validate the links between widgets in a mashup. These validations provide the foundation for an advanced composition algorithm that automatically creates meaningful mashups from a given set of widgets. Finally, we develop an agent that leverages the semantic annotations to allow users to automatically compose mashups by entering natural-language text.},
author = {Trinh, T.-D. and Wetz, P. and Do, B.-L. and Kiesling, E. and Tjoa, A.M.},
doi = {10.1145/2837185.2837194},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Trinh et al. - 2015 - Semantic mashup composition from natural language expressions Preliminary results.pdf:pdf},
isbn = {9781450334914},
journal = {17th International Conference on Information Integration and Web-Based Applications and Services, iiWAS 2015 - Proceedings},
keywords = {all or part of,auto-composition,data integration,linked,linked open data,or,or hard copies of,permission to make digital,semantic mashups,this work for personal,widgets},
title = {{Semantic mashup composition from natural language expressions: Preliminary results}},
year = {2015}
}
@article{B2014,
author = {B, Tuan-dat Trinh and Wetz, Peter and Do, Ba-lam and Anjomshoaa, Amin},
doi = {10.1007/978-3-319-11955-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/B et al. - 2014 - Linked Widgets Platform Lowering the Barrier.pdf:pdf},
isbn = {9783319119557},
pages = {171--182},
title = {{Linked Widgets Platform : Lowering the Barrier}},
year = {2014}
}
@article{Noura2018,
author = {Noura, Mahda and Heil, Sebastian and Gaedke, Martin},
doi = {10.1007/978-3-319-91662-0_29},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Noura, Heil, Gaedke - 2018 - GrOWTH Goal-Oriented End User Development for Web of Things Devices.pdf:pdf},
isbn = {9783319916620},
keywords = {end user development {\'{a}},goal-oriented interaction,web of things {\'{a}}},
pages = {358--365},
title = {{GrOWTH: Goal-Oriented End User Development for Web of Things Devices}},
url = {http://link.springer.com/10.1007/978-3-319-91662-0{\_}29},
year = {2018}
}
@article{Quan2003,
abstract = {The Semantic Web promises to open innumerable opportunities for automation and information retrieval by standardizing the protocols for metadata exchange. However, just as the success of the World Wide Web can be attributed to the ease of use and ubiquity of Web browsers, we believe that the unfolding of the Semantic Web vision depends on users getting powerful but easy-to-use tools for managing their information. But unlike HTML, which can be easily edited in any text editor, RDF is more complicated to author and does not have an obvious presentation mechanism. Previous work has concentrated on the ideas of generic RDF graph visualization and RDF Schema-based form generation. In this paper, we present a comprehensive platform for constructing end user applications that create, manipulate, and visualize arbitrary RDF-encoded information, adding another layer to the abstraction cake. We discuss a programming environment specifically designed for manipulating RDF and introduce user interface concepts on top that allow the developer to quickly assemble applications that are based on RDF data models. Also, because user interface specifications and program logic are themselves describable in RDF, applications built upon our framework enjoy properties such as network updatability, extensibility, and end user customizability – all desirable characteristics in the spirit of the Semantic Web.},
author = {Quan, Dennis and Huynh, David and Karger, DavidR. R},
doi = {10.1007/978-3-540-39718-2_47},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Quan, Huynh, Karger - 2003 - Haystack A Platform for Authoring End User Semantic Web Applications.pdf:pdf},
isbn = {978-3-540-20362-9},
issn = {03029743},
journal = {Second International Semantic Web Conference (ISWC 2003)},
pages = {738--753},
pmid = {21141876},
title = {{Haystack: A Platform for Authoring End User Semantic Web Applications}},
url = {http://dx.doi.org/10.1007/978-3-540-39718-2{\_}47},
volume = {ISCW},
year = {2003}
}
@article{Macias2007,
abstract = {Generally speaking, emerging web-based technologies are mostly intended for professional developers. They pay poor attention to users who have no programming abilities but need to customize software applications. At some point, such needs force end-users to act as designers in various aspects of software authoring and development. Every day, more new computing-related professionals attempt to create and modify existing applications in order to customize web-based artifacts that will help them carry out their daily tasks. In general they are domain experts rather than skilled software designers, and new authoring mechanisms are needed in order that they can accomplish their tasks properly. The work we present is an effort to supply end-users with easy mechanisms for authoring web-based applications. To complement this effort, we present a user study showing that it is possible to carry out a trade-off between expressiveness and ease of use in order to provide end-users with authoring facilities. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Mac{\'{i}}as, Jos{\'{e}} A. and Castells, Pablo},
doi = {10.1016/j.intcom.2007.01.006},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mac{\'{i}}as, Castells - 2007 - Providing end-user facilities to simplify ontology-driven web application authoring(2).pdf:pdf},
isbn = {0953-5438},
issn = {09535438},
journal = {Interacting with Computers},
keywords = {End-User Development,Human-computer interaction,Intelligent user interfaces,Model-Based User Interfaces,Programming by example,Semantic web},
number = {4},
pages = {563--585},
title = {{Providing end-user facilities to simplify ontology-driven web application authoring}},
volume = {19},
year = {2007}
}
@inproceedings{Zhou2007,
abstract = {Semantic search promises to provide more accurate result than present-day keyword search. However, progress with semantic search has been delayed due to the complexity of its query languages. In this paper , we explore a novel approach of adapting keywords to querying the semantic web: the approach automatically translates keyword queries into formal logic queries so that end users can use familiar keywords to perform semantic search. A prototype system named 'SPARK' has been implemented in light of this approach. Given a keyword query, SPARK outputs a ranked list of SPARQL queries as the translation result. The translation in SPARK consists of three major steps: term mapping, query graph construction and query ranking. Specifically, a probabilistic query ranking model is proposed to select the most likely SPARQL query. In the experiment, SPARK achieved an encouraging translation result.},
address = {Heidelberg},
author = {Zhou, Qi and Wang, Chong and Xiong, Miao and Wang, Haofen and Yu, Yong},
booktitle = {The Semantic Web},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - 2007 - SPARK Adapting Keyword Query to Semantic Search.pdf:pdf},
pages = {694--707},
publisher = {Springer},
title = {{SPARK: Adapting Keyword Query to Semantic Search}},
url = {http://www.w3.org/TR/rdf-sparql-query/},
year = {2007}
}
@inproceedings{Tran2007,
abstract = {Current information retrieval (IR) approaches do not formally capture the explicit meaning of a keyword query but provide a comfortable way for the user to specify information needs on the basis of keywords. Ontology-based approaches allow for sophisticated semantic search but impose a query syntax more difficult to handle. In this paper, we present an approach for translating keyword queries to DL conjunctive queries using background knowledge available in on-tologies. We present an implementation which shows that this interpretation of keywords can then be used for both exploration of asserted knowledge and for a semantics-based declarative query answering process. We also present an evaluation of our system and a discussion of the limitations of the approach with respect to our underlying assumptions which directly points to issues for future work.},
address = {Heidelberg},
author = {Tran, Thanh and Cimiano, Philipp and Rudolph, Sebastian and Studer, Rudi},
booktitle = {International Semantic Web Conference},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tran et al. - 2007 - Ontology-based Interpretation of Keywords for Semantic Search.pdf:pdf},
pages = {523--536},
publisher = {Springer},
title = {{Ontology-based Interpretation of Keywords for Semantic Search}},
url = {http://www.aifb.uni-karlsruhe.de/},
year = {2007}
}
@article{Bonino2004,
abstract = {The introduction of semantics on the web will lead to a new generation of services based on content rather than on syntax. Search engines will provide topic-based searches, retrieving resources conceptually related to the user informational need. Queries will be expressed in several ways, and will be mapped on the semantic level defining topics that must be retrieved from the web. Moving towards this new Web era, effective semantic search engines will provide means for successful searches avoiding the heavy burden experimented by users in a classical query-string based search task. In this paper we propose a search engine based on web resource semantics. Resources to be retrieved are semantically annotated using an existing open semantic elaboration platform and an ontology is used to describe the knowledge domain into which perform queries. Ontology navigation provides semantic level reasoning in order to retrieve meaningful resources with respect to a given information request.},
author = {Bonino, Dario and Corno, Fulvio and Farinetti, Laura and Bosca, Alessio},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bonino et al. - 2004 - Ontology Driven Semantic Search.pdf:pdf},
journal = {WSEAS Transaction on Information Science and Application},
pages = {1597--1605},
title = {{Ontology Driven Semantic Search}},
url = {https://pdfs.semanticscholar.org/725d/273f160b631fb99a285bd032ba7a0df5f9c1.pdf},
year = {2004}
}
@inproceedings{Cohen2003,
abstract = {XSEarch, a semantic search engine for XML, is presented. XSEarch has a simple query language , suitable for a naive user. It returns semantically related document fragments that satisfy the user's query. Query answers are ranked using extended information-retrieval techniques and are generated in an order similar to the ranking. Advanced indexing techniques were developed to facilitate efficient implementation of XSEarch. The performance of the different techniques as well as the recall and the precision were measured experimentally. These experiments indicate that XSEarch is efficient, scalable and ranks quality results highly.},
address = {Berin},
author = {Cohen, Sara and Mamou, Jonathan and Kanza, Yaron and Sagiv, Yehoshua},
booktitle = {03 Proceedings of the 29th international conference on Very large data bases },
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cohen et al. - 2003 - XSEarch A Semantic Search Engine for XML.pdf:pdf},
pages = {45--56 },
title = {{XSEarch: A Semantic Search Engine for XML}},
url = {http://delivery.acm.org/10.1145/1320000/1315457/p45-cohen.pdf?ip=134.109.199.235{\&}id=1315457{\&}acc=ACTIVE SERVICE{\&}key=2BA2C432AB83DA15.2C9D92A4DDBE0279.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1549192531{\_}3188194d38a76e8373ca576218c96447},
year = {2003}
}
@article{Buranarach2013,
abstract = {Although the semantic web standards are established, applications and uses of the data are relatively limited. This is partly due to high learning curve and efforts demanded in building semantic web and ontology-based applications. In this paper, we describe an ontology application management framework that aims to simplify creation and adoption of a semantic web application. The framework supports application development in ontology- database mapping, recommendation rule management and application templates focusing on semantic search and recommender system applications. We present some case studies that adopted our application framework in their projects. Evolution of the software tool significantly profited from the semantic web research community in Thailand who has contributed both in terms of the tool development and adoption support. {\textcopyright} Springer-Verlag 2013.},
author = {Buranarach, Marut and Thein, Ye Myat and Supnithi, Thepchai},
doi = {10.1007/978-3-642-37996-3_21},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Buranarach, Thein, Supnithi - 2013 - A community-driven approach to development of an ontology-based application management framework.pdf:pdf},
isbn = {9783642379956},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Ontology application development tool,Semantic web application framework,Software tool development model},
pages = {306--312},
title = {{A community-driven approach to development of an ontology-based application management framework}},
volume = {7774 LNCS},
year = {2013}
}
@misc{,
title = {{RDF - Semantic Web Standards}},
url = {https://www.w3.org/RDF/},
urldate = {2019-02-01}
}
@techreport{HorrocksIanDieterFenselJeenBroekstraStefanDeckerMichaelErdmannCaroleGoble2000,
author = {{Horrocks, Ian, Dieter Fensel, Jeen Broekstra, Stefan Decker, Michael Erdmann, Carole Goble}, Frank van Harmelen},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Horrocks, Ian, Dieter Fensel, Jeen Broekstra, Stefan Decker, Michael Erdmann, Carole Goble - 2000 - The Ontology Inference Layer OIL.pdf:pdf},
title = {{The Ontology Inference Layer OIL}},
url = {http://www.darpa.mil/iso/ABC/BAA0007PIP.htm.},
year = {2000}
}
@article{Patel-Schneider2005a,
abstract = {The current architecture for the Semantic Web, with its emphasis on RDF syntactic and semantic compatability, has severe problems when expressive Semantic Web languages are incorporated. An architecture less tied to RDF is proposed. In this architecture different Semantic Web languages can have different syntaxes but must use the same models. This revised architecture provides significant advantages over the currrent Semantic Web architecture while still remaining true to the vision of the Semantic Web.},
author = {Patel-Schneider, Peter F.},
doi = {10.1007/11552222_3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Patel-Schneider - 2005 - A revised architecture for Semantic Web reasoning.pdf:pdf},
isbn = {3540287930},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {32--36},
title = {{A revised architecture for Semantic Web reasoning}},
volume = {3703 LNCS},
year = {2005}
}
@techreport{Berners-Lee1998,
author = {Berners-Lee, Tim},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Berners-Lee - 1998 - Semantic Web Road map.pdf:pdf},
title = {{Semantic Web Road map}},
url = {https://www.emse.fr/{~}beaune/websem/SWRoadmapLee.pdf},
year = {1998}
}
@techreport{Kreger2001,
author = {Kreger, Heather},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kreger - 2001 - Ibm Web Services Conceptual Architecture (WSCA 1.0).pdf:pdf},
title = {{Ibm Web Services Conceptual Architecture (WSCA 1.0)}},
url = {https://www.researchgate.net/profile/Heather{\_}Kreger/publication/235720479{\_}Web{\_}Services{\_}Conceptual{\_}Architecture{\_}WSCA{\_}10/links/563a67e008ae337ef2984607.pdf},
year = {2001}
}
@inproceedings{Rao2004,
abstract = {In today's Web, Web services are created and updated on the fly. It's already beyond the human ability to analysis them and generate the composition plan manually. A number of approaches have been proposed to tackle that problem. Most of them are inspired by the researches in cross-enterprise workflow and AI planning. This paper gives an overview of recent research efforts of automatic Web service composition both from the workflow and AI planning research community.},
address = {Berlin, Heidelberg.},
author = {Rao, Jinghai and Su, Xiaomeng},
booktitle = {International Workshop on Semantic Web Services and Web Process Composition. },
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rao, Su - 2004 - A Survey of Automated Web Service Composition Methods.pdf:pdf},
pages = {43--54},
publisher = {Springer},
title = {{A Survey of Automated Web Service Composition Methods}},
url = {http://www.cs.cmu.edu/{~}jinghai/papers/survey{\_}rao.pdf},
year = {2004}
}
@techreport{Pfau2007,
author = {Pfau, Gerhard and {Karsten Pl{\"{o}}sser}, Ibm and {Ravi Rangaswamy}, Sap and {Alan Rickayzen}, Oracle and {Michael Rowley}, Sap and {Patrick Schmidt}, Bea and {Ivana Trickovic}, Sap and {Alex Yiu}, Sap and Zeller, Matthias},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pfau et al. - 2007 - WS-BPEL Extension for People (BPEL4People), Version 1.0.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pfau et al. - 2007 - WS-BPEL Extension for People (BPEL4People), Version 1.0(2).pdf:pdf},
keywords = {BPEL,BPEL4People,Web Services},
title = {{WS-BPEL Extension for People (BPEL4People), Version 1.0}},
url = {https://www.oasis-open.org/committees/download.php/27505/BPEL4PeopleV1-Contribution.pdf},
year = {2007}
}
@inproceedings{Juhnke2010a,
author = {Juhnke, Ernst and Dornemann, Tim and Kirch, Sebastian and Seiler, Dominik and Freisleben, Bernd},
booktitle = {2010 36th EUROMICRO Conference on Software Engineering and Advanced Applications},
doi = {10.1109/SEAA.2010.32},
isbn = {978-1-4244-7901-6},
month = {sep},
pages = {137--140},
publisher = {IEEE},
title = {{SimpleBPEL: Simplified Modeling of BPEL Workflows for Scientific End Users}},
url = {http://ieeexplore.ieee.org/document/5598090/},
year = {2010}
}
@incollection{Claro2006,
author = {Claro, Daniela Barreiro and Albers, Patrick and Hao, Jin-Kao},
booktitle = {Semantic Web Services, Processes and Applications},
chapter = {8},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Claro, Albers, Hao - 2006 - Chapter 8 WEB SERVICES COMPOSITION.pdf:pdf},
pages = {195--225.},
publisher = {Springer},
title = {{Chapter 8 WEB SERVICES COMPOSITION}},
url = {https://pdfs.semanticscholar.org/60a4/0b6ee9ad79e4a4e4e20cb29575683169411c.pdf},
year = {2006}
}
@article{VanDeursen2000,
abstract = {We survey the literature available on the topic of domain-specific languages as used for the construction and maintenance of software systems. We list a selection of 75 key publications in the area, and provide a summary for each of the papers. Moreover, we discuss terminology, risks and benefits, example domain-specific languages, design method-ologies, and implementation techniques.},
author = {{Van Deursen}, Arie and Klint, Paul and Visser, Joost},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Van Deursen, Klint, Visser - 2000 - Domain-Specific Languages An Annotated Bibliography.pdf:pdf},
journal = { ACM Sigplan Notices},
pages = {26--36.},
title = {{Domain-Specific Languages: An Annotated Bibliography*}},
year = {2000}
}
@book{Brambilla2008a,
abstract = {Web Engineering: Modelling and Implementing Web Applications presents the state of the art approaches for obtaining a correct and complete Web software product from conceptual schemas, represented via well-known design notations. Describing mature and consolidated approaches to developing complex applications, this edited volume is divided into three parts and covers the challenges web application developers face; design issues for web applications; and how to measure and evaluate web applications in a consistent way. With contributions from leading researchers in the field this book will appeal to researchers and students as well as to software engineers, software architects and business analysts.},
author = {Brambilla, Marco and Comai, Sara and Fraternali, Piero and Matera, Maristella and Analysis, Social Media},
doi = {10.1007/978-1-84628-923-1},
file = {:Users/baharehzarei/Downloads/Designing{\_}Web{\_}Applications{\_}with{\_}Webml{\_}and{\_}Webratio.pdf:pdf},
isbn = {978-1-84628-922-4},
issn = {15715035},
number = {May 2014},
pmid = {19084907},
title = {{Web Engineering: Modelling and Implementing Web Applications}},
url = {http://link.springer.com/10.1007/978-1-84628-923-1},
year = {2008}
}
@article{Chudnovskyy2012a,
abstract = {With the success of Web 2.0 we are witnessing a growing number of services and APIs exposed by Telecom, IT and content providers. Targeting the Web community and, in particular, Web application developers, service providers ex- pose capabilities of their infrastructures and applications in order to open new markets and to reach new customer groups. However, due to the complexity of the underly- ing technologies, the last step, i.e., the consumption and integration of the o ered services, is a non-trivial and time- consuming task that is still a prerogative of expert develop- ers. Although many approaches to lower the entry barriers for end users exist, little success has been achieved so far. In this paper, we introduce the OMELETTE 1 project and show how it addresses end-user-oriented telco mashup de- velopment. We present the goals of the project, describe its contributions, summarize current results, and describe current and future work.},
author = {Chudnovskyy, O and Nestler, T and Gaedke, M and Daniel, F and Fern{\'{a}}ndez-Villamor, J I and Chepegin, V and Fornas, J A and Wilson, S and K{\"{o}}gler, C and Chang, H},
doi = {10.1145/2187980.2188017},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chudnovskyy et al. - 2012 - End-user-oriented telco mashups The OMELETTE approach.pdf:pdf},
isbn = {9781450312295},
journal = {WWW'12 - Proceedings of the 21st Annual Conference on World Wide Web Companion},
keywords = {Content providers; End users; Entry barriers; EUD;,Hardware,World Wide Web},
pages = {235--238},
title = {{End-user-oriented telco mashups: The OMELETTE approach}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84861079517{\&}partnerID=40{\&}md5=6212869df192d5415460a69aa59d5ca0},
year = {2012}
}
@article{CasatiF.DanielF.DeAngeliA.ImranM.SoiS.WilkinsonC.R.&Marchese2012,
author = {{Casati, F., Daniel, F., De Angeli, A., Imran, M., Soi, S., Wilkinson, C. R., {\&} Marchese}, M.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Casati, F., Daniel, F., De Angeli, A., Imran, M., Soi, S., Wilkinson, C. R., {\&} Marchese - 2012 - Developing Mashup Tools for End-Users O.pdf:pdf},
journal = {International Journal of Next-Generation Computing},
keywords = {domain-specific mashup,mashup meta-model,mashup tools},
number = {2},
title = {{Developing Mashup Tools for End-Users: On the Importance of the Application Domain}},
volume = {3},
year = {2012}
}
@inproceedings{Desolda2016b,
author = {Desolda, Giuseppe and Ardito, Carmelo and Matera, Maristella},
booktitle = {International Rapid Mashup Challenge},
doi = {10.1007/978-3-319-53174-8_5},
pages = {74--93},
publisher = {Springer, Cham},
title = {{End-User Development for the Internet of Things: EFESTO and the 5W Composition Paradigm}},
url = {http://link.springer.com/10.1007/978-3-319-53174-8{\_}5},
year = {2016}
}
@article{Desolda2017,
abstract = {Developing interactive systems to access and manipulate data is a very tough task. In particular, the development of user interfaces (UIs) is one of the most time-consuming activities in the software lifecycle. This is even more demanding when data have to be retrieved by accessing flexibly different online resources. Indeed, software development is moving more and more toward composite applications that aggregate on the fly specific Web services and APIs. In this article, we present a mashup model that describes the integration, at the presentation layer, of UI components. The goal is to allow non-technical end users to visualize and manipulate (i.e., to perform actions on) the data displayed by the components, which thus become actionable UI components. This article shows how the model has guided the development of a mashup platform through which non-technical end users can create component-based interactive workspaces via the aggregation and manipulation of data fetched from distributed online resources. Due to the abundance of online data sources, facilitating the creation of such interactive workspaces is a very relevant need that emerges in different contexts. A utilization study has been performed in order to assess the benefits of the proposed model and of the Actionable UI Components; participants were required to perform real tasks using the mashup platform. The study results are reported and discussed.},
author = {Desolda, Giuseppe and Ardito, Carmelo and Costabile, Maria Francesca and Matera, Maristella},
doi = {10.1016/J.JVLC.2017.08.004},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Desolda et al. - 2017 - End-user composition of interactive applications through actionable UI components.pdf:pdf},
issn = {1045-926X},
journal = {Journal of Visual Languages {\&} Computing},
month = {oct},
pages = {46--59},
publisher = {Academic Press},
title = {{End-user composition of interactive applications through actionable UI components}},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X17300125},
volume = {42},
year = {2017}
}
@article{Bellucci2010,
abstract = {Web 2.0 can be viewed as a platform where users can develop their own web applications. It is also characterized by a vast amount of user-generated contents presenting spatial and temporal components, by means of associated metadata. These metadata has been successfully exploited to generate map-based mashups (web applications gathering data from different sources) facing different kind of crisis situations, ranging from natural disasters (earthquakes, wildfires, floods...) to human-made disasters (terrorist attacks, school shootings, conflicts...). The social and collaborative dimensions of the Web 2.0 can be also exploited for managing crisis-related information. We present here a survey of current crisis-related mashups we employed to extract design dimensions and provide a conceptual framework that can be used: a) to understand current systems and; b) to design next generation of crisis-related mashups. We propose the eStoryS system as an example of application developed following the design principles presented in this paper. On the basis of our analysis, we believe that the design dimensions posited here provide useful insights for the design of novel web mashups in the emergency management domain.},
author = {Bellucci, Andrea and Malizia, Alessio and Diaz, Paloma and Aedo, Ignacio},
file = {:Users/baharehzarei/Downloads/Framing{\_}the{\_}design{\_}space{\_}for{\_}novel{\_}crisis-related{\_}.pdf:pdf},
journal = {7th International ISCRAM Conference},
keywords = {collaboration tools,crisis informatics,georeferenced information,web mashups},
number = {May},
pages = {1--10},
title = {{Framing the design space for novel crisis-related mashups : the eStoryS example}},
year = {2010}
}
@article{Ardito2011,
abstract = {Mass customization refers to the increase in variety and customization of the manufactured products and services. It is now economically feasible thanks to the availability of computer-aided manufacturing systems, which allow people to customize standard products, and to Internet, through which many online retailers now operate, thus eliminating the constraints of physical shelf space and other bottlenecks of distribution that, in past years, prevented the pro-duction of niche products because of their high production costs. To permit mass customization, several software-based product configurators are available on the Web: they guide people in adapting a product to their needs and desires. A draw-back of such configurators is the limited range of changes permitted. We present in this paper a system that gives people more freedom in creating products that best fit their desires, thanks to the use of an ontology, which models the possible product compositions that users can perform. The proposed solution is shown through a case study, which refers to furniture production.},
author = {Ardito, Carmelo and Barricelli, Barbara Rita and Buono, Paolo and Costabile, Maria Francesca and Lanzilotti, Rosa and Piccinno, Antonio and Valtolina, Stefano},
doi = {10.1007/978-3-642-21530-8_9},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ardito et al. - 2011 - An ontology-based approach to product customization.pdf:pdf},
isbn = {9783642215292},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {end-user development,knowledge management,long tail,ontology,product customization},
pages = {92--106},
title = {{An ontology-based approach to product customization}},
volume = {6654 LNCS},
year = {2011}
}
@book{Costabile2011,
abstract = {uralt!},
author = {Costabile, Maria Francesca},
booktitle = {Lecture Notes in Computer Science},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Costabile - 2011 - End-User Development Third International Symposium.pdf:pdf},
isbn = {9783642215292},
title = {{End-User Development: Third International Symposium}},
url = {http://tocs.ulb.tu-darmstadt.de/8733295.pdf},
year = {2011}
}
@article{Boulos2007,
abstract = {Web 2.0 sociable technologies and social software are presented as enablers in health and health care, for organizations, clinicians, patients and laypersons. They include social networking services, collaborative filtering, social bookmarking, folksonomies, social search engines, file sharing and tagging, mashups, instant messaging, and online multi-player games. The more popular Web 2.0 applications in education, namely wikis, blogs and podcasts, are but the tip of the social software iceberg. Web 2.0 technologies represent a quite revolutionary way of managing and repurposing/remixing online information and knowledge repositories, including clinical and research information, in comparison with the traditional Web 1.0 model. The paper also offers a glimpse of future software, touching on Web 3.0 (the Semantic Web) and how it could be combined with Web 2.0 to produce the ultimate architecture of participation. Although the tools presented in this review look very promising and potentially fit for purpose in many health care applications and scenarios, careful thinking, testing and evaluation research are still needed in order to establish ‘best practice models' for leveraging these emerging technologies to boost our teaching and learning productivity, foster stronger ‘communities of practice', and support continuing medical education/professional development (CME/CPD) and patient education.},
archivePrefix = {arXiv},
arxivId = {A luxury brand management framework built from historical review and case study analysis},
author = {Boulos, Maged N.Kamel and Wheeler, Steve},
doi = {10.1111/j.1471-1842.2007.00701.x},
eprint = {A luxury brand management framework built from historical review and case study analysis},
file = {:Users/baharehzarei/Downloads/Boulos{\_}et{\_}al-2007-Health{\_}Information{\_}{\&}{\_}Libraries{\_}Journal.pdf:pdf},
isbn = {1471-1842},
issn = {14711834},
journal = {Health Information and Libraries Journal},
number = {1},
pages = {2--23},
pmid = {17331140},
title = {{The emerging Web 2.0 social software: An enabling suite of sociable technologies in health and health care education}},
volume = {24},
year = {2007}
}
@article{Sales2016,
abstract = {Users combine attributes and types to describe and classify entities into categories. These categories are fundamental for organising knowledge in a decentralised way acting as tags and predicates. When searching for entities, categories frequently describes the search query. Considering that users do not knowin which terms the categories are expressed, they might query the same concept by a paraphrase. While some categories are composed of simple expressions (e.g. Presidents of Ireland), others have more complex compositional patterns (e.g. French Senators Of The Second Empire). This work proposes a hybrid semantic model based on syntactic analysis, distributional semantics and named entity recognition to recognise paraphrases of entity categories. Our results showthat the proposed model outperformed the comparative baseline, in terms of recall and mean reciprocal rank, thus being suitable for addressing the vocabulary gap between user queries and entity categories.},
author = {Sales, JE and Freitas, A and Davis, B and Handschuh, S},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sales et al. - 2016 - A compositional-distributional semantic model for searching complex entity categories.pdf:pdf},
isbn = {9781941643921},
journal = {199 Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (SEM 2016)},
pages = {199--208},
title = {{A compositional-distributional semantic model for searching complex entity categories}},
url = {http://www.aclweb.org/anthology/S/S16/S16-2.pdf{\#}page=217},
year = {2016}
}
@article{Curry2015,
author = {Curry, Edward},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Curry - 2015 - Schema-Agnostic Queries for Large-Schema Databases A Distributional Semantics Approach Declaration of Authorship.pdf:pdf},
number = {June},
title = {{Schema-Agnostic Queries for Large-Schema Databases : A Distributional Semantics Approach Declaration of Authorship}},
year = {2015}
}
@article{Bastianelli2014a,
abstract = {Robots are slowly becoming part of everyday life, as they are being marketed for commercial applications (viz. telepresence, cleaning or entertainment). Thus, the ability to interact with non-expert users is becoming a key requirement. Even if user utterances can be efficiently recognized and transcribed by Automatic Speech Recognition systems, several issues arise in translating them into suitable robotic actions. In this paper, we will discuss both approaches providing two existing Natural Language Understanding workflows for Human Robot Interaction. First, we discuss a grammar based approach: it is based on grammars thus recognizing a restricted set of commands. Then, a data driven approach, based on a free-from speech recognizer and a statistical semantic parser, is discussed. The main advantages of both approaches are discussed, also from an engineering perspective, i.e. considering the effort of realizing HRI systems, as well as their reusability and robustness. An empirical evaluation of the proposed approaches is carried out on several datasets, in order to understand performances and identify possible improvements towards the design of NLP components in HRI.},
author = {Bastianelli, Emanuele and Castellucci, Giuseppe and Croce, Danilo and Basili, Roberto and Nardi, Daniele},
doi = {10.3233/978-1-61499-419-0-57},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bastianelli et al. - 2014 - Effective and robust natural language understanding for human-robot interaction.pdf:pdf},
isbn = {9781614994183},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
number = {July},
pages = {57--62},
title = {{Effective and robust natural language understanding for human-robot interaction}},
volume = {263},
year = {2014}
}
@article{Bastianelli2014,
abstract = {Recent years show the development of large scale resources (e.g. FrameNet for the Frame Semantics) that supported the definition of several state-of-the-art approaches in Natural Language Processing. However, the reuse of existing resources in heterogeneous domains such as Human Robot Interaction is not straightforward. The generalization offered by many data driven methods is strongly biased by the employed data, whose performance in out-of-domain conditions exhibit large drops. In this paper, we present the Human Robot Interaction Corpus (HuRIC). It is made of audio files paired with their transcriptions referring to commands for a robot, e.g. in a home environment. The recorded sentences are annotated with different kinds of linguistic information, ranging from morphological and syntactic information to rich semantic information, according to the Frame Semantics, to characterize robot actions, and Spatial Semantics, to capture the robot environment. All texts are represented through the Abstract Meaning Representation, to adopt a simple but expressive representation of commands, that can be more easily translated into the internal representation of the robot.},
author = {Bastianelli, Emanuele and Castellucci, Giuseppe and Croce, Danilo and Basili, Roberto and Nardi, Daniele and Iocchi, Luca},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bastianelli et al. - 2014 - HuRIC a Human Robot Interaction Corpus.pdf:pdf},
isbn = {978-2-9517408-8-4},
journal = {International Conference on language resources and evaluation},
keywords = {corpus,human-robot interaction,natural language processing},
pages = {4519--4526},
title = {{HuRIC : a Human Robot Interaction Corpus}},
year = {2014}
}
@misc{Liang2015,
author = {Liang, Percy and Potts, Christopher},
booktitle = {Annual Review of Linguistics},
doi = {10.1146/annurev-linguist-030514-125312},
month = {jan},
number = {1},
pages = {355--376},
title = {{Bringing Machine Learning and Compositional Semantics Together}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-linguist-030514-125312},
urldate = {2019-01-11},
volume = {1},
year = {2015}
}
@article{Tsarfaty2014,
abstract = {Abstract We present a model for the automatic semantic analysis of requirements elicitation documents. Our target semantic representation employs live sequence charts, a multi-modal visual language for scenariobased programming, which can be directly translated into ...},
author = {Tsarfaty, Reut},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tsarfaty - 2014 - Semantic Parsing Using Content and Context A Case Study from Requirements Elicitation.pdf:pdf},
isbn = {9781937284961},
journal = {Emnlp},
keywords = {a case study from,and context,antic parsing using content,requirements elicitation},
number = {2013},
pages = {1296--1307},
title = {{Semantic Parsing Using Content and Context : A Case Study from Requirements Elicitation}},
year = {2014}
}
@article{Patil2017,
author = {Patil, Nilam S},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Patil - 2017 - Approaches for Semantic Parsing in Machine Learning A survey.pdf:pdf},
keywords = {- semantic role labeling,-semantic role labeling,keyword,parsing},
number = {2},
pages = {3112--3115},
title = {{Approaches for Semantic Parsing in Machine Learning : A survey}},
year = {2017}
}
@article{Corona,
author = {Corona, Rodolfo},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Corona - Unknown - An Analysis of Using Semantic Parsing for Speech Recognition.pdf:pdf},
pages = {1--36},
title = {{An Analysis of Using Semantic Parsing for Speech Recognition}}
}
@article{Li2018,
abstract = {A key challenge for generalizing programming-by-demonstration (PBD) scripts is the data description problem-when a user demonstrates performing an action, the system needs to determine features for describing this action and the target object in a way that can reflect the user's intention for the action. However, prior approaches for creating data descriptions in PBD systems have problems with usability, applicability, feasibility, transparency and/or user control. Our APPINITE system introduces a multi-modal interface with which users can specify data descriptions verbally using natural language instructions. APPINITE guides users to describe their intentions for the demonstrated actions through mixed-initiative conversations. APPINITE constructs data descriptions for these actions from the natural language instructions. Our evaluation showed that APPINITE is easy-to-use and effective in creating scripts for tasks that would otherwise be difficult to create with prior PBD systems, due to ambiguous data descriptions in demonstrations on GUIs.},
author = {Li, Toby Jia Jun and Labutov, Igor and Li, Xiaohan Nancy and Zhang, Xiaoyi and Shi, Wenze and Ding, Wanling and Mitchell, Tom M. and Myers, Brad A.},
doi = {10.1109/VLHCC.2018.8506506},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - APPINITE A multi-modal interface for specifying data descriptions in programming by demonstration using natural langu.pdf:pdf},
isbn = {9781538642351},
issn = {19436106},
journal = {Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC},
keywords = {End user development,Multi-modal interaction,Natural language programming,Programming by demonstration,Verbal instruction},
pages = {105--114},
title = {{APPINITE: A multi-modal interface for specifying data descriptions in programming by demonstration using natural language instructions}},
volume = {2018-Octob},
year = {2018}
}
@article{Ernst2017,
abstract = {A powerful, but limited, way to view software is as source code alone. Treating a program as a sequence of instructions enables it to be formalized and makes it amenable to mathematical techniques such as abstract interpretation and model checking. A program consists of much more than a sequence of instructions. Developers make use of test cases, documentation, variable names, program structure, the version control repository, and more. I argue that it is time to take the blinders off of software analysis tools: tools should use all these artifacts to deduce more powerful and useful information about the program. Researchers are beginning to make progress towards this vision. This paper gives, as examples, four results that find bugs and generate code by applying natural language processing techniques to software artifacts. The four techniques use as input error messages, variable names, procedure documentation, and user questions. They use four different NLP techniques: document similarity, word semantics, parse trees, and neural networks. The initial results suggest that this is a promising avenue for future work. 1 Introduction What is software? A reasonable definition-and the one most often adopted by the programming language community-is: a sequence of instructions that perform some task. This definition accommodates the programmer's view of source code and the machine instructions that the CPU executes. Furthermore, this definition enables formalisms: the execution model of the machine, and the meaning of every instruction, can be mathematically defined, for example via denotational semantics or operational semantics. By combining the meanings of each instruction, the meaning of a program can be induced. This perspective leads to powerful static analyses, such as symbolic analysis, abstract interpretation, dataflow analysis, type checking, and model checking. Equally important and challenging theoretically-and probably more important in practice-are dynamic analyses that run the program and observe its behavior. These are at the heart of techniques such as testing, error detection and localization, debugging, profiling, tracing, and optimization. Despite the successes of viewing a program as a sequence of instructions-essentially, of treating a program as no more than an AST (abstract syntax tree)-this view is limited and foreign to working programmers, who should be the focus of research in programming {\textcopyright} Michael D. Ernst; licensed under Creative Commons License CC-BY SNAPL: The 2nd Summit on Advances In Programming Languages.},
author = {Ernst, Michael D.},
doi = {10.4230/LIPIcs.SNAPL.2017.4},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ernst - 2017 - Natural language is a programming language Applying natural language processing to software development.pdf:pdf},
isbn = {9783959770323},
issn = {18688969},
journal = {2nd Summit on Advances in Programming Languages (SNAPL'17)},
keywords = {2017,4,4230,and phrases natural language,digital object identifier 10,lipics,processing,program analysis,snapl,software development},
number = {4},
pages = {1--14},
title = {{Natural language is a programming language: Applying natural language processing to software development}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019618182{\&}doi=10.4230{\%}2FLIPIcs.SNAPL.2017.4{\&}partnerID=40{\&}md5=c9b1d770c6dfbd1b47d735f637714abc},
volume = {71},
year = {2017}
}
@article{Myers2004,
abstract = {Over the last six years, we have been working to create programming languages and environments that are more natural, or closer to the way people think about their tasks. Our goal is to make it possible for people to express their ideas in the same way they think about them. To achieve this, we have performed various studies about how people think about programming tasks, both when trying to create a new program and when trying to find and fix bugs in existing programs. We then use this knowledge to develop new tools for programming and debugging. Our user studies have shown the resulting systems provide significant benefits to users},
author = {Myers, Brad and Pane, John and Ko, Andy},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Myers, Pane, Ko - 2004 - Natural Programming Languages and Environments(2).pdf:pdf},
journal = {Communications of the ACM},
number = {9},
pages = {47--52},
title = {{Natural Programming Languages and Environments}},
volume = {47},
year = {2004}
}
@article{Thomason2015,
abstract = {Intelligent robots frequently need to understand requests from naive users through natural language. Previous approaches either cannot account for language variation, e.g., keyword search, or require gathering large annotated corpora, which can be expensive and cannot adapt to new variation. We introduce a dialog agent for mobile robots that understands human instructions through semantic parsing , actively resolves ambiguities using a dialog manager, and incrementally learns from human-robot conversations by inducing training data from user paraphrases. Our dialog agent is implemented and tested both on a web interface with hundreds of users via Mechanical Turk and on a mobile robot over several days, tasked with understanding navigation and delivery requests through natural language in an office environment. In both contexts, We observe significant improvements in user satisfaction after learning from conversations.},
author = {Thomason, Jesse and Zhang, Shiqi and Mooney, Raymond and Stone, Peter},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Thomason et al. - 2015 - Learning to Interpret Natural Language Commands through Human-Robot Dialog.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Thomason et al. - 2015 - Learning to Interpret Natural Language Commands through Human-Robot Dialog(2).pdf:pdf},
keywords = {Technical Papers — Robotics and Vision},
number = {Ijcai},
pages = {1923--1929},
title = {{Learning to Interpret Natural Language Commands through Human-Robot Dialog}},
url = {https://www.mturk.com},
year = {2015}
}
@inproceedings{Tam1998,
abstract = {Eliciting user-task models is a thorny problem in model-based user interface design, and communicating domain-specific knowledge from an expert to a knowledge engineer is a continuing problem in knowledge acquisition. We devised a task elicitation method that capitalizes on a domain expert's ability to describe a task in plain English, and on a knowledge engineer's skills to formalize it. The method bridges the gap between the two by helping the expert refine the description and by giving the engineer clues to its structure. We implemented and evaluated an interactive tool called the User-Task Elicitation Tool (U-TEL) to elicit user-task models from domain experts based on our methodology. Via direct manipulation, U-TEL provides capabilities for word processing, keyword classification, and outline refinement. By using U-TEL, domain experts can refine a textual specification of a user task into a basic user-task model suitable for use in model-based interface development environments. Our evaluation shows that U-TEL can be used effectively by domain experts with or without a background in programming or interface modeling, and that the tool can be a key element in promoting user-centered interface design in model-based systems. Keywords User-centered design, model-based user interface design, task models, knowledge elicitation INTRODUCTION In this paper, we present and evaluate a tool for eliciting informal user-task models from domain experts, in order to guide the design of a user interface. These models are},
author = {Tam, Chung-Man and Maulsby, David and Puerta, Angel R},
booktitle = {Proceedings of the 3rd international conference on Intelligent user interfaces},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tam, Maulsby, Puerta - 1998 - U-TEL A Tool for Eliciting User Task Models from Domain Experts.pdf:pdf},
title = {{U-TEL: A Tool for Eliciting User Task Models from Domain Experts}},
url = {http://www.aurelium.comhttp//smi.stanford.edu/people/puerta},
year = {1998}
}
@article{Minhas2017a,
abstract = {{\textcopyright} 2016 IEEE. End users' requirements should be incorporated in the software, and service based ephemeral applications are no exception. Mashup development is facing different challenges such as inherent tension between the non-Technical end user and complexity demands of the end user mashup development tools. We believe meta design can address these challenges. Meta design tends to provide a road map for innovative computational environments beyond the user-centered and participatory design. It seeks to bring the end user at the heart of software development by coining ideas such as co-designing at use-Time, constant evolution of systems at the hand of users and Seeding, evolutionary growth and reseeding model (SER). There has been already some ideas generation regarding using meta-design principles for mashup development. We have further explored and assessed meta-design principles and have proposed a concrete solution in the form of a process model that can provide a way forward to overcome the predicaments faced by end users who wish to program for themselves at a comfortably abstract level in an evolutionary fashion. We present the process model and describe a motivational real-world scenario for validation purpose followed by initial observations of the experiment conducted.},
author = {Minhas, Sumaira S. and Sampaio, Pedro and Mehandjiev, Nikolay},
doi = {10.1109/INMIC.2016.7840108},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Minhas, Sampaio, Mehandjiev - 2017 - Proposing a new process model for mashup development by applying meta-design principles and goals f.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Minhas, Sampaio, Mehandjiev - 2017 - Proposing a new process model for mashup development by applying meta-design principles and goal(2).pdf:pdf},
isbn = {9781509043002},
journal = {Proceedings of the 2016 19th International Multi-Topic Conference, INMIC 2016},
keywords = {End User Development,End User requirements,Mashup},
pages = {1--5},
publisher = {IEEE},
title = {{Proposing a new process model for mashup development by applying meta-design principles and goals for managing end user expectations}},
year = {2017}
}
@article{Myers2004a,
author = {Myers, Brad A. and Pane, John F. and Ko, Andy},
doi = {10.1145/1015864.1015888},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Myers, Pane, Ko - 2004 - Natural Programming Languages and Environments(2).pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {sep},
number = {9},
pages = {47},
publisher = {ACM},
title = {{Natural programming languages and environments}},
url = {http://portal.acm.org/citation.cfm?doid=1015864.1015888},
volume = {47},
year = {2004}
}
@article{Atzeni2018,
abstract = {Despite the demand for increasing automation within specified tasks by a large spectrum of different users, software development is still a complex task mainly oriented to professional programmers. Often, the exploration and understanding of large code bases is also a difficult task for experienced developers. Recently, semantic parsers and advances in research areas primarily investigated within the field of natural language human-robot interaction, have shown to be potentially useful for end-user development supported by natural language communication. Hence, this paper provides a structured review and categorization of approaches to ease software development, both for professional software programmers and for end-users with no prior programming skills. We then focus on semantic developments based on natural language understanding and on a comparison between the outlined approaches.},
author = {Atzeni, Mattia and Atzori, Maurizio},
doi = {10.1109/IRC.2018.00077},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Atzeni, Atzori - 2018 - Towards semantic approaches for general-purpose end-user development(2).pdf:pdf},
isbn = {9781538646519},
journal = {Proceedings - 2nd IEEE International Conference on Robotic Computing, IRC 2018},
keywords = {End User Development,Natural Language Processing,Robotic Commands,Semantic Parsers,Semantic Robots},
pages = {369--376},
publisher = {IEEE},
title = {{Towards semantic approaches for general-purpose end-user development}},
volume = {2018-Janua},
year = {2018}
}
@article{Steyvers2012,
abstract = {Slides},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Steyvers, M and Griffiths, T},
doi = {10.1109/TKDE.2009.122},
eprint = {1111.6189v1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Steyvers, Griffiths - 2012 - Probabilistic topic models.pdf:pdf},
isbn = {0805854185},
issn = {10414347},
journal = {Communications of the ACM},
number = {4},
pages = {424--440},
pmid = {9982802760707177758},
title = {{Probabilistic topic models}},
volume = {55},
year = {2012}
}
@article{Chudnovskyy2012e,
abstract = {End-user-development (EUD) has been a field of study for more than 30 years already. The results are visible - users, who have no or only little programming skills, have become active creators of Web applications, developing new tools to meet their situational needs, sharing them with colleagues and combining them into more complex solutions. Recent trends, like maturation of cloud computing, mass customization and changing demographics resulted in even higher demand for flexible, feature-rich and extensible platforms for end-user development. While the potential of involving end-users into developing task is extremely high, a clear need for new systematic methods has emerged, which would take both the new technological opportunities but also risks resulting from non-professional application development into account. In this paper, we present our research towards systematic, end-user-oriented Web application development. We identify relevant research challenges; derive requirements on EUD-oriented development process and show how the WebComposition approach can be extended to support end-users during the whole life-cycle of Web applications - from requirements elicitation to evolution. {\textcopyright} 2012 Springer-Verlag.},
author = {Chudnovskyy, Olexiy and Gaedke, Martin},
doi = {10.1007/978-3-642-35623-0_23},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chudnovskyy, Gaedke - 2012 - End-user-development and evolution of web applications The WebComposition EUD approach.pdf:pdf},
isbn = {9783642356223},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {221--226},
title = {{End-user-development and evolution of web applications: The WebComposition EUD approach}},
volume = {7703 LNCS},
year = {2012}
}
@article{G2004,
abstract = {End-user development (EUD) activities range from customization to component configuration and programming. Office software, such as the ubiquitous spreadsheet, provide customisation facilities, while the growth of the Web has added impetus to end-user scripting for interactive functions in websites. In scientific and engineering domains end users frequently develop complex systems with standard programming languages such as C++ and JAVA. However only a minority of users adapt COTS (Customer Off The Shelf software) products; furthermore, composing systems from reusable components, such as ERP (Enterprise Resource Plans) systems defeats most end users, who resort to expensive and scarce expert developers for implementation. So EUD is only a partial success story. We argue that the spread of end-user development depends on a fine balance between user motivation, effective tools and management support. In this article we explore that balance and investigate a future approach to EUD -- meta-design -- that proposes a vision in which design, learning and development become part of everyday working practice.},
author = {G, Fischer and E, Giaccardi and G, Sutcliffe a and N, Mehandjiev},
doi = {10.1145/1015864.1015884},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/G et al. - 2004 - Meta-Design A Manifesto for End-User Development.pdf:pdf},
isbn = {0001-0782},
issn = {0001-0782},
journal = {Communications of the ACM},
pages = {1--7},
title = {{Meta-Design : A Manifesto for End-User Development}},
url = {http://portal.acm.org/citation.cfm?id=1015884},
volume = {47},
year = {2004}
}
@article{Zhou2017,
abstract = {Crown This paper investigates End-User Development (EUD) for interactive data-analytic interfaces {\&}{\#}x2013; building upon the ideas of making machine learning transparent. The research is carried out in a business operation environment (water pipe failure prediction in our case) motivated to integrate advanced analytics into decision-making processes of an urban Internet of Things (IoT) concept. We explore effects of revealing uncertainty and correlation on user confidence in a data-driven decision making scenario. It was found that user confidence varied significantly amongst various user groups when different machine learning models were displayed with/without supplementary information. Galvanic Skin Response (GSR) signals were analyzed and shown as reasonable indices for predicting user confidence levels. Supplementary data visualizations (of inherent uncertainty and correlation in data) contributed to explicability principles while GSR indexing added towards correctibility principles. We recommend transparent machine learning as the key to effective EUD for interactive data analytics.},
author = {Zhou, Jianlong and Arshad, Syed Z. and Wang, Xiuying and Li, Zhidong and Feng, David Dagan and Chen, Fang},
doi = {10.1109/TAFFC.2017.2723402},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - 2017 - End-User Development for Interactive Data Analytics Uncertainty, Correlation and User Confidence.pdf:pdf},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Correlation,Data analysis,Decision making,Galvanic Skin Response,Human computer interaction,Skin,Uncertainty,decision making,machine learning,uncertainty,user confidence},
number = {3},
pages = {383--395},
publisher = {IEEE},
title = {{End-User Development for Interactive Data Analytics: Uncertainty, Correlation and User Confidence}},
volume = {9},
year = {2017}
}
@article{Samsudin2017,
abstract = {Ontology defines entity and their relationship among knowledge concepts within a problem domain that may provide better specifications of a system blue-print. Such basic specification can provide an interoperable format that enhances understand-ability to both human users and developers. This makes the ontology as one of the prominent techniques specially to systemize problem domains. In this paper, we describe importance of a domain ontology development, reporting on our experience in a user-oriented applications development project. The project was designed to address issues related to both human users' access to and developers' understandability on developed record management systems. Through a formal methodology we developed a domain ontology model and promoted its unique significance specially for developing system solution. Findings from our studies suggest that ontology enables provisions of design support both to end users and system developers to reflect user's real requirements that in turn maximize various user benefits.},
author = {Samsudin, Ahmad Z.H. and Shahibi, Mohd Sazili and Miah, Shah Jahan and Islam, Hafizul},
doi = {10.1109/APWC-on-CSE.2016.043},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Samsudin et al. - 2017 - Development of Domain Ontology for Enhancing Design Understand-Ability for Developers and End-Users.pdf:pdf},
isbn = {9781509057535},
journal = {Proceedings - Asia-Pacific World Congress on Computer Science and Engineering 2016 and Asia-Pacific World Congress on Engineering 2016, APWC on CSE/APWCE 2016},
keywords = {Domain ontology,Ontology-driven applications,System research},
pages = {212--217},
publisher = {IEEE},
title = {{Development of Domain Ontology for Enhancing Design Understand-Ability for Developers and End-Users}},
year = {2017}
}
@article{Jia2017,
abstract = {Conversational agents are often used to perform tasks on smartphones, but existing conversational agents are limited in capabilities and lack of customizability. My work explores using the programming-by-demonstration approach to enable end users to program new tasks for conversational agents by demonstrating using the familiar graphical user interfaces of third-party apps. I propose to use a multi-modal (demonstration and verbal instruction) interface to support generalization, editing, error handling as well as creating control structures in creating such smartphone automation.},
author = {Jia, Toby and Li, Jun},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jia, Li - 2017 - End User Mobile Task Automation using Multimodal Programming by Demonstration.pdf:pdf},
isbn = {9781538604434},
keywords = {SUGILITE,end user development,programming by demonstration,task automation},
pages = {323--324},
title = {{End User Mobile Task Automation using Multimodal Programming by Demonstration}},
url = {http://llamalab.com/automate/},
year = {2017}
}
@article{Lizcano2011,
abstract = {Background: Previous in vivo studies have shown that mesenchymal stem cell (MSC) transplantation significantly improves the condition of a number of autoimmune diseases including autoimmune cerebrospinal meningitis, multiple sclerosis, glomerulonephritis and systemic lupus erythematosus.},
author = {Lizcano, David and Alonso, Fernando and Soriano, Javier and L{\'{o}}pez, Genoveva},
doi = {10.1186/1471-2474-13-249},
file = {:Users/baharehzarei/Downloads/icons{\_}2011{\_}6{\_}10{\_}20081 (1).pdf:pdf},
isbn = {9781612081144},
journal = {6th International Conference on Systems (ICONS)},
keywords = {-End-user Development,Composite Applications,Future Internet,Internet of Services,Service Front-Ends,User-Centred Service-Oriented Architectures},
number = {c},
pages = {99--108},
title = {{End-User Development Success Factors and their Application to Composite Web Development Environments}},
url = {http://oa.upm.es/12511/3/INVE{\_}MEM{\_}2011{\_}105406.pdf},
year = {2011}
}
@article{Sales2018,
author = {Sales, Juliano Efson and Freitas, Andr{\'{e}} and Handschuh, Siegfried},
doi = {10.1109/ICSC.2018.00020},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sales, Freitas, Handschuh - 2018 - An Open Vocabulary Semantic Parser for End-User Programming Using Natural Language.pdf:pdf},
isbn = {9781538644072},
journal = {Proceedings - 12th IEEE International Conference on Semantic Computing, ICSC 2018},
keywords = {end user programming,natural language programming,open vocabulary,semantic parsing},
pages = {77--84},
publisher = {IEEE},
title = {{An Open Vocabulary Semantic Parser for End-User Programming Using Natural Language}},
volume = {2018-Janua},
year = {2018}
}
@article{Sutcliffe2003,
author = {Sutcliffe, Alistair and Lee, Darren and Mehandjiev, Nik and Box, P O and Manchester, M},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sutcliffe et al. - 2003 - Contributions , Costs and Prospects for End-User Development Department of Computation , UMIST 1 Introduction.pdf:pdf},
journal = {IST PROGRAMME},
number = {1},
title = {{Contributions , Costs and Prospects for End-User Development Department of Computation , UMIST 1 Introduction 2 Definitions and Concepts}},
year = {2003}
}
@article{Systems,
author = {Systems, Enterprise and Spahn, Michael and Wulf, Volker},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Systems, Spahn, Wulf - Unknown - End User Development of Information Artefacts.pdf:pdf},
title = {{End User Development of Information Artefacts}}
}
@article{DeAngeli2011,
author = {{De Angeli}, A and Battocchi, Alberto},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/De Angeli, Battocchi - 2011 - Conceptual Design and Evaluation of WIRE A Wisdom-Aware EUD Tool.pdf:pdf},
keywords = {contextual help,eud requirements,interactive advice,mashups,wisdom-aware development},
number = {March},
title = {{Conceptual Design and Evaluation of WIRE: A Wisdom-Aware EUD Tool}},
url = {http://eprints.biblio.unitn.it/2074/},
year = {2011}
}
@article{Paterno2017,
abstract = {The research field of end-user development has evolved, during recent years, to a certain degree of internal structure, problem awareness and consistency. Both academia and industry have begun to consider it an important field for research and development. In order to let EUD research contribute to the Information Societies, research and development must continue in a consolidated and well-balanced way. This chapter provides an overview of major challenges, motivates why these challenges should be addressed with considerable effort to bring about an Information Society with empowered end-users, and finally discusses how these challenges should be translated into a concrete research and development agenda for the short- and mid-term future. Key words. tailorability, end user programming, flexibility, usability},
author = {Patern{\`{o}}, Fabio and Wulf, Volker},
doi = {10.1007/978-3-319-60291-2},
file = {:Users/baharehzarei/Downloads/2016EUDevaluationbookTetteroov2.pdf:pdf},
isbn = {9783319602912},
journal = {New Perspectives in End-User Development},
number = {August},
pages = {1--459},
title = {{New perspectives in end-user development}},
year = {2017}
}
@article{Roßner2018a,
author = {Ro{\ss}ner, Daniel and Atzenbeck, Claus},
doi = {10.1145/3215611.3215612},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ro{\ss}ner, Atzenbeck - 2018 - Spatial Hypertext for End-User Development Tools.pdf:pdf},
isbn = {9781450356589},
journal = {Proceedings of the 1st Workshop on Human Factors in Hypertext  - HUMAN '18},
keywords = {CB-OHS,End-User Development,HCI,Mother,acm reference format,cb-ohs,end-user development,hci,mother,open hypermedia systems,spatial hypertext},
pages = {9--15},
title = {{Spatial Hypertext for End-User Development Tools}},
url = {http://dl.acm.org/citation.cfm?doid=3215611.3215612},
year = {2018}
}
@article{Munir2018b,
abstract = {The dramatic increase in the use of knowledge discovery applications requires end users to write complex database search requests to retrieve information. Such users are not only expected to grasp the structural complexity of complex databases but also the semantic relationships between data stored in databases. In order to overcome such difficulties, researchers have been focusing on knowledge representation and interactive query generation through ontologies, with particular emphasis on improving the interface between data and search requests in order to bring the result sets closer to users research requirements. This paper discusses ontology-based information retrieval approaches and techniques by taking into consideration the aspects of ontology modelling, processing and the translation of ontological knowledge into database search requests. It also extensively compares the existing ontology-to-database transformation and mapping approaches in terms of loss of data and semantics, structural mapping and domain knowledge applicability. The research outcomes, recommendations and future challenges presented in this paper can bridge the gap between ontology and relational models to generate precise search requests using ontologies. Moreover, the comparison presented between various ontology-based information retrieval, database-to-ontology transformations and ontology-to-database mappings approaches provides a reference for enhancing the searching capabilities of massively loaded information management systems.},
author = {Munir, Kamran and {Sheraz Anjum}, M.},
doi = {10.1016/j.aci.2017.07.003},
file = {:Users/baharehzarei/Downloads/1-s2.0-S2210832717300649-main.pdf:pdf},
issn = {22108327},
journal = {Applied Computing and Informatics},
keywords = {Database,Domain knowledge,Information retrieval,Information systems,Knowledge management,Ontology},
number = {2},
pages = {116--126},
pmid = {20932598},
publisher = {King Saud University},
title = {{The use of ontologies for effective knowledge modelling and information retrieval}},
url = {https://doi.org/10.1016/j.aci.2017.07.003},
volume = {14},
year = {2018}
}
@article{Constantin2018b,
abstract = {Recently advancements in deep learning allowed the development of end-to-end trained goal-oriented dialog systems. Although these systems already achieve good performance, some simplifications limit their usage in real-life scenarios. In this work, we address two of these limitations: ignoring positional information and a fixed number of possible response candidates. We propose to use positional encodings in the input to model the word order of the user utterances. Furthermore, by using a feedforward neural network, we are able to generate the output word by word and are no longer restricted to a fixed number of possible response candidates. Using the positional encoding, we were able to achieve better accuracies in the Dialog bAbI Tasks and using the feedforward neural network for generating the response, we were able to save computation time and space consumption.},
archivePrefix = {arXiv},
arxivId = {1803.02279},
author = {Constantin, Stefan and Niehues, Jan and Waibel, Alex},
doi = {10.1073/pnas.0607180103},
eprint = {1803.02279},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Constantin, Niehues, Waibel - 2018 - An End-to-End Goal-Oriented Dialog System with a Generative Natural Language Response Generation.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
pmid = {17108084},
title = {{An End-to-End Goal-Oriented Dialog System with a Generative Natural Language Response Generation}},
url = {http://arxiv.org/abs/1803.02279},
year = {2018}
}
@article{Zhang2011a,
abstract = {By constructing a framework of knowledge management system based on ontology, this paper expounds the function of each layer, and analyses the implementation of this system from the knowledge organization and expression and knowledge retrieval. Finally, it provides a case which implements the management system and realizes some parts of retrieval modules. This management system establishes a sharable ontology that can be understood both by human and computer, which people can found more relations of different concepts through a better circumstance of knowledge retrieval interface. In addition, the system is also open to some extent, so it can accumulate tacit knowledge constantly and polymerize explicit knowledge efficiently, which can lead to a better management and application of knowledge, to support the innovation for the designers. {\textcopyright} 2011 Published by Elsevier Ltd.},
author = {Zhang, Junsong and Zhao, Wu and Xie, Gang and Chen, Hong},
doi = {10.1016/j.proeng.2011.08.189},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2011 - Ontology-based knowledge management system and application.pdf:pdf},
isbn = {1877-7058},
issn = {18777058},
journal = {Procedia Engineering},
keywords = {Knowledge management,Knowledge organization and expression,Knowledge retrieval,Ontology},
pages = {1021--1029},
title = {{Ontology-based knowledge management system and application}},
url = {http://dx.doi.org/10.1016/j.proeng.2011.08.189},
volume = {15},
year = {2011}
}
@article{Borowski2006,
author = {Borowski, Richard A.},
doi = {10.1007/1-4020-5386-X},
file = {:Users/baharehzarei/Downloads/Future{\_}Perspectives{\_}in{\_}End-User{\_}Development.pdf:pdf},
isbn = {978-1-4020-4220-1},
journal = {SAE Technical Papers},
number = {January},
title = {{FUTURE PERSPECTIVES IN END-USER DEVELOPMENT}},
year = {2006}
}
@article{Pantazos2016b,
abstract = {{\textcopyright} 2016 Society for Imaging Science and Technology.In this article the authors investigated a visualization tool (uVis) for end-user developers, in order to see how end users actually use it. The tool was an early version and the investigation helped the authors to improve it. The investigation showed that users appreciated the simple formula language, the coordinated panels, and the drag-and-drop mechanism. However, the most important thing for them was the immediate response when they changed something, for instance part of a formula. The entire visualization was updated immediately without having to switch from development view to production view. With uVis, developers construct a visualization from simple visual components such as boxes, curvePoints, and textboxes. All component properties such as Top and BackColor can be complex formulas similar to spreadsheet formulas. The operands in the formula can address relational data in a database, other visual objects, and dialog data provided by the user. A special Rows property can bind to a database query and make the component replicate itself for each row in the query. In this way, traditional as well as novel visualizations can be constructed. The most serious usability problems were data binding and not noticing errors (errors were shown in an error list, but not in the formula that had the error). There were many other usability problems. Removing them would speed up learning and make the tool more successful.},
author = {Pantazos, Kostas and Lauesen, Soren},
doi = {10.2352/J.ImagingSci.Technol.2016.60.1.010408},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pantazos, Lauesen - 2016 - End-User Development of Visualizations.pdf:pdf},
issn = {10623701},
journal = {Journal of Imaging Science and Technology},
number = {1},
pages = {104081--1040810},
title = {{End-User Development of Visualizations}},
url = {http://www.ingentaconnect.com/contentone/ist/jist/2016/00000060/00000001/art00009},
volume = {60},
year = {2016}
}
@inproceedings{Eagan2008,
address = {New York, New York, USA},
author = {Eagan, James R. and Stasko, John T.},
booktitle = {Proceeding of the twenty-sixth annual CHI conference on Human factors in computing systems - CHI '08},
doi = {10.1145/1357054.1357324},
isbn = {9781605580111},
pages = {1729},
publisher = {ACM Press},
title = {supporting user tailorability in awareness applications},
url = {http://portal.acm.org/citation.cfm?doid=1357054.1357324},
year = {2008}
}
@article{Scaffidi2005,
abstract = {In 1995, Boehm predicted that by 2005, there would be "55 million performers" of "end user programming" in the United States. The original context and method which generated this number had two weaknesses, both of which we address. First, it relies on undocumented, judgment based factors to estimate the number of end user programmers based on the total number of end users; we address this weakness by identifying specific end user sub populations and then estimating their sizes. Second, Boehm's estimate relies on additional undocumented, judgment based factors to adjust for rising computer usage rates; we address this weakness by integrating fresh Bureau of Labor Statistics (BLS) data and projections as well as a richer estimation method. With these improvements to Boehm{\'{y}}s method, we estimate that in 2012 there will be 90 million end users in American workplaces. Of these, we anticipate that over 55 million will use spreadsheets or databases (and therefore may potentially program), while over 13 million will describe themselves as programmers, compared to BLS projections of fewer than 3 million professional programmers. We have validated our improved method by generating estimates for 2001 and 2003, then verifying that our estimates are consistent with existing estimates from other sources.},
author = {Scaffidi, Christopher and Shaw, Mary and Myers, Brad},
doi = {10.1109/VLHCC.2005.34},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Scaffidi, Shaw, Myers - 2005 - Estimating the numbers of end users and end user programmers.pdf:pdf},
isbn = {0-7695-2443-5},
issn = {0769524435},
journal = {Proceedings - 2005 IEEE Symposium on Visual Languages and Human-Centric Computing},
pages = {207--214},
title = {{Estimating the numbers of end users and end user programmers}},
volume = {2005},
year = {2005}
}
@article{Mackowiak2018,
abstract = {Context: End-user programming is becoming more and more important. However, existing programming paradigms and the languages based on them seem far-removed from what end-user programmers would need, especially in the area of Management Information. Objective: To evaluate the understandability of a set of programming constructs based on the spreadsheet metaphor from the point of view of end-user programmers in the context of Management Information. The examined set comprises single assignment with exemplary computations, data-driven iterations, selection by colours, and read-write heads (we refer to this as Board Programming). Method: A series of experiments was performed with students of Management Engineering, split into an experimental group and a control group. Each participant was given a piece of code expressed either with the proposed programming construct (experimental group) or its classical counterpart (control group). Their task was to predict the results. For the purpose of evaluation, the FACT indicators of understandability (First attempt failure rate, Attempt number, Cancellation ratio, prediction Time) were proposed and measured. Results: Three of the four examined features, i.e. single assignment with exemplary computations, data-driven iterations, and read-write heads, proved to increase understandability of the chosen programs with regard to three out of the four FACT indicators, and these results were statistically significant. Selection by colours was not as effective as expected: the FACT indicator values were improved by that feature, but the difference was not statistically significant. Conclusions: The described programming constructs appear to be an interesting option when designing an end-user programming language for domain experts in the field of Management Information.},
author = {Ma{\'{c}}kowiak, Micha{\l} and Nawrocki, Jerzy and Ochodek, Miros{\l}aw},
doi = {10.1016/j.jss.2018.03.064},
file = {:Users/baharehzarei/Downloads/1-s2.0-S0164121218300633-main.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Code understanding,End-user programming,Spreadsheets},
pages = {206--222},
title = {{On some end-user programming constructs and their understandability}},
volume = {142},
year = {2018}
}
@inproceedings{Scaffidi2005a,
address = {Dallas},
author = {Scaffidi, C. and Shaw, M. and Myers, B.},
booktitle = {2005 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC'05)},
doi = {10.1109/VLHCC.2005.34},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Scaffidi, Shaw, Myers - 2005 - Estimating the numbers of end users and end user programmers.pdf:pdf},
isbn = {0-7695-2443-5},
pages = {207--214},
publisher = {IEEE},
title = {{Estimating the Numbers of End Users and End User Programmers}},
url = {http://ieeexplore.ieee.org/document/1509505/},
year = {2005}
}
@inproceedings{Rosson2007,
author = {Rosson, Mary Beth and Sinha, Hansa and Bhattacharya, Mithu and Zhao, Dejin},
booktitle = {IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2007)},
doi = {10.1109/VLHCC.2007.45},
isbn = {0-7695-2987-9},
month = {sep},
pages = {189--196},
publisher = {IEEE},
title = {{Design Planning in End-User Web Development}},
url = {http://ieeexplore.ieee.org/document/4351347/},
year = {2007}
}
@article{Duarte2016,
archivePrefix = {arXiv},
arxivId = {236},
author = {Duarte, Emanuel Felipe and Baranauskas, M. Cec{\'{i}}lia C.},
doi = {10.1145/3033701.3033740},
eprint = {236},
file = {:Users/baharehzarei/Downloads/2016-RevisitingtheThreeHCIWaves-DuarteBaranauskas.pdf:pdf},
isbn = {9781450352352},
issn = {09242244},
journal = {Proceedings of the 15th Brazilian Symposium on Human Factors in Computer Systems  - IHC '16},
number = {October},
pages = {1--4},
title = {{Revisiting the Three HCI Waves}},
url = {http://dl.acm.org/citation.cfm?doid=3033701.3033740},
year = {2016}
}
@inproceedings{Bødker2006,
abstract = {This paper surveys the current status of second generation HCI theory, faced with the challenges brought to HCI by the so-called third wave. In the third wave, the use context and application types are broadened, and intermixed, relative to the focus of the second wave on work. Technology spreads from the workplace to our homes and everyday lives and culture. Using these challenges the paper specifically addresses the topics of multiplicity, context, boundaries , experience and participation in order to discuss where second wave theory and conceptions can still be positioned to make a contribution as part of the maturing of our handling of the challenges brought on by the third wave.},
author = {B{\o}dker, Susanne},
booktitle = {Proceedings of the 4th Nordic conference on Human-computer interaction: changing roles},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/B{\o}dker - Unknown - When second wave HCI meets third wave challenges.pdf:pdf},
keywords = {Author Keywords Multiplicity,HCI): Miscellaneous,boundaries,context,experience,participation ACM Classification Keywords H5m Info},
pages = {1--8},
publisher = {ACM},
title = {{When second wave HCI meets third wave challenges}},
url = {https://pure.au.dk/ws/files/93115128/nordichipreprint.pdf},
year = {2006}
}
@article{Henderson1995,
abstract = {Design is a process that is tightly coupled to use and that continues during the use of a system. This chapter discusses what may be involved in continuing design in use, and how one, in the initial design process, may create systems that are tailorable. Ideal tailorable systems are those in which there are means for the users, or supporters near the users, to make them fit different work situations. Tailoring a system, continuing designing in use, is an activity different from initial design. The activity is related to specific use situations and the result is not a new system, but a modified system, that is, a system with a history that relates it to the earlier version and problems with its use. The chapter also discusses various activities involved in modifying computer systems.},
author = {Henderson, Austin and Kyng, Morten},
doi = {10.1016/B978-0-08-051574-8.50082-0},
isbn = {9780080515748},
journal = {Readings in Human–Computer Interaction},
month = {jan},
pages = {793--803},
publisher = {Morgan Kaufmann},
title = {{There's No Place Like Home: Continuing Design in Use}},
url = {https://www.sciencedirect.com/science/article/pii/B9780080515748500820},
year = {1995}
}
@inproceedings{Maceli2017,
abstract = {The fundamental ideas core to the field of end-user development (EUD) emerged in the early 1990s with influential authors advocating for the need for modifiable software that could be crafted by the end user. The modern technology landscape that emerged in the intervening years is vastly different – with technology interwoven into every aspect of our lives and becoming increasingly malleable. In pursuit of building our understanding of what tech- nology tools currently demonstrate EUD concepts and how this has changed over time, this work reports on the results of an assessment of 73 research articles from EUD publication venues, from 2004 to 2016, which emphasize the original development or extension of a technology tool. The technology tools largely fell into the categories of programming environments and frameworks, web and information authoring tools, mashup-creation and spreadsheet tools, with a diverse range of relatively infrequent tool types observed as well.},
author = {Maceli, Monica G.},
booktitle = {6th International Symposium, IS-EUD 2017},
doi = {10.1007/978-3-319-58735-6},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Maceli - 2017 - Tools of the Trade A Survey of Technologies in End-User Development Literature(2).pdf:pdf},
isbn = {978-3-319-58734-9},
issn = {00010782},
keywords = {End-user development ? Technology tools ? Literatu},
pages = {71--75},
pmid = {23619695},
publisher = {Springer International Publishing AG 2017},
title = {{Tools of the Trade: A Survey of Technologies in End-User Development Literature}},
url = {http://portal.acm.org/citation.cfm?doid=1370847.1370863},
year = {2017}
}
@incollection{Lieberman2006b,
address = {Dordrecht},
author = {Lieberman, Henry and Patern{\`{o}}, Fabio and Klann, Markus and Wulf, Volker},
booktitle = {End User Development},
doi = {10.1007/1-4020-5386-X_1},
pages = {1--8},
publisher = {Springer Netherlands},
title = {{End-User Development: An Emerging Paradigm}},
url = {http://link.springer.com/10.1007/1-4020-5386-X{\_}1},
year = {2006}
}
@article{Sayed2017,
abstract = {The vast availability of information, that added in a very fast pace, in the data repositories creates a challenge in extracting correct and accurate information. Which has increased the competition among developers in order to gain access to technology that seeks to understand the intent researcher and contextual meaning of terms. While the competition for developing an Arabic Semantic Search systems are still in their infancy, and the reason could be traced back to the complexity of Arabic Language. It has a complex morphological, grammatical and semantic aspects, as it is a highly inflectional and derivational language. In this paper, we try to highlight and present an Ontological Search Engine called IBRI-CASONTO for Colleges of Applied Sciences, Oman. Our proposed engine supports both Arabic and English language. It is also employed two types of search which are a keyword-based search and a semantics-based search. IBRI-CASONTO is based on different technologies such as Resource Description Framework (RDF) data and Ontological graph. The experiments represent in two sections, first it shows a comparison among Entity-Search and the Classical-Search inside the IBRI-CASONTO itself, second it compares the Entity-Search of IBRI-CASONTO with currently used search engines, such as Kngine, Wolfram Alpha and the most popular engine nowadays Google, in order to measure their performance and efficiency.},
author = {Sayed, Awny and {Al Muqrishi}, Amal},
doi = {10.1016/j.eij.2017.01.001},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sayed, Al Muqrishi - 2017 - IBRI-CASONTO Ontology-based semantic search engine.pdf:pdf},
isbn = {1-57230-828-1},
issn = {11108665},
journal = {Egyptian Informatics Journal},
keywords = {Keyword-based search,Ontological graph,Ontological search engine,Resource Description Framework (RDF),Semantics-based search},
number = {3},
pages = {181--192},
publisher = {Faculty of Computers and Information, Cairo University},
title = {{IBRI-CASONTO: Ontology-based semantic search engine}},
url = {https://doi.org/10.1016/j.eij.2017.01.001},
volume = {18},
year = {2017}
}
@article{Li,
author = {Li, Toby Jia-jun and Labutov, Igor and Myers, Brad A and Azaria, Amos and Alexander, I and Mitchell, Tom M},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - An End User Development Approach for Failure Handling in Goal-oriented Conversational Agents.pdf:pdf},
pages = {1--19},
title = {{An End User Development Approach for Failure Handling in Goal-oriented Conversational Agents}}
}
@article{Zhang2018a,
abstract = {With the broad adoption of service-oriented architecture, many software systems have been developed by composing loosely-coupled Web services. Service discovery, a critical step of building service-based systems (SBSs), aims to find a set of candidate services for each functional task to be performed by an SBS. The keyword-based search technology adopted by existing service registries is insufficient to retrieve semantically similar services for queries. Although many semantics-aware service discovery approaches have been proposed, they are hard to apply in practice due to the difficulties in ontology construction and semantic annotation. This paper aims to help service requesters (e.g., SBS designers) obtain relevant services accurately with a keyword query by exploiting domain knowledge about service functionalities (i.e., service goals) mined from textual descriptions of services. We firstly extract service goals from services' textual descriptions using an NLP-based method and cluster service goals by measuring their semantic similarities. A query expansion approach is then proposed to help service requesters refine initial queries by recommending similar service goals. Finally, we develop a hybrid service discovery approach by integrating goal-based matching with two practical approaches: keyword-based and topic model-based. Experiments conducted on a real-world dataset show the effectiveness of our approach.},
author = {Zhang, Neng and Wang, Jian and Ma, Yutao and He, Keqing and Li, Zheng and Liu, Xiaoqing (Frank (Frank)},
doi = {10.1016/j.jss.2018.04.046},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Web service discovery based on goal-oriented query expansion.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Query expansion,Service discovery,Service goal knowledge,Service-based system (SBS),Web service},
month = {aug},
pages = {73--91},
publisher = {Elsevier},
title = {{Web service discovery based on goal-oriented query expansion}},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300748},
volume = {142},
year = {2018}
}
@article{Wu2009,
author = {Wu, Qing and Li, Ying},
doi = {10.1109/WCSE.2009.824},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Li - 2009 - An adaptive and semantic component model for adaptive middleware in ubiquitous computing environments.pdf:pdf},
isbn = {9780769538815},
journal = {2nd International Workshop on Computer Science and Engineering, WCSE 2009},
number = {1},
pages = {333--337},
title = {{An adaptive and semantic component model for adaptive middleware in ubiquitous computing environments}},
volume = {2},
year = {2009}
}
@article{Atzeni2018a,
abstract = {Despite the demand for increasing automation within specified tasks by a large spectrum of different users, software development is still a complex task mainly oriented to professional programmers. Often, the exploration and understanding of large code bases is also a difficult task for experienced developers. Recently, semantic parsers and advances in research areas primarily investigated within the field of natural language human-robot interaction, have shown to be potentially useful for end-user development supported by natural language communication. Hence, this paper provides a structured review and categorization of approaches to ease software development, both for professional software programmers and for end-users with no prior programming skills. We then focus on semantic developments based on natural language understanding and on a comparison between the outlined approaches.},
author = {Atzeni, M and Atzori, M},
doi = {10.1109/IRC.2018.00077},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Atzeni, Atzori - 2018 - Towards Semantic Approaches for General-Purpose End-User Development.pdf:pdf},
isbn = {0769563708},
journal = {2018 Second IEEE International Conference on Robotic Computing (IRC)},
keywords = {Data mining,End User Development,Natural Language Processing,Natural languages,Programming,Robotic Commands,Semantic Parsers,Semantic Robots,Semantics,Software,Task analysis,Visualization,experienced developers,general-purpose end-user development,human-robot interaction,interactive systems,natural language communication,natural language human-robot interaction,natural language processing,natural language understanding,professional programmers,professional software programmers,semantic developments,semantic parsers,software development,software engineering,structured review},
pages = {369--376},
title = {{Towards Semantic Approaches for General-Purpose End-User Development}},
year = {2018}
}
@article{DeKeukelaere2008,
abstract = {Mashup applications mix and merge content (data and code) from multiple content providers in a user's browser, to pro- vide high-value web applications that can rival the user ex- perience provided by desktop applications. Current browser security models were not designed to support such appli- cations and they are therefore implemented with insecure workarounds. In this paper, we present a secure component model, where components are provided by different trust do- mains, and can interact using a communication abstraction that allows ease of specification of a security policy. We have developed an implementation of this model that works currently in all major browsers, and addresses challenges of communication integrity and frame-phishing. An evaluation of the performance of our implementation shows that this approach is not just feasible but also practical. The tech- nology discussed in this paper allows mutually mistrusting client-side components to communicate safely without any modifications to current browsers, and hence has the poten- tial to achieve immediate and widespread adoption.},
author = {{De Keukelaere}, F. and Bhola, Sumeer and Steiner, Michael and Chari, Suresh and Yoshihama, Sachiko},
doi = {10.1145/1367497.1367570},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/De Keukelaere et al. - 2008 - SMash secure component model for cross-domain mashups on unmodified browsers.pdf:pdf},
isbn = {9781605580852},
journal = {Proceeding of the 17th international conference on World Wide Web},
keywords = {0,browser,component model,mashup,web 2},
pages = {535--544},
title = {{SMash: secure component model for cross-domain mashups on unmodified browsers}},
url = {http://dl.acm.org/citation.cfm?id=1367570},
year = {2008}
}
@book{Virgilio2017,
author = {Virgilio, Roberto De and Eds, Riccardo Torlone and Conference, International and Hutchison, David},
booktitle = {17th International Conference, ICWE 2017 Rome, Italy, June 5–8, 2017 Proceedings},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Virgilio et al. - 2017 - Web Engineering.pdf:pdf},
isbn = {9783319601304},
title = {{Web Engineering}},
year = {2017}
}
@article{Namoun2016,
abstract = {Mobile devices are everywhere, and the scope of their use is growing from simple calling and texting through Internet browsing to more technical activities such as creating message processing filters and connecting different apps. However, building tools which provide effective support for such advanced technical use of mobile devices by non-programmers (mobile end user development or mEUD) requires thorough understanding of user needs and motivations, including factors which can impact user intentions regarding mEUD activities. We propose a model linking these mEUD factors with mobile users{\&}{\#}x2018; attitudes towards, and intent of doing mEUD, and discuss a number of implications for supporting mEUD. Our research process is user-centered, and we formulate a number of hypotheses by fusing results from an exploratory survey which gathers facts about mEUD motivations and activities, and from a focus group study, which delivers deeper understanding of particular mEUD practices and issues. We then test the hypothesized relationships through a follow-up enquiry mixing quantitative and qualitative techniques, leading to the creation of a preliminary mEUD model. Altogether we have involved 275 mobile users in our research. Our contribution links seven mEUD factors with mEUD intentions and attitudes, and highlights a number of implications for mEUD support.},
author = {Namoun, Abdallah and Daskalopoulou, Athanasia and Mehandjiev, Nikolay and Xun, Zhang},
doi = {10.1109/TSE.2016.2532873},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Namoun et al. - 2016 - Exploring mobile end user development Existing use and design factors.pdf:pdf},
isbn = {0098-5589 VO - PP},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Human factors in software design,mobile environments,models and principles,requirements/specifications},
number = {10},
pages = {960--976},
title = {{Exploring mobile end user development: Existing use and design factors}},
volume = {42},
year = {2016}
}
@misc{,
title = {{Bluetooth Low Energy (BLE) Introduction - Part 2 | EmbeTronicX}},
url = {https://embetronicx.com/tutorials/tech{\_}devices/bluetooth-low-energy-ble-introduction-part-2/},
urldate = {2018-06-22}
}
@misc{,
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - bluetooth-low-energy-part-1-introduction-ble @ www.mikroe.com.html:html},
title = {bluetooth-low-energy-part-1-introduction-ble @ www.mikroe.com},
url = {https://www.mikroe.com/blog/bluetooth-low-energy-part-1-introduction-ble}
}
@misc{,
title = {{Bluetooth Low Energy - Part 1: Introduction To BLE}},
url = {https://www.mikroe.com/blog/bluetooth-low-energy-part-1-introduction-ble},
urldate = {2018-06-22},
year = {2016}
}
@book{Pahlavan2013,
author = {Pahlavan, Kaveh and Krishnamurthy, Prashant},
publisher = {John Wiley {\&} Sons},
title = {{Principles of wireless access and localization}},
year = {2013}
}
@misc{,
title = {{Radius Networks/bluez}}
}
@misc{,
title = {{Bluetooth Protocols}},
year = {2009}
}
@misc{OpenWrt,
author = {OpenWrt},
title = {{OpenWrt Wiki}}
}
@article{West2014,
author = {West, Alex},
journal = {Dost{\c{e}}pne w Internecie: www. bluetooth. com/Pages/Smartphones. aspx},
title = {{Smartphone, the key for Bluetooth low energy technology}},
year = {2014}
}
@inproceedings{Zhang2011,
author = {Zhang, Jianbi},
booktitle = {Electrical and Control Engineering (ICECE), 2011 International Conference on},
pages = {2142--2145},
title = {{Design of embedded bluetooth information broadcast system}},
year = {2011}
}
@article{Decuir2014,
author = {Decuir, Joseph},
journal = {IEEE Consumer Electronics Magazine},
number = {1},
pages = {12--18},
publisher = {IEEE},
title = {{Introducing Bluetooth Smart: Part 1: A look at both classic and new technologies.}},
volume = {3},
year = {2014}
}
@unpublished{Karlsson2002,
author = {Karlsson, Jimi and Persson, Albin},
title = {{Device and Service Discovery in Bluetooth Networks}},
year = {2002}
}
@article{Bhagwat2001,
author = {Bhagwat, Pravin},
journal = {IEEE Internet Computing},
number = {3},
pages = {96--103},
publisher = {IEEE},
title = {{Bluetooth: technology for short-range wireless apps}},
volume = {5},
year = {2001}
}
@misc{Microchip,
author = {Microchip},
title = {{BLE Link Layer Roles and States}}
}
@inproceedings{Frank2014,
author = {Frank, Raphael and Bronzi, Walter and Castignani, German and Engel, Thomas},
booktitle = {Wireless On-demand Network Systems and Services (WONS), 2014 11th Annual Conference on},
pages = {104--107},
title = {{Bluetooth low energy: An alternative technology for VANET applications}},
year = {2014}
}
@inproceedings{Bradac2003,
author = {Bradac, Zdenek and Fiedler, Petr and Cach, Petr and Vrba, Radimir},
booktitle = {Electronics, Circuits and Systems, 2003. ICECS 2003. Proceedings of the 2003 10th IEEE International Conference on},
pages = {659--662},
title = {{Wireless communication in automation}},
volume = {2},
year = {2003}
}
@inproceedings{Balogh2017,
author = {Balogh, Andr{\'{a}}s and Imre, S{\'{a}}ndor},
booktitle = {Software, Telecommunications and Computer Networks (SoftCOM), 2017 25th International Conference on},
pages = {1--8},
title = {{Simulation and analysis of concurrent BLE link layer state machines running within the same physical device}},
year = {2017}
}
@unpublished{McKinsey2015,
author = {McKinsey},
title = {{The Internet of Things: Mapping the value beyond the hype}},
year = {2015}
}
@misc{NordicSemiconductor,
author = {{Nordic Semiconductor}},
title = {{nRF Sniffer}}
}
@misc{Microchipa,
author = {Microchip},
title = {{Microchip developer help}}
}
@book{Townsend2014,
author = {Townsend, Kevin and Cufi, Carles and Davidson, Robert and Others},
publisher = {" O'Reilly Media, Inc."},
title = {{Getting started with Bluetooth low energy: tools and techniques for low-power networking}},
year = {2014}
}
@misc{Microchipb,
author = {Microchip},
title = {{Bluetooth Low Energy Connection Process}}
}
@misc{Bluetooth,
author = {Bluetooth},
title = {{How the Bluetooth Community Revolutionized Data Transfer}}
}
@unpublished{Cisco2014,
author = {Cisco},
title = {{Radio Frequency Fundamentals}},
year = {2014}
}
@misc{Verizon,
author = {Verizon},
title = {{State of the Market: Internet of Things 2017}}
}
@inproceedings{Kciuk2014,
author = {Kciuk, Marek},
booktitle = {Research and Education in Mechatronics (REM), 2014 15th International Workshop on},
pages = {1--4},
title = {{OpenWRT operating system based controllers for mobile robot and building automation system students projects realization}},
year = {2014}
}
@article{Rfxphqwv2011,
author = {Rfxphqwv, Wudfwlrq Iurp W and Wh, R Q H and Phwkrg, W Plqlqj and Whfkqltxhv, Ydulrxv and Surfhvv, W R and Shv, Gliihuhqw W and Vrxufhv, R I},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rfxphqwv et al. - 2011 - relation extraction from text documents.pdf:pdf},
pages = {1565--1570},
title = {relation extraction from text documents},
year = {2011}
}
@book{Harte2009,
author = {Harte, Lawrence},
publisher = {Althos},
title = {{Introduction to bluetooth}},
year = {2009}
}
@misc{,
title = {{BlueZ}}
}
@article{Mohamed2008,
author = {Mohamed, Mohsen A M and {Abu El-Azm}, Atef El-Sayed and El-Fishawy, Nawal Ahmad and El-Tokhy, Mostafa Ali and {Abd El-Samie}, Fathi El-Sayed},
journal = {Progress In Electromagnetics Research},
pages = {101--110},
publisher = {EMW Publishing},
title = {{Optimization of Bluetooth frame format for efficient performance}},
volume = {1},
year = {2008}
}
@misc{Bluetootha,
author = {Bluetooth, S I G},
title = {{Host Controller Interface Functional Specification, version 2.1+ EDR}}
}
@inproceedings{Jameel2015,
author = {Jameel, Mohamed Imran and Dungen, Jeffrey},
booktitle = {Internet of Things (WF-IoT), 2015 IEEE 2nd World Forum on},
pages = {597--602},
title = {{Low-power wireless advertising software library for distributed M2M and contextual IoT}},
year = {2015}
}
@misc{Qualcomm,
author = {Qualcomm},
title = {{Introduction to Bluetooth Low Energy}}
}
@unpublished{Cisco2014a,
author = {Cisco},
title = {{Radio Operating Frequencies and Data Rates}},
year = {2014}
}
@article{Li2017,
author = {Li, Xiaomin and Li, Di and Wan, Jiafu and Vasilakos, Athanasios V and Lai, Chin-Feng and Wang, Shiyong},
journal = {Wireless networks},
number = {1},
pages = {23--41},
publisher = {Springer},
title = {{A review of industrial wireless networks in the context of industry 4.0}},
volume = {23},
year = {2017}
}
@unpublished{,
title = {{Specification of the Bluetooth System}},
year = {1999}
}
@misc{Freescalesemiconductor2015,
author = {Freescale semiconductor},
title = {{Wireless Coexistence in the 2.4 GHz Band}},
year = {2015}
}
@article{Aftab2017,
author = {bin Aftab, Muhammad Usama},
publisher = {Packt Publishing},
title = {{Building Bluetooth Low Energy Systems}},
year = {2017}
}
@inproceedings{Wang2013a,
author = {Wang, Haolin and Xi, Minjun and Liu, Jia and Chen, Canfeng},
booktitle = {Advanced Communication Technology (ICACT), 2013 15th International Conference on},
pages = {72--77},
title = {{Transmitting IPv6 packets over Bluetooth low energy based on BlueZ}},
year = {2013}
}
@article{Nobelius,
author = {Nobelius, Dennis},
title = {{Strategic Actions in Ericssons Management of'Bluetooth'}}
}
@article{Song-Joo,
author = {Song-Joo, Goh},
title = {{Bluetooth architecture, protocol and applications}}
}
@misc{Saltzstein,
author = {Saltzstein, Bill},
title = {{Bluetooth Low-Energy Technology Isn't Just Another Bluetooth Revision – It's a Whole New Technology}}
}
@misc{Bluetoothb,
author = {Bluetooth},
title = {{Bluetooth Low Energy - It starts with Advertising}}
}
@article{Bhagwat2001a,
author = {Bhagwat, Pravin},
title = {{Technology for Short-Range Wireless Apps}},
year = {2001}
}
@misc{,
title = {{Bluetooth Low Energy (BLE) Introduction - Part 2 | EmbeTronicX}},
url = {https://embetronicx.com/tutorials/tech{\_}devices/bluetooth-low-energy-ble-introduction-part-2/},
urldate = {2018-06-22},
year = {2017}
}
@article{Vijayarajan2016,
abstract = {In the internet era, search engines play a vital role in information retrieval from web pages. Search engines arrange the retrieved results using various ranking algorithms. Additionally, retrieval is based on statistical searching techniques or content-based information extraction methods. It is still difficult for the user to understand the abstract details of every web page unless the user opens it separately to view the web content. This key point provided the motivation to propose and display an ontology-based object-attribute-value (O-A-V) information extraction system as a web model that acts as a user dictionary to refine the search keywords in the query for subsequent attempts. This first model is evaluated using various natural language processing (NLP) queries given as English sentences. Additionally, image search engines, such as Google Images, use content-based image information extraction and retrieval of web pages against the user query. To minimize the semantic gap between the image retrieval results and the expected user results, the domain ontology is built using image descriptions. The second proposed model initially examines natural language user queries using an NLP parser algorithm that will identify the subject-predicate-object (S-P-O) for the query. S-P-O extraction is an extended idea from the ontology-based O-A-V web model. Using this S-P-O extraction and considering the complex nature of writing SPARQL protocol and RDF query language (SPARQL) from the user point of view, the SPARQL auto query generation module is proposed, and it will auto gener-ate the SPARQL query. Then, the query is deployed on the ontology, and images are retrieved based on the auto-generated SPARQL query. With the proposed methodol-ogy above, this paper seeks answers to following two questions. First, how to combine the use of domain ontology and semantics to improve information retrieval and user experience? Second, does this new unified framework improve the standard informa-tion retrieval systems? To answer these questions, a document retrieval system and an image retrieval system were built to test our proposed framework. The web document retrieval was tested against three key-words/bag-of-words models and a semantic ontology model. Image retrieval was tested on IAPR TC-12 benchmark dataset. The precision, recall and accuracy results were then compared against standard informa-tion retrieval systems using TREC{\_}EVAL. The results indicated improvements over the standard systems. A controlled experiment was performed by test subjects querying the retrieval system in the absence and presence of our proposed framework. The que-ries were measured using two metrics, time and click-count. Comparisons were made Open Access},
author = {Vijayarajan, V. and Dinakaran, M. and Tejaswin, Priyam and Lohani, Mayank},
doi = {10.1186/s13673-016-0074-1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Vijayarajan et al. - 2016 - A generic framework for ontology‑based information retrieval and image retrieval in web data.pdf:pdf},
issn = {21921962},
journal = {Human-centric Computing and Information Sciences},
keywords = {Image retrieval,Information retrieval,Natural language processing,Ontology,SPARQL query},
number = {1},
pages = {1--30},
publisher = {Springer Berlin Heidelberg},
title = {{A generic framework for ontology‑based information retrieval and image retrieval in web data}},
volume = {6},
year = {2016}
}
@article{Katasonov2010,
abstract = {In the environments where heterogeneous devices need to share information, utilize services of each other, and participate as components in various smart applications, it is common to rely on the advantages of the semantic data model and ontologies. Our work extends this approach so that also the process of software development for such environments is ontology-driven. The goals are to raise the level of abstraction of smart application development, to enable development by non-programmers, and to partially automate the development to make it easier and faster. In this paper, we describe the Smart Modeller that consists of 1) a design tool that enables the developer to graphically create a model of a smart space application and 2) a framework that provides core interfaces for extensions supporting both the model and ontology-driven development. These extensions enable: ontology-based creation of model elements, discovery and reuse of both the software components and partial models through a repository mech- anism, and generation of executable programming code for models.},
author = {Katasonov, Artem and Palviainen, Marko},
doi = {10.1109/PERCOMW.2010.5470523},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Katasonov, Palviainen - 2010 - Towards ontology-driven development of applications for smart environments.pdf:pdf},
isbn = {978-1-4244-6605-4},
journal = {2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)},
keywords = {driven software engineering,interoperability,ontology-,semantic technologies,smart environment},
pages = {696--701},
title = {{Towards ontology-driven development of applications for smart environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5470523},
year = {2010}
}
@article{Ahmad,
author = {Ahmad, Mohammad Nazir and Wahid, Mohd Taib},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ahmad, Wahid - Unknown - An Overview Towards Ontology-Driven Software Development.pdf:pdf},
title = {{An Overview: Towards Ontology-Driven Software Development}},
url = {https://pdfs.semanticscholar.org/0869/80c620ceccafaa7b7ad0665439e777419f3a.pdf}
}
@inproceedings{Knublauch2004,
abstract = {Recent efforts towards the Semantic Web vision have lead to a number of stan-dards such as OWL and Web Service languages. While these standards provide a technical infrastructure, software developers have little guidance on how to build real-world Semantic Web applications. Based on a realistic application scenario, we present some initial thoughts on a software architecture and a development methodology for Web services and agents for the Semantic Web. This architecture is driven by formal domain models (ontologies). The methodology applies best practices from agile development methodologies, including systematic tests, short feedback loops, and close involvement of domain experts. We illustrate how these techniques can be put into practice using the modern Semantic Web development tool Prot{\'{e}}g{\'{e}}, and indicate future possibilities.},
author = {Knublauch, Holger},
booktitle = {1st International workshop on the model-driven semantic web (MDSW2004)},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Knublauch - Unknown - Ontology-Driven Software Development in the Context of the Semantic Web An Example Scenario with Prot{\'{e}}g{\'{e}}OWL.pdf:pdf},
pages = {381--401},
title = {{Ontology-Driven Software Development in the Context of the Semantic Web: An Example Scenario with Prot{\'{e}}g{\'{e}}/OWL}},
url = {https://pdfs.semanticscholar.org/5845/67d85beb0d31442566a2fde70f9b53d02a71.pdf},
year = {2004}
}
@book{Presutti2014,
abstract = {In this paper we present a domain-independent framework that creates a sentiment analysis model by mixing Semantic Web technologies with natural language processing approaches (This work is supported by the project PRISMA SMART CITIES, funded by the Italian Ministry of Research and Education under the program PON.). Our system, called Sentilo, provides a core sentiment analysis engine which fully exploits semantics. It identifies the holder of an opinion, topics and sub-topics the opinion is referred to, and assesses the opinion trigger. Sentilo uses an OWL opinion ontology to represent all this information with an RDF graph where holders and topics are resolved on Linked Data. Anyone can plug its own opinion scoring algorithm to compute scores of opinion expressing words and come up with a combined scoring algorithm for each identified entities and the overall sentence.},
author = {Presutti, Valentina and Blomqvist, Eva and Troncy, Rapha{\"{e}}l and Sack, Harald and Papadakis, Ioannis and Tordai, Anna and Hutchison, David},
doi = {10.1007/978-3-319-11955-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Presutti et al. - 2014 - The Semantic Web ESWC 2014 Satellite Events.pdf:pdf},
isbn = {978-3-319-11954-0},
issn = {16113349},
title = {{The Semantic Web: ESWC 2014 Satellite Events}},
url = {http://link.springer.com/10.1007/978-3-319-11955-7},
volume = {8798},
year = {2014}
}
@article{Jain2015,
abstract = {Current World Wide Web also recognized as Web 2.0 is an immense library of interlinked documents that are transferred by computers and presented to people. Search engine is considered the most important tool to discover any information from WWW. Inspite of having lots of development and novel research in current search engines techniques, they are still syntactic in nature and display search results on the basis of keyword matching without understanding the meaning of query, resulting in the production of list of WebPages containing a large number of irrelevant documents as an output. Semantic Web (Web 3.0), the next version of World Wide Web is being developed with the aim to reduce the problem faced in Web 2.0 by representing data in structured form and to discover such data from Semantic Web, Semantic Search Engines (SSE) are being developed in many domains. This paper provides a survey on some of the prevalent SSEs focusing on their architecture; and presents a comparative study on the basis of technique they follow for crawling, reasoning, indexing, ranking etc.},
author = {Jain, Ranjna and Duhan, Neelam and Faridabad, Ymcaust and Sharma, India A K and Dean, {\&}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jain et al. - 2015 - Comparative Study on Semantic Search Engines.pdf:pdf},
journal = {International Journal of Computer Applications},
keywords = {Semantic Search Engines,Semantic Web,Web 20},
number = {14},
pages = {975--8887},
title = {{Comparative Study on Semantic Search Engines}},
url = {https://pdfs.semanticscholar.org/d53a/39ee4615e0fa76c942eb258c9bc41a8f23c3.pdf},
volume = {131},
year = {2015}
}
@inproceedings{Hao2017,
abstract = {— With the growing number of unstructured arti-cles written in natural-language, automated extraction of knowledge, such as associations between entities, is be-coming essential for many applications. In this paper, we develop an automated verb-based algorithm for multiple-relation extraction from unstructured data obtained on-line. Named Entity Recognition (NER) techniques were applied to extract biomedical entities and relations were recognized by algorithms with Natural Language Processing (NLP) techniques. Evaluation based on F-measure with a random sample of sentences from biomedical literature results an average precision of 90{\%} and recall of 82{\%}. We also compared the performance of the proposed algorithm with a single-relation extraction algorithm, indicating improve-ments of this work. In conclusion, the preliminary study in-dicates that this method for multiple-relation extraction from unstructured literature is effective. With different training dataset, the algorithm can be applied to different domains. The automated method can be applied to detect and predict hidden relationships among varying areas.},
author = {Hao, Qi and Keppens, Jeroen and Rodrigues, Odinaldo},
booktitle = {Information and Knowledge Engineering IKE'17},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hao, Keppens, Rodrigues - 2017 - A Verb-based Algorithm for Multiple-Relation Extraction from Single Sentences.pdf:pdf},
keywords = {Multiple-relation extraction,Named Entity Recognition (NER),Natural Language Pro-cessing (NLP),verb-based al-gorithm},
pages = {115 -- 121},
title = {{A Verb-based Algorithm for Multiple-Relation Extraction from Single Sentences}},
url = {https://csce.ucmss.com/cr/books/2017/LFS/CSREA2017/IKE3227.pdf},
year = {2017}
}
@article{Rozenfeld2008,
abstract = {Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities and relations. Unlike traditional Information Extraction methods, the Web extraction systems do not label every mention of the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision of the resulting list reasonably high. SRES is a self-supervised Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes. SRES automatically generates the training data needed for its pattern-learning component. The performance of SRES is further enhanced by classifying its output instances using the properties of the instances and the patterns. The features we use for classification and the trained classification model are independent from the target relation, which we demonstrate in a series of experiments. We also compare the performance of SRES to the performance of the state-of-the-art KnowItAll system, and to the performance of its pattern learning component, which learns simpler pattern language than SRES.},
author = {Rozenfeld, Benjamin and Feldman, Ronen},
doi = {10.1007/s10115-007-0110-6},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rozenfeld, Feldman - 2008 - Self-supervised relation extraction from the Web.pdf:pdf},
journal = {Knowl Inf Syst},
keywords = {Pattern learning {\textperiodcentered},Relationship extraction,Text mining {\textperiodcentered},Unsupervised learning {\textperiodcentered},Web extraction {\textperiodcentered}},
pages = {17--33},
title = {{Self-supervised relation extraction from the Web}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10115-007-0110-6.pdf},
volume = {17},
year = {2008}
}
@article{Lukasiewicz2011,
author = {Lukasiewicz, Thomas},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lukasiewicz - 2011 - Ontology-Based Semantic Search on the Web.pdf:pdf},
journal = {1st International Workshop on Semantic Search Over the Web - SSW},
title = {{Ontology-Based Semantic Search on the Web}},
url = {http://www.cs.ox.ac.uk/people/thomas.lukasiewicz/ssw11.pdf},
year = {2011}
}
@article{Kaushik2017,
abstract = {In the present era of Big Data the demand for developing efficient information processing techniques for different applications is expanding steadily. One such possible application is automatic creation of ontology. Such an ontology is often found to be helpful for answering queries for the underlying domain. The present work proposes a scheme for designing an ontology for agriculture domain. The proposed scheme works in two steps. In the first step it uses domain-dependent regular expressions and natural language processing techniques for automatic extraction of vocabulary pertaining to agriculture domain. In the second step semantic relationships between the extracted terms and phrases are identified. A rule-based reasoning algorithm RelExOnt has been proposed for the said task. Human evaluation of the term extraction output yields precision and recall of 75.7{\%} and 60{\%}, respectively. The relation extraction algorithm, RelExOnt performs well with an average precision of 86.89{\%}.},
author = {Kaushik, Neha and Chatterjee, Niladri},
doi = {10.1016/J.INPA.2017.11.003},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kaushik, Chatterjee - 2017 - Automatic relationship extraction from agricultural text for ontology construction.pdf:pdf},
issn = {2214-3173},
journal = {Information Processing in Agriculture},
month = {mar},
number = {1},
pages = {60--73},
publisher = {Elsevier},
title = {{Automatic relationship extraction from agricultural text for ontology construction}},
url = {https://www.sciencedirect.com/science/article/pii/S2214317317300227},
volume = {5},
year = {2017}
}
@article{Faaborg2006,
abstract = {Many users are familiar with the interesting but limited functionality of Data Detector interfaces like Microsoft's Smart Tags and Google's AutoLink. In this paper we significantly expand the breadth and functionality of this type of user interface through the use of large-scale knowledge bases of semantic information. The result is a Web browser that is able to generate personalized semantic hypertext, providing a goal-oriented browsing experience.We present (1) Creo, a Programming by Example system for the Web that allows users to create a general-purpose procedure with a single example, and (2) Miro, a Data Detector that matches the content of a page to high-level user goals.An evaluation with 34 subjects found that they were more efficient using our system, and that the subjects would use features like these if they were integrated into their Web browser.},
author = {Faaborg, Alexander and Lieberman, Henry},
doi = {doi: 10.1145/1124772.1124883},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Faaborg, Lieberman - 2006 - A goal-oriented web browser(2).pdf:pdf},
isbn = {1-59593-372-7},
journal = {CHI '06: Proceedings of the SIGCHI conference on Human Factors in computing systems},
keywords = {2470-081,adaptive-interface,agents,www},
pages = {751--760},
title = {{A goal-oriented web browser}},
url = {http://dx.doi.org/10.1145/1124772.1124883{\%}0Aciteulike-article-id:852698},
year = {2006}
}
@article{Tirado2016,
abstract = {A constantly growing amount of information is available through the web. Unfortunately, extracting useful content from this massive amount of data still remains an open issue. The lack of standard data models and structures forces developers to create adhoc solutions from the scratch. The figure of the expert is still needed in many situations where developers do not have the correct background knowledge. This forces developers to spend time acquiring the needed background from the expert. In other directions, there are promising solutions employing machine learning techniques. However, increasing accuracy requires an increase in system complexity that cannot be endured in many projects. In this work, we approach the web knowledge extraction problem using an expertcentric methodology. This methodology defines a set of configurable, extendible and independent components that permit the reutilisation of large pieces of code among projects. Our methodology differs from similar solutions in its expert-driven design. This design, makes it possible for subject-matter expert to drive the knowledge extraction for a given set of documents. Additionally, we propose the utilization of machine assisted solutions that guide the expert during this process. To demonstrate the capabilities of our methodology, we present a real use case scenario in which public procurement data is extracted from the web-based repositories of several public institutions across Europe. We provide insightful details about the challenges we had to deal with in this use case and additional discussions about how to apply our methodology.},
archivePrefix = {arXiv},
arxivId = {1603.07534},
author = {Tirado, Juan M. and Serban, Ovidiu and Guo, Qiang and Yoneki, Eiko},
eprint = {1603.07534},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tirado et al. - 2016 - Web Data Knowledge Extraction(2).pdf:pdf},
number = {881},
title = {{Web Data Knowledge Extraction}},
url = {http://arxiv.org/abs/1603.07534},
year = {2016}
}
@article{Cunningham2013,
author = {Cunningham, Hamish and Tablan, Valentin and Roberts, Angus and Bontcheva, Kalina},
doi = {10.1371/journal.pcbi.1002854},
editor = {Prlic, Andreas},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {feb},
number = {2},
pages = {e1002854},
title = {{Getting More Out of Biomedical Documents with GATE's Full Lifecycle Open Source Text Analytics}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1002854},
volume = {9},
year = {2013}
}
@inproceedings{Sinha2012,
abstract = {In this 21 st Century, globalization has become a common knack for all. In most used ways, globalization is made by creating websites. As a result, information volume of internet has increased exponentially [1]. Therefore, it is increasingly becoming difficult to retrieve specific information from the internet. Search engine plays a big role to retrieve information from internet [2][3]. Effective search result for a user given search string always increase search engine efficiency. Most of the search engines are available in the market that produced search results in two parts, i.e. one is highlighted part and the other one is main part. Most of the cases in the main part of their search results produced by appending title tag, URL and meta-tag information or first few words from the Web-page content. While searching of any information for few commonly used products like book, mobile handset, medicine, jewelry, etc., Web-searcher wants to know some basic attributes based on the product. Now, if we display these products basic attributes information in the search result instead of displaying meta-tag or first few words from the Web-page content, then no need to visit all the URLs displayed in the search result. As a result all the Web-searcher saves their time and Web-page download cost. In this paper, we have proposed a design and development mechanism of Ontology based domain specific Web-search engine for commonly used products using Resource Description Framework (RDF).},
author = {Sinha, Sukanta and Dattagupta, Rana and Mukhopadhyay, Debajyoti},
booktitle = {Proceedings of the CUBE International Information Technology Conference},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sinha, Dattagupta, Mukhopadhyay - 2012 - Designing an Ontology based Domain Specific Web Search Engine for Commonly used Products using.pdf:pdf},
keywords = {Algorithms,Information filtering,Ontology,Ontology Based Search,Retrieval models General Terms Design},
pages = {612--617},
title = {{Designing an Ontology based Domain Specific Web Search Engine for Commonly used Products using RDF}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=D6A247CFD73906B6610A4BC80829EF6A?doi=10.1.1.475.5681{\&}rep=rep1{\&}type=pdf},
year = {2012}
}
@article{Singh2012,
abstract = {The document retrieval is one of the fast growing and complex research area in the field of information retrieval. An effective Information retrieval can be obtained only under strong document retrieval algorithm. As compared to the information retrieval, document retrieval is also a tedious process. The accurate retrieval of a document needs highly precise and mathematically vibrant methods. A number of researches have been targeted for the document retrieval, which yielded expected result within their boundaries. In this paper, we proposed an ontology-based augmented method for document retrieval. The ontology defined in our proposed approach gives extra freedom to choose between the documents and thus give an accurate retrieval of the documents. The mutual association (MA) value specifies the interrelated documents in the problem space. The array index values, which we provide, give accurate distinction between each document. The results and analysis of our proposed method showed expected results and a comparative analysis was subjected for analyzing the proposed method with an existing algorithm. The F-measure comparison showed the performance improvement of the proposed method with respect to the existing method.},
author = {Singh, R P and {Bimla Devi}, Director},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Singh, Bimla Devi - 2012 - OntDR An Ontology-based Augmented Method for Document Retrieval.pdf:pdf},
journal = {International Journal of Computer Applications},
keywords = {Document retrieval,Information retrieval,Ontology,array index,f-measure,mutual association,precision,recall},
number = {17},
pages = {975--8887},
title = {{OntDR: An Ontology-based Augmented Method for Document Retrieval}},
url = {https://pdfs.semanticscholar.org/6ae4/bb82f38c5046ea07b4134e467a670f393315.pdf},
volume = {53},
year = {2012}
}
@article{Dong2014,
abstract = {The task of data fusion is to identify the true values of data items (e.g., the true date of birth for Tom Cruise) among multiple observed values drawn from different sources (e.g., Web sites) of varying (and unknown) reliabil-ity. A recent survey [20] has provided a detailed comparison of various fusion methods on Deep Web data. In this paper, we study the applicability and limitations of different fusion techniques on a more challenging prob-lem: knowledge fusion. Knowledge fusion identifies true subject-predicate-object triples extracted by multiple information extractors from multiple information sources. These extractors perform the tasks of entity linkage and schema alignment, thus introducing an additional source of noise that is quite different from that traditionally considered in the data fusion litera-ture, which only focuses on factual errors in the original sources. We adapt state-of-the-art data fusion techniques and apply them to a knowledge base with 1.6B unique knowledge triples extracted by 12 extractors from over 1B Web pages, which is three orders of magnitude larger than the data sets used in previous data fusion papers. We show great promise of the data fusion approaches in solving the knowledge fusion problem, and suggest interest-ing research directions through a detailed error analysis of the methods.},
author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Murphy, Kevin and Sun, Shaohua and Zhang, Wei},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - 2014 - From Data Fusion to Knowledge Fusion.pdf:pdf},
journal = {Proceedings of the VLDB Endowment},
pages = {881--92},
title = {{From Data Fusion to Knowledge Fusion}},
url = {http://www.vldb.org/pvldb/vol7/p881-dong.pdf},
volume = {7},
year = {2014}
}
@inproceedings{Etzioni2004,
abstract = {Manually querying search engines in order to accumulate a large body of factual information is a tedious, error-prone process of piecemeal search. Search engines retrieve and rank potentially rel-evant documents for human perusal, but do not extract facts, assess confidence, or fuse information from multiple documents. This pa-per introduces KNOWITALL, a system that aims to automate the tedious process of extracting large collections of facts from the web in an autonomous, domain-independent, and scalable manner. The paper describes preliminary experiments in which an in-stance of KNOWITALL, running for four days on a single machine, was able to automatically extract 54,753 facts. KNOWITALL asso-ciates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KNOWITALL's architecture and re-ports on lessons learned for the design of large-scale information extraction systems.},
author = {Etzioni, Oren and Cafarella, Michael and Downey, Doug and Kok, Stanley and Popescu, Ana-Maria and Shaked, Tal and Soderland, Stephen and Weld, Daniel S and Yates, Alexander},
booktitle = {In Proceedings of the 13th international conference on World Wide Web},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Etzioni et al. - 2004 - Web-Scale Information Extraction in KnowItAll (Preliminary Results).pdf:pdf},
keywords = {H33 [Information Systems],I26 [Artificial Intelligence],Learning—knowledge ac-quisition,Mutual Information,Natural Language Processing—text analysis,Search},
pages = {100--110},
title = {{Web-Scale Information Extraction in KnowItAll (Preliminary Results)}},
url = {http://delivery.acm.org/10.1145/990000/988687/p100-etzioni.pdf?ip=134.109.184.147{\&}id=988687{\&}acc=ACTIVE SERVICE{\&}key=2BA2C432AB83DA15.2C9D92A4DDBE0279.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1520939239{\_}8975c0c0330c39ae9db1fad3c702b433},
year = {2004}
}
@article{Carlson2010,
abstract = {We consider here the problem of building a never-ending lan-guage learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, informa-tion from the web to populate a growing structured knowl-edge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a par-tial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74{\%} after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.},
author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R and Mitchell, Tom M},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Carlson et al. - 2010 - Toward an Architecture for Never-Ending Language Learning.pdf:pdf},
journal = {AAAI},
pages = {3},
title = {{Toward an Architecture for Never-Ending Language Learning}},
url = {http://www.cs.cmu.edu/{~}acarlson/papers/carlson-aaai10.pdf},
volume = {5},
year = {2010}
}
@inproceedings{Nakashole2011,
abstract = {Harvesting relational facts from Web sources has received great at-tention for automatically constructing large knowledge bases. State-of-the-art approaches combine pattern-based gathering of fact can-didates with constraint-based reasoning. However, they still face major challenges regarding the trade-offs between precision, recall, and scalability. Techniques that scale well are susceptible to noisy patterns that degrade precision, while techniques that employ deep reasoning for high precision cannot cope with Web-scale data. This paper presents a scalable system, called PROSPERA, for high-quality knowledge harvesting. We propose a new notion of n-gram-itemsets for richer patterns, and use MaxSat-based constraint reasoning on both the quality of patterns and the validity of fact candidates. We compute pattern-occurrence statistics for two bene-fits: they serve to prune the hypotheses space and to derive informa-tive weights of clauses for the reasoner. The paper shows how to in-corporate these building blocks into a scalable architecture that can parallelize all phases on a Hadoop-based distributed platform. Our experiments with the ClueWeb09 corpus include comparisons to the recent ReadTheWeb experiment. We substantially outperform these prior results in terms of recall, with the same precision, while having low run-times.},
author = {Nakashole, Ndapandula and Theobald, Martin and Weikum, Gerhard},
booktitle = {Proceedings of the fourth ACM international conference on Web search and data mining},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Nakashole, Theobald, Weikum - 2011 - Scalable Knowledge Harvesting with High Precision and High Recall.pdf:pdf},
keywords = {Information Extraction,Scalability},
pages = {227--236},
title = {{Scalable Knowledge Harvesting with High Precision and High Recall}},
url = {http://www.nakashole.com/papers/2011-wsdm-prospera.pdf},
year = {2011}
}
@inproceedings{Li2011,
abstract = {Emerging text-intensive enterprise applica-tions such as social analytics and semantic search pose new challenges of scalability and usability to Information Extraction (IE) sys-tems. This paper presents SystemT, a declar-ative IE system that addresses these challenges and has been deployed in a wide range of en-terprise applications. SystemT facilitates the development of high quality complex annota-tors by providing a highly expressive language and an advanced development environment. It also includes a cost-based optimizer and a high-performance, flexible runtime with mini-mum memory footprint. We present SystemT as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable IE systems.},
author = {Li, Yunyao and Reiss, Frederick R and Chiticariu, Laura},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Systems Demonstrations.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Li, Reiss, Chiticariu - 2011 - SystemT A Declarative Information Extraction System.pdf:pdf},
pages = {109--114},
title = {{SystemT: A Declarative Information Extraction System}},
url = {https://pdfs.semanticscholar.org/6b45/0f61c46c1bf4309a430e3902ba28ef28738d.pdf},
year = {2011}
}
@inproceedings{Shen2007,
abstract = {In this paper we argue that developing information extrac-tion (IE) programs using Datalog with embedded procedu-ral extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, e.g., Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches: programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide ini-tial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE pro-grams and that we can effectively optimize IE programs written in this proposed framework.},
author = {Shen, Warren and Doan, Anhai and Naughton, Jeffrey F and Ramakrishnan, Raghu},
booktitle = {Proceedings of the 33rd international conference on Very large data bases},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2007 - Declarative Information Extraction Using Datalog with Embedded Extraction Predicates.pdf:pdf},
pages = {1033--1044},
title = {{Declarative Information Extraction Using Datalog with Embedded Extraction Predicates}},
url = {http://pages.cs.wisc.edu/{~}anhai/papers/xlog-vldb07.pdf},
year = {2007}
}
@article{Tirado2016a,
abstract = {A constantly growing amount of information is available through the web. Unfortunately, extracting useful content from this massive amount of data still remains an open issue. The lack of standard data models and structures forces developers to create adhoc solutions from the scratch. The figure of the expert is still needed in many situations where developers do not have the correct background knowledge. This forces developers to spend time acquiring the needed background from the expert. In other directions, there are promising solutions employing machine learning techniques. However, increasing accuracy requires an increase in system complexity that cannot be endured in many projects. In this work, we approach the web knowledge extraction problem using an expertcentric methodology. This methodology defines a set of configurable, extendible and independent components that permit the reutilisation of large pieces of code among projects. Our methodology differs from similar solutions in its expert-driven design. This design, makes it possible for subject-matter expert to drive the knowledge extraction for a given set of documents. Additionally, we propose the utilization of machine assisted solutions that guide the expert during this process. To demonstrate the capabilities of our methodology, we present a real use case scenario in which public procurement data is extracted from the web-based repositories of several public institutions across Europe. We provide insightful details about the challenges we had to deal with in this use case and additional discussions about how to apply our methodology.},
archivePrefix = {arXiv},
arxivId = {1603.07534},
author = {Tirado, Juan M. and Serban, Ovidiu and Guo, Qiang and Yoneki, Eiko},
eprint = {1603.07534},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tirado et al. - 2016 - Web Data Knowledge Extraction.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tirado et al. - 2016 - Web Data Knowledge Extraction(2).pdf:pdf},
number = {881},
title = {{Web Data Knowledge Extraction}},
url = {http://arxiv.org/abs/1603.07534},
year = {2016}
}
@article{Dou2015,
abstract = {Semantic Data Mining refers to the data mining tasks that systematically incorporate domain knowledge, especially for- mal semantics, into the process. In the past, many research efforts have attested the benefits of incorporating domain knowledge in data mining. At the same time, the proliferation of knowledge engineering has enriched the family of domain knowledge, espe- cially formal semantics and SemanticWeb ontologies. Ontology is an explicit specification of conceptualization and a formal way to define the semantics of knowledge and data. The formal structure of ontology makes it a nature way to encode domain knowledge for the data mining use. In this survey paper, we introduce general concepts of semantic data mining. We investigate why ontology has the potential to help semantic data mining and how formal semantics in ontologies can be incorporated into the data mining process. We provide detail discussions for the advances and state of art of ontology-based approaches and an introduction of approaches that are based on other form of knowledge representations.},
author = {Dou, Dejing and Wang, Hao and Liu, Haishan},
doi = {10.1109/ICOSC.2015.7050814},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Dou, Wang, Liu - 2015 - Semantic data mining A survey of ontology-based approaches.pdf:pdf},
isbn = {9781479979356},
issn = {2325-6516},
journal = {Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing, IEEE ICSC 2015},
pages = {244--251},
title = {{Semantic data mining: A survey of ontology-based approaches}},
year = {2015}
}
@article{Ferrara2012,
abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of applications. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the literature in the field of Web Data Extraction. We provided a simple classification framework in which existing Web Data Extraction applications are grouped into two main classes, namely applications at the Enterprise level and at the Social Web level. At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. At the Social Web level, Web Data Extraction techniques allow to gather a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities to analyze human behavior at a very large scale. We discuss also the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
archivePrefix = {arXiv},
arxivId = {1207.0246},
author = {Ferrara, Emilio and {De Meo}, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
doi = {10.1016/j.knosys.2014.07.007},
eprint = {1207.0246},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ferrara et al. - 2012 - Web Data Extraction, Applications and Techniques A Survey.pdf:pdf},
isbn = {978-1-4244-5678-9},
issn = {09507051},
journal = {Knowledge-Based Systems journal},
keywords = {web information extraction},
pages = {301--323},
pmid = {18244404},
title = {{Web Data Extraction, Applications and Techniques: A Survey}},
url = {http://arxiv.org/abs/1207.0246{\%}0Ahttp://dx.doi.org/10.1016/j.knosys.2014.07.007},
volume = {70},
year = {2012}
}
@article{Dong2017,
abstract = {Domain terms extraction based on Chinese Web document is an important step in the field of Chinese text information into machine recognition, and it is also the technical basis of intelligence information processing in domain ontology construction, text knowledge mining, and etc. The traditional methods of terms extracting are: Based on the dictionary, based on the rules and the statistical method. But each of methods contains some limitations, such as in single word processing, synonyms merging and so on. In order to solve these problems, the intelligence extraction method based on hierarchical combination strategy is proposed. It is divided into three layers: the first layer is the document preprocessing layer, the second layer is the words preparation layer, and the third layer is the term extraction layer. The effectiveness of this method is verified by experimental data in the field of weapon and equipment. Comparing with the traditional method, it is proved that this method has good accuracy and recall rate for the domain terms extraction based on the Chinese Web documents.},
author = {Dong, Yangyi and Li, Weihua and Yu, Hui},
doi = {10.1109/ICITBS.2016.57},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Dong, Li, Yu - 2017 - Intelligence extraction method of domain terms for Chinese web documents based on hierarchical combination strateg.pdf:pdf},
isbn = {9781509060610},
journal = {Proceedings - 2016 International Conference on Intelligent Transportation, Big Data and Smart City, ICITBS 2016},
keywords = {Intelligence information processing Introduction,Layering Combination Strategy,Term extraction,Web text mining},
pages = {225--228},
title = {{Intelligence extraction method of domain terms for Chinese web documents based on hierarchical combination strategy}},
year = {2017}
}
@article{Sangers2013,
author = {Sangers, Jordy and Frasincar, Flavius and Hogenboom, Frederik and Chepegin, Vadim},
doi = {10.1016/j.eswa.2013.02.011},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sangers et al. - 2013 - Expert Systems with Applications Semantic Web service discovery using natural language processing techniques.pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {natural language processing,semantic web services},
number = {11},
pages = {4660--4671},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications Semantic Web service discovery using natural language processing techniques}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.02.011},
volume = {40},
year = {2013}
}
@article{Sangers2013a,
abstract = {This paper proposes a semantic Web service discovery framework for finding semantic Web services by making use of natural language processing techniques. The framework allows searching through a set of semantic Web services in order to find a match with a user query consisting of keywords. By specifying the search goal using keywords, end-users do not need to have knowledge about semantic languages, which makes it easy to express the desired semantic Web services. For matching keywords with semantic Web service descriptions given in WSMO, techniques like part-of-speech tagging, lemmatization, and word sense disambiguation are used. After determining the senses of relevant words gathered from Web service descriptions and the user query, a matching process takes place. The performance evaluation shows that the three proposed matching algorithms are able to effectively perform matching and approximate matching.},
author = {Sangers, Jordy and Frasincar, Flavius and Hogenboom, Frederik and Chepegin, Vadim},
doi = {10.1016/J.ESWA.2013.02.011},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sangers et al. - 2013 - Semantic Web service discovery using natural language processing techniques.pdf:pdf},
issn = {0957-4174},
journal = {Expert Systems with Applications},
month = {sep},
number = {11},
pages = {4660--4671},
publisher = {Pergamon},
title = {{Semantic Web service discovery using natural language processing techniques}},
url = {https://vpngate4.hrz.tu-chemnitz.de/+CSCO+0075676763663A2F2F6A6A6A2E667076726170727176657270672E70627A++/science/article/pii/S0957417413001279},
volume = {40},
year = {2013}
}
@article{Radeck2014,
author = {Radeck, Carsten and Blichmann, Gregor and Mro{\ss}, Oliver and Mei{\ss}ner, Klaus},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Radeck et al. - 2014 - Semantic Mediation Techniques for Composite Web Applications.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Radeck et al. - 2014 - Semantic Mediation Techniques for Composite Web Applications(2).pdf:pdf},
issn = {16113349},
keywords = {data mediation,end user development,mashup,semantics},
pages = {450--459},
title = {{Semantic Mediation Techniques for Composite Web Applications}},
year = {2014}
}
@article{Liu2002,
abstract = {A novice search engine user may find searching the web for information difficult and frustrating because she may naturally$\backslash$nexpress search goals rather than the topic keywords search engines need. In this paper, we present GOOSE (goal-oriented searchengine), an adaptive search engine interface that uses natural language processing to parse a user's search goal, and uses“common sense” reasoning to translate this goal into an effective query. For a source of common sense knowledge, we use OpenMind, a knowledge base of approximately 400,000 simple facts such as “If a pet is sick, take it to the veterinarian” garneredfrom a Web-wide network of contributors. While we cannot be assured of the robustness of the common sense inference, in asubstantial number of cases, GOOSE is more likely to satisfy the user's original search goals than simple keywords or conventionalquery expansion.},
author = {Liu, Hugo and Lieberman, Henry and Selker, Ted},
doi = {10.1007/3-540-47952-X_27},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Lieberman, Selker - 2002 - GOOSE A goal-oriented search engine with commonsense.pdf:pdf},
isbn = {3540437371},
issn = {03029743},
journal = {Adaptive Hypermedia and Adaptive Web-Based Systems},
pages = {253--263},
title = {{GOOSE: A goal-oriented search engine with commonsense}},
url = {http://www.springerlink.com/index/3UT6WFX36D7ACP90.pdf},
year = {2002}
}
@article{Hogan2011,
abstract = {In this paper, we discuss the architecture and implementation of the Semantic Web Search Engine (SWSE). Following traditional search engine architecture, SWSE consists of crawling, data enhancing, indexing and a user interface for search, browsing and retrieval of information; unlike traditional search engines, SWSE operates over RDF Web data - loosely also known as Linked Data - which implies unique challenges for the system design, architecture, algorithms, implementation and user interface. In particular, many challenges exist in adopting Semantic Web technologies for Web data: the unique challenges of the Web - in terms of scale, unreliability, inconsistency and noise - are largely overlooked by the current Semantic Web standards. Herein, we describe the current SWSE system, initially detailing the architecture and later elaborating upon the function, design, implementation and performance of each individual component. In so doing, we also give an insight into how current Semantic Web standards can be tailored, in a best-effort manner, for use on Web data. Throughout, we offer evaluation and complementary argumentation to support our design choices, and also offer discussion on future directions and open research questions. Later, we also provide candid discussion relating to the difficulties currently faced in bringing such a search engine into the mainstream, and lessons learnt from roughly six years working on the Semantic Web Search Engine project. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Hogan, Aidan and Harth, Andreas and Umbrich, J{\"{u}}rgen and Kinsella, Sheila and Polleres, Axel and Decker, Stefan},
doi = {10.1016/j.websem.2011.06.004},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hogan et al. - 2011 - Searching and browsing Linked Data with SWSE The Semantic Web Search Engine.pdf:pdf},
isbn = {9783642250088},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Linked Data,RDF,Semantic Web,Semantic search,Web search},
number = {4},
pages = {365--401},
publisher = {Elsevier B.V.},
title = {{Searching and browsing Linked Data with SWSE: The Semantic Web Search Engine}},
url = {http://dx.doi.org/10.1016/j.websem.2011.06.004},
volume = {9},
year = {2011}
}
@article{Diamantini2016,
abstract = {One of the most critical issues in Ambient Assisted Living (AAL) is the design of systems that can evolve to meet the requirements of individuals as their needs and health conditions change. Although much work has been done on home and building automation systems for AAL, often referred to as assistive domotics, there is in fact still a substantial lack of solutions capable to support system designers in the early stage of development of such assistive systems. To this aim, the work contributes to the research on design of assistive domotic systems by presenting an ontology-driven methodology aimed to guide the development process. The novel contributions of the paper include the goal-oriented approach of the methodology, which involves the elicitation and analysis of AAL requirements and their formal representation in an ontology, where high-level goals are described in terms of subgoals and tasks, that are then linked to corresponding measures and devices. Moreover, logic-based reasoning enables more advanced functionalities useful at design time. We present a validation of the methodology showing typical use cases both related to the development from scratch of a domotic system with assistive capabilities starting from a set of high-level user requirements and the redesign of existing implementations according to changed requirements.},
author = {Diamantini, Claudia and Freddi, Alessandro and Longhi, Sauro and Potena, Domenico and Storti, Emanuele},
doi = {10.1016/j.eswa.2016.07.032},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Diamantini et al. - 2016 - A goal-oriented, ontology-based methodology to support the design of AAL environments.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Ambient assisted living,Goal-oriented design methodology,Ontology,Reasoning},
pages = {117--131},
title = {{A goal-oriented, ontology-based methodology to support the design of AAL environments}},
volume = {64},
year = {2016}
}
@article{Rathore2014,
author = {Rathore, Abhishek Singh and Roy, Devshri},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rathore, Roy - 2014 - Ontology based Web Page Topic Identification.pdf:pdf},
journal = {International Journal of Computer Applications},
keywords = {1,although keywords present in,but it is not,documents in meta tag,extracted from the,extraction from web documents,html,keywords are,meta,necessary that all web,see fig,tags and headers of,the,title,web page},
number = {6},
pages = {35--40},
title = {{Ontology based Web Page Topic Identification}},
volume = {85},
year = {2014}
}
@article{Weal2007,
abstract = {This paper investigates the role of ontologies as a central part of an architecture to repurpose existing material from the web. A prototype system called ArtEquAKT is presented, which combines information extraction, knowledge management and consolidation techniques and adaptive document generation. All of these components are co-ordinated using one central ontology, providing a common vocabulary for describing the information fragments as they are processed. Each of the components of the architecture is described in detail and an evaluation of the system discussed. Conclusions are drawn as to the effectiveness of such an approach and further challenges are outlined. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Weal, Mark J. and Alani, Harith and Kim, Sanghee and Lewis, Paul H. and Millard, David E. and Sinclair, P. A S and {De Roure}, David C. and Shadbolt, Nigel R.},
doi = {10.1016/j.ijhcs.2007.02.001},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Weal et al. - 2007 - Ontologies as facilitators for repurposing web documents.pdf:pdf},
isbn = {1071-5819},
issn = {10715819},
journal = {International Journal of Human Computer Studies},
keywords = {Knowledge extraction,Narrative generation,Ontology population},
number = {6},
pages = {537--562},
title = {{Ontologies as facilitators for repurposing web documents}},
volume = {65},
year = {2007}
}
@article{Alani2003,
abstract = {A large amount of digital information available is written as text documents in the form of web pages, reports, papers, emails, etc. Extracting the knowledge of interest from such documents from multiple sources in a timely fashion is therefore crucial. This paper provides an update on the Artequakt system which uses natural language tools to automatically extract knowledge about artists from multiple documents based on a predefined ontology. The ontology represents the type and form of knowledge to extract. This knowledge is then used to generate tailored biographies. The information extraction process of Artequakt is detailed and evaluated in this paper.},
author = {Alani, Harith and Kim, Sanghee and Millard, David E and Weal, Mark J and Lewis, Paul H and Hall, Wendy and Shaboldt, Nigel and Shadbolt, N},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Alani et al. - 2003 - Automatic Extraction of Knowledge from Web Documents.pdf:pdf},
journal = {Web and Web Services},
pages = {1--11},
title = {{Automatic Extraction of Knowledge from Web Documents}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.6445{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@article{Alani2003a,
abstract = {Automatically extracts knowledge about artists from the Web, populates a knowledge base, and uses it to generate personalized biographies.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0402205},
author = {Alani, Harith and Kim, Sanghee and Millard, David E and Weal, Mark J and Hall, Wendy and Lewis, Paul H and Shadbolt, Nigel R},
doi = {10.1109/MIS.2003.1179189},
eprint = {0402205},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Alani et al. - 2003 - Automatic ontology- extraction from web documents.pdf:pdf},
isbn = {1541-1672},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
pages = {14--21},
primaryClass = {quant-ph},
title = {{Automatic ontology- extraction from web documents}},
year = {2003}
}
@article{Al-Zaidy2017,
abstract = {continuing growth of published scholarly content on the web ensures the availability of the most recent scientiic cndings to researchers. Scholarly documents, such as research articles, are easily accessed by using academic search engines that are built on large repositories of scholarly documents. Scientiic informa-tion extraction from documents into a structured knowledge graph representation facilitates automated machine understanding of a document's content. Traditional information extraction approaches, that either require training samples or a preexisting knowledge base to assist in the extraction, can be challenging when applied to large repositories of digital documents. Labeled training examples for such large scale are diicult to obtain for such datasets. Also, most available knowledge bases are built from web data and do not have suucient coverage to include concepts found in scientiic articles. In this paper we aim to construct a knowledge graph from schol-arly documents while addressing both these issues. We propose a fully automatic, unsupervised system for scientiic information extraction that does not build on an existing knowledge base and avoids manually-tagged training data. We describe and evaluate a constructed taxonomy that contains over 15k entities resulting from applying our approach to 10k documents.},
author = {Al-Zaidy, Rabah A and Giles, C Lee},
doi = {10.1145/3103010.3121043},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Al-Zaidy, Giles - 2017 - Automatic Knowledge Base Construction from Scholarly Documents.pdf:pdf},
isbn = {9781450346894},
keywords = {Document structure,•Information systems  Information extraction},
pages = {149--152},
title = {{Automatic Knowledge Base Construction from Scholarly Documents}},
url = {http://delivery.acm.org/10.1145/3130000/3121043/p149-al-zaidy.pdf?ip=169.231.134.31{\&}id=3121043{\&}acc=OPEN{\&}key=CA367851C7E3CE77.022A0CC51A76093F.4D4702B0C3E38B35.6D218144511F3437{\&}CFID=984410947{\&}CFTOKEN=47254604{\&}{\_}{\_}acm{\_}{\_}=1505438289{\_}007bf8c2da1b50195414b534fd01},
year = {2017}
}
@article{,
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Web documents types 1.pdf:pdf},
pages = {1--17},
title = {{Web documents types 1}}
}
@article{Shen2014,
abstract = {This paper argues for placing ontologies at the centre of the software development life cycle for distributed component-based systems and, in particular, for service-oriented systems. It presents an ontology-based development process which relies on three levels of abstraction using ontologies: architecture layer, application layer and domain layer. The paper discusses the key roles of ontologies with respect to the various abstraction layers and their corresponding impact on the concomitant workproducts. In addition, a peer-to-peer-based service selecting and composing tool is suggested as a way of supporting the process. The paper presents the architecture of the proposed tool and illustrates the whole process in the development of a mobile banking application based on dynamic Web services.{\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Shen, Jun and Beydoun, Ghassan and Low, Graham and Wang, Lijuan},
doi = {10.1016/j.future.2013.08.005},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2014 - Aligning ontology-based development with service oriented systems.pdf:pdf},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Agents,Multi-agent systems,Ontologies,Peer-to-peer systems,Service oriented systems,Software development life cycle},
number = {1},
pages = {263--273},
publisher = {Elsevier B.V.},
title = {{Aligning ontology-based development with service oriented systems}},
url = {http://dx.doi.org/10.1016/j.future.2013.08.005},
volume = {32},
year = {2014}
}
@article{Khamis2014,
author = {Khamis, Ninus},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Khamis - 2014 - OwlExporter.pdf:pdf},
title = {{OwlExporter}},
year = {2014}
}
@article{Witte2010,
abstract = {Ontology population from text is becoming increasingly important for NLP applications. Ontologies in OWL format provide for a standardized means of modelling, querying, and reasoning over large knowledge bases. Populated from natural language texts, they offer significant advantages over traditional export formats, such as plain XML. The development of text analysis systems has been greatly facilitated by modern NLP frameworks, such as the General Architecture for Text Engineering (GATE). However, ontology population is not currently supported by a standard component. We developed a GATE resource called the OwlExporter that allows to easily map existing NLP analysis pipelines to OWL ontologies, thereby allowing language engineers to create ontology population systems without requiring extensive knowledge of ontology APIs. A particular feature of our approach is the concurrent population and linking of a domain- and NLP-ontology, including NLP-specific features such as safe reasoning over coreference chains.},
author = {Witte, R and Khamis, Ninus and Rilling, Juergen},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Witte, Khamis, Rilling - 2010 - Flexible Ontology Population from Text The OwlExporter.pdf:pdf},
isbn = {2-9517408-6-7},
journal = {International Conference on Language Resources and Evaluation (LREC)},
pages = {3845--3850},
title = {{Flexible Ontology Population from Text: The OwlExporter}},
url = {http://lrec.elra.info/proceedings/lrec2010/pdf/932{\_}Paper.pdf},
year = {2010}
}
@article{Shah,
abstract = {We propose a domain-agnostic framework for building and evolving a domain-specific taxonomy, given an initial set of well-organized data points. The idea is to automatically build and evolve the taxonomy with high precision and recall, but with minimal assistance from a domain expert. The approach used is to evolve two graphs simultane-ously: one which is built using minimized involve-ment from the domain expert, and the other which is obtained by an automatic and controlled subset-ting from a suitable Internet knowledge database (WordNet). While the former if high on precision, the latter provides better recall. Further, we de-fine a mapping from the expert's graph to WordNet, hence providing a two-level domain taxonomy. We apply this framework on a dataset of text and videos that capture best practices of rural populations, and find encouraging results for the same.},
author = {Shah, Simoni S and Bhattad, Shraddha and Lokegaonkar, Sanket and Ramakrishnan, Ganesh},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Shah et al. - Unknown - Building Complementary Domain Taxonomies using Query Enrichment.pdf:pdf},
title = {{Building Complementary Domain Taxonomies using Query Enrichment}}
}
@article{Nguyen2016,
author = {Nguyen, Thi Thanh Sang},
doi = {10.1145/3011141.3011195},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen - 2016 - A deep learning framework for book search.pdf:pdf},
isbn = {9781450348072},
journal = {Proceedings of the 18th International Conference on Information Integration and Web-based Applications and Services - iiWAS '16},
keywords = {and return appropriate content,entities,identify related,in the,in the database,matching method,more accurate searching results,proposed method can produce,searching content to entities,that the,the experimental results show,this method is used},
pages = {81--85},
title = {{A deep learning framework for book search}},
url = {http://dl.acm.org/citation.cfm?doid=3011141.3011195},
year = {2016}
}
@article{Minarro-Gimenez2015,
abstract = {BACKGROUND: The amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying properties of pharmaceuticals based on mid-sized, unstructured medical text corpora available on the web. Properties included relationships to diseases ('may treat') or physiological processes ('has physiological effect'). We compared the relationships identified by word2vec with manually curated information from the National Drug File - Reference Terminology (NDF-RT) ontology as a gold standard. RESULTS: Our results revealed a maximum accuracy of 49.28{\%} which suggests a limited ability of word2vec to capture linguistic regularities on the collected medical corpora compared with other published results. We were able to document the influence of different parameter settings on result accuracy and found and unexpected trade-off between ranking quality and accuracy. Pre-processing corpora to reduce syntactic variability proved to be a good strategy for increasing the utility of the trained vector models. CONCLUSIONS: Word2vec is a very efficient implementation for computing vector representations and for its ability to identify relationships in textual data without any prior domain knowledge. We found that the ranking and retrieved results generated by word2vec were not of sufficient quality for automatic population of knowledge bases and ontologies, but could serve as a starting point for further manual curation.},
archivePrefix = {arXiv},
arxivId = {1502.03682},
author = {Mi{\~{n}}arro-Gim{\'{e}}nez, JA},
eprint = {1502.03682},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mi{\~{n}}arro-Gim{\'{e}}nez - 2015 - Applying deep learning techniques on medical corpora from the World Wide Web a prototypical system and evalua.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--14},
title = {{Applying deep learning techniques on medical corpora from the World Wide Web: a prototypical system and evaluation}},
url = {http://arxiv.org/abs/1502.03682},
year = {2015}
}
@article{Mahajan,
abstract = {This paper outlines an approach to improving upon the word2vec skip-gram model. In order to obtain better word representations for morphologically rich languages and to yield more accurate results for rare or unseen words. A word representation is derived from the vector embeddings of its constituent n-grams, which are updated and learned in the training process. We train two models on an English corpus: one with 3-gram embeddings, and another with 2-to 6-gram em-beddings. We evaluate the models using word similarity and word analogy tasks and comparing to a word2vec skip-gram baseline.},
author = {Mahajan, Deepti and Patil, Radhika and Sankar, Varsha},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mahajan, Patil, Sankar - Unknown - Word2Vec using Character n-grams.pdf:pdf},
pages = {1--7},
title = {{Word2Vec using Character n-grams}},
url = {https://web.stanford.edu/class/cs224n/reports/2761021.pdf}
}
@article{Kumar1237,
abstract = {A knowledge graph is a structured graphical representation of semantic knowl-edge and relations where nodes in the graph represent the entities and the edges represent the relation between them. Constructing a knowledge graph involve extracting relations from unstructured text followed by efficient stor-age in graphical databases.In this project, we propose a method for extracting relations using semantic regularity in the distributed word vector embedding space. Such a semi-supervised approach is independent of language syntax and can be used to extract relations from any language. We also investigate various similarity metrics to annotate each extracted relation with a confidence score. We use 'Neo4j' graphical database for efficient storage of extracted relations and constructing a knowledge graph. We further build a question answering system that parses the natural language queries using regular expressions and extracts answers from the knowledge graph.},
author = {Kumar, Kundan and Mukherjee, Amitabh},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Mukherjee - 1237 - Constructing knowledge graph from unstructured text.pdf:pdf},
journal = {Siddhant Manocha},
number = {12375},
title = {{Constructing knowledge graph from unstructured text}},
url = {http://home.iitk.ac.in/{~}kundan/report{\_}365.pdf},
year = {1237}
}
@article{Hyland2015,
abstract = {Neural language models are a powerful tool to embed words into semantic vector spaces. However, learning such models generally relies on the availability of abundant and diverse training examples. In highly specialised domains this requirement may not be met due to difficulties in obtaining a large corpus, or the limited range of expression in average use. Such domains may encode prior knowledge about entities in a knowledge base or ontology. We propose a generative model which integrates evidence from diverse data sources, enabling the sharing of semantic information. We achieve this by generalising the concept of co-occurrence from distributional semantics to include other relationships between entities or words, which we model as affine transformations on the embedding space. We demonstrate the effectiveness of this approach by outperforming recent models on a link prediction task and demonstrating its ability to profit from partially or fully unobserved data training labels. We further demonstrate the usefulness of learning from different data sources with overlapping vocabularies.},
archivePrefix = {arXiv},
arxivId = {1510.00259},
author = {Hyland, Stephanie L. and Karaletsos, Theofanis and R{\"{a}}tsch, Gunnar},
eprint = {1510.00259},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hyland, Karaletsos, R{\"{a}}tsch - 2015 - A Generative Model of Words and Relationships from Multiple Sources.pdf:pdf},
isbn = {9781577357605},
title = {{A Generative Model of Words and Relationships from Multiple Sources}},
url = {http://arxiv.org/abs/1510.00259},
year = {2015}
}
@article{Maynard2008,
abstract = {This chapter investigates NLP techniques for ontology population, using a combination of rule-based approaches and machine learning. We describe a method for term recognition using linguistic and statistical techniques, making use of contextual information to bootstrap learning. We then investigate how term recognition techniques can be useful for the wider task of information extraction, making use of similarity metrics and contextual information. We describe two tools we have developed which make use of contextual information to help the development of rules for named entity recognition. Finally, we evaluate our ontology-based information extraction results using a novel technique we have developed which makes use of similarity-based metrics first developed for term recognition.},
author = {Maynard, D and Li, Y and Peters, W},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Maynard, Li, Peters - 2008 - NLP techniques for term extraction and ontology population.pdf:pdf},
isbn = {9781586038182},
issn = {09226389},
journal = {Ontology Learning and Population},
keywords = {information extraction,ontology population,term recognition},
pages = {107--127},
title = {{NLP techniques for term extraction and ontology population}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=wcBSjk2a2{\_}sC{\&}oi=fnd{\&}pg=PA107{\&}dq=Nlp+techniques+for+term+extraction+and+ontology+population{\&}ots=jUDkW94Qjq{\&}sig=HtAGKqWUCAUQOqiJIb-tt4AnbmU},
year = {2008}
}
@article{Jayawardana2017,
abstract = {In many modern day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study, we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.},
archivePrefix = {arXiv},
arxivId = {1709.02911},
author = {Jayawardana, Vindula and Lakmal, Dimuthu and de Silva, Nisansa and Perera, Amal Shehan and Sugathadasa, Keet and Ayesha, Buddhi and Perera, Madhavi},
eprint = {1709.02911},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jayawardana et al. - 2017 - Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings.pdf:pdf},
keywords = {ontology,ontology population,word embeddings},
title = {{Semi-Supervised Instance Population of an Ontology using Word Vector Embeddings}},
url = {http://arxiv.org/abs/1709.02911},
year = {2017}
}
@article{Jayawardana2017a,
abstract = {Selecting a representative vector for a set of vectors is a very common requirement in many algorithmic tasks. Traditionally, the mean or median vector is selected. Ontology classes are sets of homogeneous instance objects that can be converted to a vector space by word vector embeddings. This study proposes a methodology to derive a representative vector for ontology classes whose instances were converted to the vector space. We start by deriving five candidate vectors which are then used to train a machine learning model that would calculate a representative vector for the class. We show that our methodology out-performs the traditional mean and median vector representations.},
archivePrefix = {arXiv},
arxivId = {1706.02909},
author = {Jayawardana, Vindula and Lakmal, Dimuthu and de Silva, Nisansa and Perera, Amal Shehan and Sugathadasa, Keet and Ayesha, Buddhi},
doi = {10.1109/INTECH.2017.8102426},
eprint = {1706.02909},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jayawardana et al. - 2017 - Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings.pdf:pdf},
isbn = {9781509039883},
keywords = {neural networks,ontology,representative vector,word embedding,word2vec},
number = {Intech},
title = {{Deriving a Representative Vector for Ontology Classes with Instance Word Vector Embeddings}},
url = {http://arxiv.org/abs/1706.02909},
year = {2017}
}
@article{Aghaee2011,
address = {New York, New York, USA},
author = {Aghaee, Saeed and Pautasso, Cesare},
doi = {10.1145/2095536.2095591},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aghaee, Pautasso - 2011 - The mashup component description language.pdf:pdf},
isbn = {9781450307840},
journal = {Proceedings of the 13th International Conference on Information Integration and Web-based Applications and Services - iiWAS '11},
keywords = {component model,language,mashup,mashup components},
pages = {311},
publisher = {ACM Press},
title = {{The mashup component description language}},
url = {http://dl.acm.org/citation.cfm?doid=2095536.2095591},
year = {2011}
}
@article{Cappiello2009,
author = {Cappiello, Cinzia and Daniel, Florian and Matera, Maristella},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cappiello, Daniel, Matera - 2009 - A quality model for mashup components.pdf:pdf},
journal = {Web Engineering},
pages = {236--250},
title = {{A quality model for mashup components}},
url = {http://www.springerlink.com/index/f7w4244803074u76.pdf},
year = {2009}
}
@article{Garcia2012,
abstract = {Semantic Web Services discovery is commonly a heavyweight task, which has scalability issues when the number of services or the ontology complexity increase, because most approaches are based on Description Logic reasoning. As a higher number of services becomes available, there is a need for solutions that improve discovery performance. Our proposal tackles this scalability problem by adding a preprocessing stage based on two SPARQL queries that filter service repositories, discarding service descriptions that do not refer to any functionality or non-functional aspect requested by the user before the actual discovery takes place. This approach fairly reduces the search space for discovery mechanisms, consequently improving the overall performance of this task. Furthermore, this particular solution does not provide yet another discovery mechanism, but it is easily applicable to any of the existing ones, as our prototype evaluation shows. Moreover, proposed queries are automatically generated from service requests, transparently to the user. In order to validate our proposal, this article showcases an application to the OWL-S ontology, in addition to a comprehensive performance analysis that we carried out in order to test and compare the results obtained from proposed filters and current discovery approaches, discussing the benefits of our proposal. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Garc{\'{i}}a, Jos{\'{e}} Mar{\'{i}}a and Ruiz, David and Ruiz-Cort{\'{e}}s, Antonio},
doi = {10.1016/j.websem.2012.07.002},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Garc{\'{i}}a, Ruiz, Ruiz-Cort{\'{e}}s - 2012 - Improving semantic web services discovery using SPARQL-based repository filtering.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Scalability,Semantic web query languages,Semantic web services,Service discovery,Service repositories},
pages = {12--24},
title = {{Improving semantic web services discovery using SPARQL-based repository filtering}},
volume = {17},
year = {2012}
}
@inproceedings{Daniel2009d,
abstract = {Sometimes it looks like development for Web 2.0 is completely detached from the "traditional" world of web engineering. It is true that Web 2.0 introduced new and powerful instruments such as tags, micro formats, RESTful services, and light-weight programming models, which ease web development. However, it is also true that they didn't really substitute conventional practices such as component-based development and conceptual modeling. Traditional web engineering is still needed, especially when it comes to developing components for mashups, i.e., components such as web services or UI components that are meant to be combined, possibly by web users who are not skilled programmers. We argue that mashup components do not substantially differ from common web applications and that, hence, they might benefit from traditional web engineering methods and instruments. As a bridge toward Web 2.0, in this paper we show how, thanks to the adoption of suitable models and abstractions, generic web applications can comfortably be turned into mashup components.},
address = {Berlin, Heidelberg},
author = {Daniel, Florian and Matera, Maristella},
booktitle = {ICWE 2009},
editor = {Gaedke, Martin and Grossniklaus, Michael and D{\'{i}}az, Oscar},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Daniel, Matera - 2009 - Turning Web Applications into Mashup Components Issues, Models, and Solutions.pdf:pdf},
month = {jun},
pages = {45--60},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Turning Web Applications into Mashup Components: Issues, Models, and Solutions}},
url = {http://dl.acm.org/citation.cfm?id=1574438.1574444},
volume = {5648},
year = {2009}
}
@article{Chabeb2016,
author = {Chabeb, Yassin and Tata, Samir and Ozanne, Alain and Chabeb, Yassin and Tata, Samir and Ozanne, Alain and Chabeb, Yassin and Tata, Samir and Ozanne, Alain},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chabeb et al. - 2016 - YASA-M a semantic Web service matchmaker To cite this version HAL Id hal-01356801 YASA-M A Semantic Web Servi.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chabeb et al. - 2016 - YASA-M a semantic Web service matchmaker To cite this version HAL Id hal-01356801 YASA-M A Semantic Web Servi(2).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chabeb et al. - 2016 - YASA-M a semantic Web service matchmaker To cite this version HAL Id hal-01356801 YASA-M A Semantic Web Servi(3).pdf:pdf},
number = {Aina 2010},
title = {{YASA-M : a semantic Web service matchmaker To cite this version : HAL Id : hal-01356801 YASA-M : A Semantic Web Service Matchmaker}},
year = {2016}
}
@article{Novelli2012,
abstract = {There are many situations where it is needed to represent and analyze the concepts that describe a document or a collection of documents. One of such situations is the information retrieval, which is becoming more complex by the growing number and variety of document types. One way to represent the concepts is through a formal structure using ontologies. Thus, this article presents a fast and simple method for automatic extraction of ontologies from documents or from a collections of documents that is independent of the document type and uses the junction of several theories and techniques, such as latent semantic for the extraction of initial concepts, Wordnet and similarity to obtain the correlation between the concepts.},
author = {Novelli, Andreia Dal Ponte and {Parente De Oliveira}, Jos{\'{e}} Maria},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Novelli, Parente De Oliveira - 2012 - Simple Method for Ontology Automatic Extraction from Documents.pdf:pdf},
journal = {International Journal of Advanced Computer Science and Applications(IJACSA)},
keywords = {document ontology,ontology creation,ontology ext},
number = {12},
pages = {44--51},
title = {{Simple Method for Ontology Automatic Extraction from Documents}},
url = {http://ijacsa.thesai.org/},
volume = {3},
year = {2012}
}
@article{Domingos2010,
abstract = {Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate OntoUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47{\%} and greatly outperforms previous state-of-the-art approaches.},
author = {Domingos, Pedro and Poon, Hoifung and Domingos, Pedro},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Domingos, Poon, Domingos - 2010 - Unsupervised ontology induction from text.pdf:pdf},
isbn = {9781617388088},
journal = {ACL Workshops},
pages = {296--305},
title = {{Unsupervised ontology induction from text}},
url = {http://portal.acm.org/citation.cfm?id=1858681.1858712},
year = {2010}
}
@article{Zouaq2011,
author = {Zouaq, Amal and Gasevic, Dragan and Hatala, Marek},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zouaq, Gasevic, Hatala - 2011 - Unresolved Issues in Ontology Learning - Position Paper -.pdf:pdf},
journal = {Sciences-New York},
keywords = {challenges,ontology learning,semantic web},
pages = {52--57},
title = {{Unresolved Issues in Ontology Learning - Position Paper -}},
year = {2011}
}
@article{Ahmad2005,
author = {Ahmad, Khurshid and Gillam, Lee},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ahmad, Gillam - 2005 - Automatic Ontology Extraction from Unstructured Texts.pdf:pdf},
journal = {CoopIS/DOA/ODBASE 2005 (LNCS 3761)},
number = {3761},
pages = {1330--1346},
title = {{Automatic Ontology Extraction from Unstructured Texts}},
year = {2005}
}
@article{Wohlgenannt2016,
abstract = {Ontology learning has been an important research area in the Semantic Web field in the last 20 years. Ontology learning systems generate domain models from data (typically text) using a combination of sophisticated methods. In this poster, we study the use of Google's word2vec to emulate a simple ontology learning system, and compare the results to an existing " traditional " ontology learning system.},
author = {Wohlgenannt, Gerhard and Minic, Filip},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wohlgenannt, Minic - 2016 - Using word2vec to build a simple ontology learning system.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Ontology learning,Term extraction,Word2vec},
pages = {2--5},
title = {{Using word2vec to build a simple ontology learning system}},
volume = {1690},
year = {2016}
}
@article{Makris2010,
abstract = {The web of data consists of distributed, diverse (in terms of schema adopted), and large RDF datasets. In this paper we present a SPARQL query rewriting method which can be used to achieve interop- erability in semantic information retrieval and/or knowledge discovery processes over interconnected RDF data sources. Formal mappings be- tween different overlapping ontologies are exploited in order to rewrite initial user SPARQL queries, so that they can be evaluated over different RDF data sources on different sites. The proposed environment is uti- lized by an ontology-based mediator system, which we have developed in order to provide data integration within the SemanticWeb environment.},
author = {Makris, Konstantinos and Gioldasis, Nektarios and Bikakis, Nikos and Christodoulakis, Stavros},
doi = {10.1007/978-3-642-16949-6_32},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Makris et al. - 2010 - Ontology mapping and SPARQL rewriting for querying federated RDF data sources (short paper).pdf:pdf},
isbn = {3642169481},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {1108--1117},
title = {{Ontology mapping and SPARQL rewriting for querying federated RDF data sources (short paper)}},
volume = {6427 LNCS},
year = {2010}
}
@article{Meszaros2017,
author = {Meszaros, Erica L. and Chandarana, Meghan and Trujillo, Anna and {Danette Allen}, B.},
doi = {10.1109/ICUAS.2017.7991401},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Meszaros et al. - 2017 - Speech-based natural language interface for UAV trajectory generation.pdf:pdf},
isbn = {9781509044948},
journal = {2017 International Conference on Unmanned Aircraft Systems, ICUAS 2017},
pages = {46--55},
title = {{Speech-based natural language interface for UAV trajectory generation}},
year = {2017}
}
@article{Hearst2011,
abstract = {The future of user interfaces will involve support for natural human interaction, gesturing with fingers, speaking rather than typing, watching video rather than reading, and using IT socially rather than alone. This article has explored why these trends will also affect user interfaces for search, high-lighting recent work reflecting these trends. Using advanced processing techniques over huge sets of behavioral data, future search interfaces will better support finding other people to answer questions or provide opinions, more natural dialogue-like interaction, and information expressed as non-textual content through non-textual input. More-natural modes of interaction have long been goals of interface design, but recent developments have brought them closer to reality.},
author = {Hearst, Marti A.},
doi = {10.1145/2018396.2018414},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hearst - 2011 - 'Natural' search user interfaces.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {among others,at,face,inter-,is promoted by researchers,microsoft,natural inter-,not sur-,people are drawn to,prisingly,the term},
number = {11},
pages = {60},
pmid = {15784401},
title = {{'Natural' search user interfaces}},
url = {http://dl.acm.org/citation.cfm?doid=2018396.2018414},
volume = {54},
year = {2011}
}
@article{Malki2012,
abstract = {Mashups allowed a significant advance in the automation of interac- tions between applications and Web resources. In particular, the combination of Web APIs is seen as a strength, which can meet the complex needs by combin- ing the functionality and data from multiple services within a single Mashup application. Automating the process of building Mashup based mainly on the Semantics Web APIs facilitate to the developer their selection and matching. In this paper, we propose SAWADL (Semantic Annotation for Web Application Description Language), an extension of the WADL language that allows the semantization of the REST Web Service. We introduce a reference architecture with five layers representing the main functional blocks for annotating and combining web APIs, and therefore make the engineering process of Mashup applications more agile and more flexible.},
author = {Malki, Abdelhamid and Benslimane, Sidi Mohammed},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Malki, Benslimane - 2012 - Building semantic mashup.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {API,Matching,REST,SAWADL,SAWSDL,SOAP,Semantic mashup},
pages = {40--49},
title = {{Building semantic mashup}},
volume = {867},
year = {2012}
}
@article{Aghaee2012,
abstract = {Mashups are built by combining building blocks, which are commonly referred to as mashup components. These components are characterized by a high level of heterogeneity in terms of technologies, access methods, and the behavior they may exhibit within a mashup. Abstracting away this heterogeneity is the mission of the so-called mashup tools aiming at automating or semi-automating mashup development to serve non-programmers. The challenge is to ensure this abstraction mechanism does not limit the support for heterogeneous mashup components. In this paper, we propose a novel evaluation framework that can be applied to assess the degree to which a given mashup tool addresses this challenge. The evaluation framework can serve as a benchmark for future improved design of mashup tools with respect to heterogeneous mashup components support. In order to demonstrate the applicability of the framework, we also apply it to evaluate some existing tools.},
author = {Aghaee, Saeed and Pautasso, Cesare},
doi = {10.1007/978-3-642-27997-3_1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aghaee, Pautasso - 2012 - An evaluation of mashup tools based on support for heterogeneous mashup components.pdf:pdf},
isbn = {9783642279966},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Component Model,Evaluation Framework,Expressive Power,Mashup Components},
pages = {1--12},
title = {{An evaluation of mashup tools based on support for heterogeneous mashup components}},
volume = {7059 LNCS},
year = {2012}
}
@article{Oberle2014,
abstract = {This paper contributes an argumentation line for howtechnological features of ontologies lead to benefits for enterprise applications. Although many features are also available in precursory or alternative technologies, we claim that combinations of specific features are uniquely provided by ontologies. A careful elicitation of the available features therefore is a prerequisite for the argumentation line. As a second contribution, this paper reports on several challenges that frequently occur when trying to adopt ontologies in existing enterprise settings. These challenges have to be contrasted with the often overstressed benefits in Semantic Web literature. Together with reports for several SAP Research case studies, this paper channels back experiences in applying ontologies to the Semantic Web community. As a third contribution, we give several recommendations for future research directions based on the gathered experiences.},
author = {Oberle, Daniel},
doi = {10.3233/SW-130114},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Oberle - 2014 - How ontologies benefit enterprise applications.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
keywords = {Semantics,application,commercial,enterprise,industry,ontologies,reasoning,semantic technologies},
number = {6},
pages = {473--491},
title = {{How ontologies benefit enterprise applications}},
volume = {5},
year = {2014}
}
@article{Fischer2004,
abstract = {Therefore, EUD is only a partial success story. Here, we argue the spread of EUD depends on a fine balance between user motivation, effec-tive tools, and management sup-port. We explore that balance and investigate a future approach to EUD—meta-design—that proposes a vision in which design, learning, and development become part of everyday working practice. Designing language for user-computer communication poses a conflict between complexity and power. More complex lan-guages can address a wider range of problems but impose an End-user development (EUD) activities range from customization to component configuration and programming. Office software, such as the ubiquitous spreadsheet, provides customization facilities, while the growth of the Web has added impetus to end-user scripting for interactive func-tions in Web sites. In scientific and engineering domains, end users frequently develop complex sys-tems with standard programming languages such as C++ and Java. However, only a minority of users adapt commercial off-the-shelf (COTS) software products. Indeed, composing systems from reusable components, such as enterprise resource planing (ERP) systems, defeats most end users who resort to expensive and scarce expert developers for imple-mentation.},
author = {Fischer, G and Giaccardi, E and Ye, Y and Sutcliffe, A G and Mehandjiev, N},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fischer et al. - 2004 - META-DESIGN A MANIFESTO FOR END-USER DEVELOPMENT.pdf:pdf},
journal = {COMMUNICATIONS OF THE ACM},
number = {9},
title = {{META-DESIGN: A MANIFESTO FOR END-USER DEVELOPMENT}},
url = {http://delivery.acm.org/10.1145/1020000/1015884/p33-fischer.pdf?ip=134.109.184.147{\&}id=1015884{\&}acc=ACTIVE SERVICE{\&}key=2BA2C432AB83DA15.2C9D92A4DDBE0279.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=850073606{\&}CFTOKEN=58793854{\&}{\_}{\_}acm{\_}{\_}=1515667270{\_}7dbe08da3ba8d64576c},
volume = {47},
year = {2004}
}
@article{BURNETT2001,
author = {BURNETT, MARGARET},
journal = {Handbook of Software Engineering and Knowledge Engineering},
title = {{SOFTWARE ENGINEERING FOR VISUAL PROGRAMMING LANGUAGES}},
url = {ftp://ftp.cs.orst.edu/pub/burnett/softEngVPLs.pdf},
volume = {2},
year = {2001}
}
@article{Costabile2003,
abstract = { Software shaping workshops (SSWs) described in this paper are software environments designed to support various activities of end-user development (EUD) and tailoring. A design methodology to create easy-to-develop-and-tailor visual interactive systems that are organised as SSWs is illustrated. Users of an interactive system are in many cases experts in some domain different from computer science, who need to perform some task with the aid of the computer system. The design methodology allows users to directly collaborate to the system design and tailoring process to face co-evolution of user's and systems. The strategy feasibility is discussed, outlining its implementation through a Web-based prototype.},
author = {Costabile, M. F. and Fogli, D. and Fresta, G. and Mussio, P. and Piccinno, a.},
doi = {10.1109/HCC.2003.1260199},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Costabile et al. - 2003 - Building environments for end-user development and tailoring.pdf:pdf},
isbn = {0780382250},
issn = {17207525},
journal = {Proceedings - 2003 IEEE Symposium on Human Centric Computing Languages and Environments, HCC 2003},
pages = {31--38},
title = {{Building environments for end-user development and tailoring}},
year = {2003}
}
@inproceedings{Soi2010,
abstract = {Last years, aside the proliferation of Web 2.0, we assisted to the drastic growth of the mashup market. An increasing number of different mashup solutions and platforms emerged, some focusing on data integration (a la Yahoo! Pipes), others on user interface (UI) integration and some trying to integrate both UI and data. Most of proposed solutions have a common characteristic: they aim at providing non-programmers with a flexible and intuitive general-purpose development environment. While these generic environments could be useful for web users to develop simple applications, they are often too generic to address domain-specific needs and to allow users to develop real-life complex applications. In particular, proposed mashup mechanisms do not reflect those specific concepts that are proper of a given domain, which domain-experts are familiar with and could autonomously manage.We argue the need for domain-specific mashup architectures, also going beyond todays enterprise platforms, in which standard mashup mechanisms and components are driven by an underlying domain-specific layer. This layer will provide a service and component ecosystem built upon a shared and uniform conceptual model specific for the given domain. This way, domain experts will be provided with mashup components and mechanisms, following those well-known concepts and rules proper of the domain they belong to, that they are able to understand, use and, finally, profitably compose. In this paper, we will show the necessity of such an architecture through a real-life use case in the context of scientific publications and journals.},
author = {Soi, Stefano and Baez, Marcos},
booktitle = {ICWE 2010 Workshops},
doi = {10.1007/978-3-642-16985-4},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Soi, Baez - 2010 - Domain Specific Mashups From All to All You Need.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Soi, Baez - 2010 - Domain Specific Mashups From All to All You Need(2).pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Soi, Baez - 2010 - Domain Specific Mashups From All to All You Need(3).pdf:pdf},
isbn = {9783642169847},
keywords = {undefined},
pages = {384--395--395},
title = {{Domain Specific Mashups: From All to All You Need}},
url = {http://www.springerlink.com/content/315q477t07m6jq35/},
volume = {6385},
year = {2010}
}
@article{Klusch2008,
abstract = {The convergence of Semantic Web with service-oriented computing is manifested by Semantic Web service (SWS) technology. It addresses the major challenge of automated, interoperable and meaningful coordination of Web services to be carried out by intelligent software agents. In this chapter, we briefly discuss prominent SWS description frameworks, that are the standard SAWSDL, OWL-S and WSML1. This is complemented by a critique, and selected references to further readings on the subject.},
author = {Klusch, Matthias},
doi = {10.1007/978-3-7643-8575-0_3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Klusch - 2008 - Semantic Web Service Description.pdf:pdf},
isbn = {9783764385743},
journal = {CASCOM: Intelligent Service Coordination in the Semantic Web},
pages = {31--57},
title = {{Semantic Web Service Description}},
url = {http://dx.doi.org/10.1007/978-3-7643-8575-0{\_}3},
year = {2008}
}
@article{Bakshi2005,
author = {Bakshi, Karun and Karger, David R.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bakshi, Karger - 2005 - End-user application development for the Semantic Web.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{End-user application development for the Semantic Web}},
volume = {175},
year = {2005}
}
@article{Ghiani2016,
abstract = {End-User Development aims to find novel ways that are suitable and intuitive for end users to create their own applications. We present a graphical environment in which users create new mashups by directly selecting interaction elements, content and functionalities from existing Web applications without requiring the intervention of expert developers. Then, users just need to exploit a copy-paste metaphor to indicate how to compose the selected interactive content and functionalities in the new mashup. The environment is enabled by a Web-based platform accessible from any browser, and is suitable for users without particular programming skills. We describe the architecture of our platform and how it works, including its intelligent support, show example applications, and report the results of first user studies.},
author = {Ghiani, Giuseppe and Patern{\`{o}}, Fabio and Spano, Lucio Davide and Pintori, Giuliano},
doi = {10.1016/j.ijhcs.2015.10.008},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ghiani et al. - 2016 - An environment for End-User Development of Web mashups.pdf:pdf},
issn = {10959300},
journal = {International Journal of Human Computer Studies},
keywords = {End User Development,User interface development tools,Web mashups},
pages = {38--64},
publisher = {Elsevier},
title = {{An environment for End-User Development of Web mashups}},
url = {http://dx.doi.org/10.1016/j.ijhcs.2015.10.008},
volume = {87},
year = {2016}
}
@article{Jabar2013,
abstract = {Software Product Line was proposed in 2000 as a systematic approach toward reuse. Although benefits of this strategy is totally well known but there are still uncertainties between organizations about how they can apply SPL to their software product. This review tries to study articles in which adoption models and their properties were discussed in order to get a clear perspective about SPLs implementations and adoption strategy. This study is conducted as a Systematic Literature Review which was used to identify important characteristic which should be considered for product line adoption. 22 primary studies from different sources were evaluated to answer 2 research questions. The research identified more than 30 paper on this study but only 21 of them was precisely relevant in the field of SPL adoption. This research provides a general guideline for organization which wants to use SPL in their company. Our preliminary study conclude that in order to choose an appropriate adoption model organizations should identify their needs and choose best way according to that. {\textcopyright} 2005 - 2013 JATIT  {\&}  LLS. All rights reserved.},
author = {Jabar, M.A. and Zarei, M.B. and Sidi, F. and Sani, N.F.M.},
issn = {18173195},
journal = {Journal of Theoretical and Applied Information Technology},
keywords = {Adoption,Adoption model,Adoption strategy,Software product line},
number = {1},
title = {{A review of software product line adoption}},
volume = {57},
year = {2013}
}
@article{Hang2009,
abstract = {This paper proposes a data mining methodology called Business Intelligence-driven Data Mining (BIdDM). It combines knowledge-driven data mining and method-driven data mining, and fills the gap between business intelligence knowledge and existent various data mining methods in e-Business. BIdDM contains two processes: a construction process of a four-layer framework and a data mining process. A methodology is established in setting up the four-layer framework, which is an important part in BIdDM. A case study of B2C e-Shop is provided to illustrate the use of BIdDM.},
author = {Hang, Yang and Fong, Simon},
doi = {10.1109/NCM.2009.403},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hang, Fong - 2009 - A framework of business intelligence-driven data mining for e-Business.pdf:pdf},
isbn = {9780769537696},
journal = {NCM 2009 - 5th International Joint Conference on INC, IMS, and IDC},
keywords = {BI-driven Data Mining,Business intelligence,Data mining},
pages = {1964--1970},
title = {{A framework of business intelligence-driven data mining for e-Business}},
year = {2009}
}
@article{Soylu2017,
author = {Soylu, Ahmet and Giese, Martin and Evgeny, Ernesto Jimenez-ruiz and Dmitriy, Kharlamov and Ian, Zheleznyakov},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Soylu et al. - Unknown - Ontology-based End-user Visual Query Formulation Why , what , who , how , and which.pdf:pdf},
journal = {Universal Access in the Information Society},
keywords = {big data,data,ontology-based data access,retrieval,usability,visual query formulation},
number = {2},
pages = {435--467},
title = {{Ontology-based End-user Visual Query Formulation : Why , what , who , how , and which ?}},
volume = {16},
year = {2017}
}
@article{Spahn2008,
abstract = {Business users need to analyse changing sets of information to effectively support their working tasks. Due to the complexity of enterprise systems and available tools, especially technically unskilled users face considerable challenges when trying to flexibly retrieve needed data in an ad-hoc manner. As a consequence, available data is limited to information artefacts like queries or reports which have been predefined for them by IT experts. To improve information self-service capabilities of business users, we present an ontology-based architecture and end-user tool, enabling easy data access and query creation for business users. Our approach is based on a semantic middleware integrating data from heterogeneous information systems and providing a comprehensible data model in the form of a business level ontology (BO). We show how our end-user tool Semantic Query Designer (SQD) enables convenient navigation and query building upon the BO, and illustrate its usage and the processing of data over all layers of our system architecture in detail, using a comprehensible use case example. As flexible query creation is a crucial precondition of leveraging the usage of enterprise data, we contribute to the enablement of business users of making better informed decisions, thus increasing effectiveness and efficiency of business processes.},
author = {Spahn, Michael and Kleb, Joachim and Grimm, Stephan and Scheidl, Stefan},
doi = {10.1145/1452567.1452577},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Spahn et al. - 2008 - Supporting business intelligence by providing ontology-based end-user information self-service.pdf:pdf},
isbn = {9781605582191},
journal = {Proceedings of the first international workshop on Ontology-supported business intelligence - OBI '08},
keywords = {business intelligence,conceptual query design,end-,information integration,ontology,user development},
pages = {1--12},
title = {{Supporting business intelligence by providing ontology-based end-user information self-service}},
url = {http://portal.acm.org/citation.cfm?doid=1452567.1452577},
year = {2008}
}
@article{Kollia2011,
abstract = {The SPARQL query language is currently being extended by W3C with so-called entailment regimes, which define how queries are evaluated under more expressive semantics than SPARQL's standard simple entailment. We describe a sound and complete algorithm for the OWL Direct Semantics entailment regime. The queries of the regime are very expressive since variables can occur within complex class expressions and can also bind to class or property names. We propose several novel optimizations such as strategies for determining a good query execution order, query rewriting techniques, and show how specialized OWL reasoning tasks and the class and property hierarchy can be used to reduce the query execution time. We provide a prototypical implementation and evaluate the efficiency of the proposed optimizations. For standard conjunctive queries our system performs comparably to already deployed systems. For complex queries an improvement of up to three orders of magnitude can be observed.},
archivePrefix = {arXiv},
arxivId = {1402.0576},
author = {Kollia, Ilianna and Glimm, Birte and Horrocks, Ian},
doi = {10.1007/978-3-642-21034-1_26},
eprint = {1402.0576},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kollia, Glimm, Horrocks - 2011 - SPARQL Query Answering over OWL Ontologies.pdf:pdf},
isbn = {978-3-642-21033-4},
issn = {10769757},
journal = {The Semantic Web: Research and Applications},
keywords = {query,sparql},
pages = {382--396},
title = {{SPARQL Query Answering over OWL Ontologies}},
url = {http://dx.doi.org/10.1007/978-3-642-21034-1{\_}26},
volume = {6643},
year = {2011}
}
@article{Martins2008,
abstract = {Knowledge is of general utility and should be captured thinking in reuse. A key idea underlining knowledge capturing for reuse is to consider that there are two major kinds of knowledge: domain and task knowledge. Ontologies can be used for representing both kinds of knowledge. However, while domain ontologies are broadly used and there are many proposals of models for representing them, the same does not occur for task ontologies. In this paper we propose the use of UML activity diagrams for capturing task control-flow, and UML class diagrams for capturing the knowledge roles involved in its activities. We also discuss the interrelationship between these two models and how they can be combined with domain ontologies in order to describe the knowledge involved in a class of applications.},
author = {Martins, Aline Freitas and {De Almeida Falbo}, Ricardo},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Martins, De Almeida Falbo - 2008 - Models for representing task ontologies.pdf:pdf},
isbn = {16130073 (ISSN)},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{Models for representing task ontologies}},
volume = {427},
year = {2008}
}
@article{Shen2011,
abstract = {Discovering proper search intents is a vi- tal process to return desired results. It is constantly a hot research topic regarding information retrieval in recent years. Existing methods are mainly limited by utilizing context-based mining, query expansion, and user profiling techniques, which are still suffering from the issue of ambiguity in search queries. In this pa- per, we introduce a novel ontology-based approach in terms of a world knowledge base in order to construct personalized ontologies for identifying adequate con- cept levels for matching user search intents. An iter- ative mining algorithm is designed for evaluating po- tential intents level by level until meeting the best re- sult. The propose-to-attempt approach is evaluated in a large volume RCV1 data set, and experimental results indicate a distinct improvement on top precision after compared with baseline models.},
author = {Shen, Yan and Li, Yuefeng and Xu, Yue and Iannella, Renato},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2011 - An ontology-based mining approach for user search intent discovery.pdf:pdf},
isbn = {9781921426926},
journal = {2011: Proceedings of},
keywords = {lcsh,ontology mining,search intent},
number = {December},
title = {{An ontology-based mining approach for user search intent discovery}},
url = {http://eprints.qut.edu.au/48096/},
year = {2011}
}
@article{Cunningham2001,
abstract = {2 of the system, a complete re- implementation and This book describes how to use to , test their},
author = {Cunningham, Hamish and Maynard, D and Bontcheva, K and Tablan, V and Ursu, C and Dimitrov, M and Dowman, M and Aswani, N},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cunningham et al. - 2001 - Developing language processing components with GATE (a user guide).pdf:pdf},
isbn = {0956599311, 9780956599315},
journal = {University of Sheffield},
number = {Gate 2},
pages = {1--457},
title = {{Developing language processing components with GATE (a user guide)}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.1949{\&}rep=rep1{\&}type=pdf},
volume = {2006},
year = {2001}
}
@article{Hao2010,
author = {Hao, Jin-xing and Yan, Angela and Chi-Wai, Ron},
doi = {10.2991/icebi.2010.19},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hao, Yan, Chi-Wai - 2010 - A Semantic Analysis Method for Concept Map-based Knowledge Modeling.pdf:pdf},
isbn = {978-90-78677-40-6},
journal = {Proceedings of the 1st International Conference on E-Business Intelligence (ICEBI-2010)},
keywords = {concept map},
pages = {281--287},
title = {{A Semantic Analysis Method for Concept Map-based Knowledge Modeling}},
url = {http://www.atlantis-press.com/php/paper-details.php?id=2029},
year = {2010}
}
@book{Krempels2011,
author = {Krempels, Karl-heinz and Majchrzak, Tim A and Eds, Paolo Traverso},
doi = {10.1007/978-3-642-22810-0},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Krempels, Majchrzak, Eds - 2011 - Web Information Systems and Technologies.pdf:pdf},
isbn = {978-3-642-22809-4},
title = {{Web Information Systems and Technologies}},
url = {http://link.springer.com/10.1007/978-3-642-22810-0},
volume = {75},
year = {2011}
}
@article{Pietschmann2012,
author = {Pietschmann, Stefan},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pietschmann - 2012 - Modellgetriebene Entwicklung adaptiver, komponentenbasierter Mashup-Anwendungen.pdf:pdf},
journal = {Qucosa.De},
keywords = {adaptive hypermedia,adaptivity,cbse,component,context,context-awareness,mashup,mdd,model-driven development,soa,universal composition,web},
number = {May},
title = {{Modellgetriebene Entwicklung adaptiver, komponentenbasierter Mashup-Anwendungen}},
year = {2012}
}
@article{Ramkumar2014,
author = {Ramkumar, A. Sudha and Poorna, B.},
doi = {10.1109/ICICA.2014.82},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ramkumar, Poorna - 2014 - Ontology Based Semantic Search An Introduction and a Survey of Current Approaches.pdf:pdf},
isbn = {978-1-4799-3966-4},
journal = {2014 International Conference on Intelligent Computing Applications},
keywords = {an information retrieval process,been defined to be,classification criteria,domain ontology is a,hierarchically structured set of,ontology,semantic search,that uses,the background knowledge of,the domain ontology},
pages = {372--376},
title = {{Ontology Based Semantic Search: An Introduction and a Survey of Current Approaches}},
url = {http://ieeexplore.ieee.org/document/6965074/},
year = {2014}
}
@article{Luddecke2014,
abstract = {Context-aware systems aim to improve the interaction between a computer and a human being by using contextual information about the system itself, the user, and their environment. The number of relevant contextual information is expected to grow rapidly within the next years which tends to result in a complex, error-prone and hence, expensive task of programming context-aware systems. Model-based development can overcome these issues. Current approaches do not allow to model calculation of reliabilities and do not offer options to handle multiple sources of contextual information. In this paper, we present an approach of modeling contextual information of a context-aware system using the example of a contextaware in-car infotainment system. In particular, we show how developers of context-aware in-car infotainment systems can model reliability calculations of contextual information and handling of multiple sources of contextual information by using a hybrid, ontology-based modeling technique.},
author = {L{\"{u}}ddecke, Daniel and Bergmann, Nina and Schaefer, Ina},
doi = {10.1007/978-3-319-11653-2_30},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/L{\"{u}}ddecke, Bergmann, Schaefer - 2014 - Ontology-based modeling of context-aware systems.pdf:pdf},
isbn = {978-3-319-11652-5},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Context-aware,Infotainment,Modeling,Ontology},
pages = {484--500},
title = {{Ontology-based modeling of context-aware systems}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84921636805{\&}partnerID=tZOtx3y1},
volume = {8767},
year = {2014}
}
@article{Pietschmann2010,
abstract = {Recently, mashups, i.e., composite web applications, have gained momentum in both the consumer and enterprise sector. However, they lack a structured development process which abstracts from specific platforms and enables a universal composition including the presentation layer. Moreover, support for context-awareness in such applications has been neglected so far. Yet, it becomes increasingly necessary due to the growing heterogeneity of users and devices. We address these issues by proposing an open, extensible metamodel for component-based mashup applications. It defines their control flow, layout, and adaptivity while building on lessons learned from traditional web application models. This metamodel forms the basis for a structured, model-driven development process entailing reusability and independence from specific composition platforms. We implemented the metamodel and successfully proved its practicability with a number of exemplary applications deployed on different platforms.},
author = {Pietschmann, Stefan and Tietz, Vincent and Reimann, Jan and Liebing, Christian and Pohle, Mich{\`{e}}l and Mei{\ss}ner, Klaus},
doi = {10.1145/1967486.1967551},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pietschmann et al. - 2010 - A metamodel for context-aware component-based mashup applications(2).pdf:pdf},
isbn = {9781450304214},
journal = {Proceedings of the 12th International Conference on Information Integration and Web-based Applications {\&} Services - iiWAS '10},
pages = {413--420},
title = {{A metamodel for context-aware component-based mashup applications}},
url = {http://doi.acm.org/10.1145/1967486.1967551{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1967486.1967551},
year = {2010}
}
@article{Pietschmann2011,
author = {Pietschmann, Stefan and Radeck, Carsten and Mei{\ss}ner, Klaus},
doi = {10.1145/2076006.2076014},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pietschmann, Radeck, Mei{\ss}ner - 2011 - Semantics-based discovery, selection and mediation for presentation-oriented mashups.pdf:pdf},
isbn = {9781450308236},
journal = {Proceedings of the 5th International Workshop on Web APIs and Service Mashups - Mashups '11},
keywords = {-based search performed by,naturally,the users},
number = {September},
pages = {1},
title = {{Semantics-based discovery, selection and mediation for presentation-oriented mashups}},
url = {http://dl.acm.org/citation.cfm?doid=2076006.2076014},
year = {2011}
}
@article{Rodriguez-Echeverria2012,
abstract = {Nowadays, there is a current trend in software industry to modernize traditional Web Applications (WAs) to Rich Internet Applications (RIAs). RIAs improve the user experience by combining the lightweight distribution architecture of the Web with the interface interactivity and computation power of desktop applications. In this context, Model Driven Web Engineering (MDWE) approaches have been extended with new modeling primitives to obtain the benefits provided by RIA features. However, during the last decade, widespread language-specific web frameworks have supported actual web system development. In this paper we present a model driven modernization process to obtain RIAs from legacy web systems based on such frameworks. model driven techniques reduce complexity and improve reusability of the process, making the development more systematic and less error prone. Being navigational information of upmost importance for the modernization process of a web application, the paper is focused on presenting the model driven extraction of such concern from the legacy system artifact, presenting the extraction tools and process. {\textcopyright} 2012 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Rodr{\'{i}}guez-Echeverr{\'{i}}a, Roberto and Conejero, Jos{\'{e}} M. and Clemente, Pedro J. and Pav{\'{o}}n, V{\'{i}}ctor M. and S{\'{a}}nchez-Figueroa, Fernando},
doi = {10.1007/978-3-642-35623-0},
eprint = {9780201398298},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rodr{\'{i}}guez-Echeverr{\'{i}}a et al. - 2012 - Current Trends in Web Engineering.pdf:pdf},
isbn = {978-3-642-35622-3},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Model-driven Engineering,RIA,Re-engineering,Web Applications},
number = {August 2014},
pages = {56--70},
pmid = {4520227},
title = {{Current Trends in Web Engineering}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84870866911{\&}partnerID=tZOtx3y1},
volume = {7703},
year = {2012}
}
@article{Pietschmann2010a,
abstract = {Recently, mashups, i.e., composite web applications, have gained momentum in both the consumer and enterprise sector. However, they lack a structured development process which abstracts from specific platforms and enables a universal composition including the presentation layer. Moreover, support for context-awareness in such applications has been neglected so far. Yet, it becomes increasingly necessary due to the growing heterogeneity of users and devices. We address these issues by proposing an open, extensible metamodel for component-based mashup applications. It defines their control flow, layout, and adaptivity while building on lessons learned from traditional web application models. This metamodel forms the basis for a structured, model-driven development process entailing reusability and independence from specific composition platforms. We implemented the metamodel and successfully proved its practicability with a number of exemplary applications deployed on different platforms.},
author = {Pietschmann, Stefan and Tietz, Vincent and Reimann, Jan and Liebing, Christian and Pohle, Mich{\`{e}}l and Mei{\ss}ner, Klaus},
doi = {10.1145/1967486.1967551},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pietschmann et al. - 2010 - A metamodel for context-aware component-based mashup applications.pdf:pdf},
isbn = {9781450304214},
journal = {Proceedings of the 12th International Conference on Information Integration and Web-based Applications {\&} Services - iiWAS '10},
keywords = {model-driven development, composite applications,},
number = {June},
pages = {413--420},
title = {{A metamodel for context-aware component-based mashup applications}},
url = {http://doi.acm.org/10.1145/1967486.1967551{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1967486.1967551},
year = {2010}
}
@article{Kimmig2014,
author = {Kimmig, Markus and Monperrus, Martin and Mezini, Mira and Kimmig, Markus and Monperrus, Martin and Mezini, Mira},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kimmig et al. - 2014 - A Natural Language Interface for Code Search To cite this version A Natural Language Interface for Code Search.pdf:pdf},
title = {{A Natural Language Interface for Code Search To cite this version : A Natural Language Interface for Code Search}},
year = {2014}
}
@article{Heil2012,
author = {Heil, Sebastian},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Heil - 2012 - Master ´ s Thesis.pdf:pdf},
number = {August},
title = {{Master ´ s Thesis}},
year = {2012}
}
@book{Grossniklaus2012,
abstract = {ICWE 2012 International Workshops MDWE, ComposableWeb, WeRE, QWE, and Doctoral Consortium, Berlin, Germany, July 23-27, 2012, Revised Selected Papers},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Grossniklaus, Michael and Wimmer, Manuel},
doi = {10.1007/978-3-642-35623-0},
eprint = {9780201398298},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Grossniklaus, Wimmer - 2012 - Current Trends in Web Engineering.pdf:pdf},
isbn = {978-3-642-35622-3},
issn = {0302-9743},
pages = {264},
pmid = {4520227},
title = {{Current Trends in Web Engineering}},
url = {http://link.springer.com/10.1007/3-540-68339-9{\_}34{\%}5Cnhttp://link.springer.com/10.1007/978-3-642-35623-0},
volume = {7703},
year = {2012}
}
@book{Daniel2014,
abstract = {Mashups have emerged as an innovative software trend that re-interprets existing Web building blocks and leverages the composition of individual components in novel, value-adding ways. Additional appeal also derives from their potential to turn non-programmers into developers. Daniel and Matera have written the first comprehensive reference work for mashups. They systematically cover the main concepts and techniques underlying mashup design and development, the synergies among the models involved at different levels of abstraction and the way models materialize into composition paradigms and architectures of corresponding development tools. The book deliberately takes a balanced approach, combining a scientific perspective on the topic with an in-depth view on relevant technologies. To this end, the first part of the book introduces the theoretical and technological foundations for designing and developing mashups, as well as for designing tools that can aid mashup development. The second part then focuses more specifically on various aspects of mashups. It discusses a set of core component technologies, core approaches and architectural patterns, with a particular emphasis on tool-aided mashup development exploiting model-driven architectures. Development processes for mashups are also discussed and special attention is paid to composition paradigms for the end-user development of mashups and quality issues. Overall, the book is of interest to a wide range of readers. Students, lecturers, and researchers will find a comprehensive overview of core concepts and technological foundations for mashup implementation and composition. Even without low-level coding details, practitioners like software architects will find guidance on key implementation concepts, architectural patterns and development tools and approaches. A related website provides additional teaching material which can be used either as part of a course or for self study.},
author = {Daniel, Florian and Matera, Maristella},
doi = {10.1007/978-3-642-55049-2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Daniel, Matera - 2014 - Mashups Concepts, Models and Architectures.pdf:pdf},
isbn = {978-3-642-55048-5},
pages = {319},
publisher = {Springer-Verlag Berlin Heidelberg},
title = {{Mashups: Concepts, Models and Architectures}},
year = {2014}
}
@article{Cabot2017,
abstract = {Includes author index.},
author = {Cabot, Jordi and {De Virgilio}, Roberto and Torlone, Riccardo},
doi = {10.1007/978-3-319-60131-1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cabot, De Virgilio, Torlone - 2017 - Web engineering 17th international conference, ICWE 2017 Rome, Italy, June 5-8, 2017 proceedings.pdf:pdf},
isbn = {9783319601304},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {end user development,knowledge extraction},
pages = {608--615},
title = {{Web engineering: 17th international conference, ICWE 2017 Rome, Italy, June 5-8, 2017 proceedings}},
volume = {10360 LNCS},
year = {2017}
}
@book{Lieberman2006,
address = {Dordrecht},
doi = {10.1007/1-4020-5386-X},
editor = {Lieberman, Henry and Patern{\`{o}}, Fabio and Wulf, Volker},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rode, Rosson - 2006 - End User Development.pdf:pdf},
isbn = {978-1-4020-4220-1},
publisher = {Springer Netherlands},
series = {Human-Computer Interaction Series},
title = {{End User Development}},
url = {http://link.springer.com/10.1007/1-4020-5386-X},
volume = {9},
year = {2006}
}
@article{Tsegaye2015,
author = {Tsegaye, Gashaw and Atnafu, Solomon},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tsegaye, Atnafu - 2015 - Design of Local Web Content Observatory System.pdf:pdf},
isbn = {9781479974986},
keywords = {crawler,information retrieval,language identification,local web content observatory,web document categorization},
title = {{Design of Local Web Content Observatory System}},
year = {2015}
}
@book{Barbosa2017,
author = {Barbosa, Simone and Patern{\`{o}}, Fabio and Eds, Stefano Valtolina and Symposium, International and Hutchison, David},
doi = {10.1007/978-3-319-58735-6},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Barbosa et al. - 2017 - End-User Development.pdf:pdf},
isbn = {978-3-319-58734-9},
issn = {16113349},
title = {{End-User Development}},
url = {http://link.springer.com/10.1007/978-3-319-58735-6},
volume = {10303},
year = {2017}
}
@article{Latih2014,
abstract = {End-user Programming for the web is currently of interest because Web 2.0 technologies have resulted in a vast array of tools available for mashup making. This paper presents a Systematic Literature Review of EUP for web mashups. Its objective is to outline a comprehensive review and synthesis of the literature related to EUP for web mashups. A Systematic Literature Review was performed of peer reviewed published studies that focused on research in EUP for Web mashups. A review was conducted on 21 relevant articles, mostly recent (published between January 1st 2000 and December 31st 2012) and published in English. Five EUP approaches for web mashups were identified from the studies; browsing, programming by demonstration or example, spreadsheet, widget, data-flow and block-based approach. Other researches regarding EUP for web mashups were also identified, such as ubiquitous platform mashups, users' support functions, data extraction techniques, and process-oriented mashups. {\textcopyright} 2005 - 2014 JATIT {\&} LLS. All rights reserved.},
author = {Latih, Rodziah and Patel, Ahmed and Zin, Abdullah Mohd},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Latih, Patel, Zin - 2014 - A Systematic Literature Review of end-user programming for the web mashup.pdf:pdf},
issn = {18173195},
journal = {Journal of Theoretical and Applied Information Technology},
keywords = {End User Programming,SLR,Web 2.0,Web Meshup},
number = {1},
pages = {119--132},
title = {{A Systematic Literature Review of end-user programming for the web mashup}},
volume = {60},
year = {2014}
}
@article{,
doi = {10.1016/J.JVLC.2004.08.004},
issn = {1045-926X},
journal = {Journal of Visual Languages {\&} Computing},
month = {feb},
number = {1-2},
pages = {119--152},
publisher = {Academic Press},
title = {{The JOpera visual composition language}},
url = {http://www.sciencedirect.com/science/article/pii/S1045926X04000400?via{\%}3Dihub},
volume = {16},
year = {2005}
}
@incollection{Fogli2016,
author = {Fogli, Daniela and Lanzilotti, Rosa and Piccinno, Antonio},
doi = {10.1007/978-3-319-39862-4_7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fogli, Lanzilotti, Piccinno - 2016 - End-User Development Tools for the Smart Home A Systematic Literature Review.pdf:pdf},
month = {jul},
pages = {69--79},
publisher = {Springer, Cham},
title = {{End-User Development Tools for the Smart Home: A Systematic Literature Review}},
url = {http://link.springer.com/10.1007/978-3-319-39862-4{\_}7},
year = {2016}
}
@incollection{Pietschmann2009,
author = {Pietschmann, Stefan and Voigt, Martin and R{\"{u}}mpel, Andreas and Mei{\ss}ner, Klaus},
doi = {10.1007/978-3-642-02818-2_41},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pietschmann et al. - 2009 - CRUISe Composition of Rich User Interface Services.pdf:pdf},
pages = {473--476},
publisher = {Springer, Berlin, Heidelberg},
title = {{CRUISe: Composition of Rich User Interface Services}},
url = {http://link.springer.com/10.1007/978-3-642-02818-2{\_}41},
year = {2009}
}
@article{Mashups2008,
author = {Mashups, Service and Maximilien, E Michael},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mashups, Maximilien - 2008 - An Online Platform for Web.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mashups, Maximilien - 2008 - An Online Platform for Web(2).pdf:pdf},
journal = {Ieee Internet Computing},
title = {{An Online Platform for Web}},
year = {2008}
}
@incollection{Cappiello2011,
author = {Cappiello, Cinzia and Matera, Maristella and Picozzi, Matteo and Sprega, Gabriele and Barbagallo, Donato and Francalanci, Chiara},
doi = {10.1007/978-3-642-22233-7_11},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cappiello et al. - 2011 - DashMash A Mashup Environment for End User Development.pdf:pdf},
pages = {152--166},
publisher = {Springer, Berlin, Heidelberg},
title = {{DashMash: A Mashup Environment for End User Development}},
url = {http://link.springer.com/10.1007/978-3-642-22233-7{\_}11},
year = {2011}
}
@article{Lin2008,
abstract = {Mashups are an increasingly popular way to integrate data from multiple web sites to fit a particular need, but it often requires substantial technical expertise to create them. To lower the barrier for creating mashups, we have extended the CoScripter web automation tool with a spreadsheet-like environment called Vegemite. Our system uses direct-manipulation and programming-by-demonstration tech-niques to automatically populate tables with information collected from various web sites. A particular strength of our approach is its ability to augment a data set with new values computed by a web site, such as determining the driving distance from a particular location to each of the addresses in a data set. An informal user study suggests that Vegemite may enable a wider class of users to address their information needs.},
author = {Lin, James and Wong, Jeffrey and Nichols, Jeffrey and Cypher, Allen and Lau, Tessa A.},
doi = {10.1145/1502650.1502667},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2008 - End-user programming of mashups with vegemite.pdf:pdf},
isbn = {9781605581682},
journal = {Proceedingsc of the 13th international conference on Intelligent user interfaces - IUI '09},
pages = {97},
title = {{End-user programming of mashups with vegemite}},
url = {http://portal.acm.org/citation.cfm?doid=1502650.1502667},
year = {2008}
}
@incollection{Maximilien2007,
address = {Berlin, Heidelberg},
author = {Maximilien, E. Michael and Wilkinson, Hernan and Desai, Nirmit and Tai, Stefan},
booktitle = {Service-Oriented Computing – ICSOC 2007},
doi = {10.1007/978-3-540-74974-5_2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Maximilien et al. - 2007 - A Domain-Specific Language for Web APIs and Services Mashups.pdf:pdf},
pages = {13--26},
publisher = {Springer Berlin Heidelberg},
title = {{A Domain-Specific Language for Web APIs and Services Mashups}},
url = {http://link.springer.com/10.1007/978-3-540-74974-5{\_}2},
year = {2007}
}
@article{Ennals2007,
author = {Ennals, Rob and Brewer, Eric and Garofalakis, Minos and Shadle, Michael and Gandhi, Prashant},
doi = {10.1145/1361348.1361355},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ennals et al. - 2007 - Intel Mash Maker.pdf:pdf},
issn = {01635808},
journal = {ACM SIGMOD Record},
keywords = {data integration,mashup,personalization,visualization},
month = {dec},
number = {4},
pages = {27},
publisher = {ACM},
title = {{Intel Mash Maker}},
url = {http://portal.acm.org/citation.cfm?doid=1361348.1361355},
volume = {36},
year = {2007}
}
@inproceedings{Daniel:2009:HUC:1694170.1694213,
address = {Berlin, Heidelberg},
author = {Daniel, Florian and Casati, Fabio and Benatallah, Boualem and Shan, Ming-Chien},
booktitle = {Proceedings of the 28th International Conference on Conceptual Modeling},
doi = {10.1007/978-3-642-04840-1_32},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Daniel et al. - 2009 - Hosted Universal Composition Models, Languages and Infrastructure in mashArt.pdf:pdf},
isbn = {978-3-642-04839-5},
pages = {428--443},
publisher = {Springer-Verlag},
series = {ER '09},
title = {{Hosted Universal Composition: Models, Languages and Infrastructure in mashArt}},
url = {http://dx.doi.org/10.1007/978-3-642-04840-1{\_}32},
year = {2009}
}
@article{Wang2008,
abstract = {If  editors inherited the key characteristics of  strong visual format, they would help  users in  and improving  experience in building . },
author = {Wang, G and Yang, S and Han, Y},
doi = {10.1145/1526709.1526825},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Yang, Han - 2008 - Mashroom End-User Mashup Programming Using Nested Tables.pdf:pdf},
isbn = {9781605584874},
journal = {Proceedings of the 18th International Conference on World Wide Web},
keywords = {end-user programming,mashup,nested table,spreadsheet},
pages = {861--870},
title = {{Mashroom: End-User Mashup Programming Using Nested Tables}},
url = {papers2://publication/uuid/9E0A3C3A-4117-4A39-B6CA-BD5DD6D6A20D},
year = {2008}
}
@article{Leshed2008,
abstract = {Modern enterprises are replete with numerous online processes. Many must be performed frequently and are tedious, while others are done less frequently yet are complex or hard to remember. We present interviews with knowledge workers that reveal a need ...},
author = {Leshed, Gilly and Haber, Eben M. and Matthews, Tara and Lau, Tessa},
doi = {10.1145/1357054.1357323},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Leshed et al. - 2008 - CoScripter Automating {\&} Sharing How-To Knowledge in the Enterprise.pdf:pdf},
isbn = {9781605580111},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1719--1728},
title = {{CoScripter : Automating {\&} Sharing How-To Knowledge in the Enterprise}},
year = {2008}
}
@book{Desolda2016,
address = {Cham},
author = {Desolda, Giuseppe and Ardito, Carmelo and Matera, Maristella},
doi = {10.1007/978-3-319-28727-0},
editor = {Daniel, Florian and Pautasso, Cesare},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Desolda, Ardito, Matera - 2016 - EFESTO A Platform for the End-User Development of Interactive Workspaces for Data Exploration.pdf:pdf},
isbn = {978-3-319-28726-3},
language = {en},
pages = {63--81},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{EFESTO: A Platform for the End-User Development of Interactive Workspaces for Data Exploration}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-28727-0{\_}5/fulltext.html},
volume = {591},
year = {2016}
}
@inproceedings{Aghaee2012a,
abstract = {Mashup is defined as the practice of lightweight composition, serendipitous reuse, and user-centric development on the Web. In spite of the fact that the development of mashups is rather simple due to the reuse of all the required layers of a Web application (functionality, data, and user interface), it still requires programming experience. This is a significant hurdle for non-programmers (end-users with minimal or no programming experience), who constitute the majority of Web users. To cope with this, an End-User Programming (EUP) tool can be designed to reduce the barriers of mashup development, in a way that even non-programmers will be able to create innovative, feature-rich mashups. In this paper, we give an overview of the existing EUP approaches for mashup development, as well as a list of open research challenges.},
author = {Aghaee, Saeed and Pautasso, Cesare},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-27997-3_38},
isbn = {9783642279966},
issn = {03029743},
pages = {347--351},
publisher = {Springer-Verlag},
title = {{End-user programming for web mashups: Open research challenges}},
url = {http://link.springer.com/10.1007/978-3-642-27997-3{\_}38},
volume = {7059 LNCS},
year = {2012}
}
@article{Fischer1990,
abstract = { Convivial systems encourage users to be actively engaged in generating creative extensions to the artifacts given to them. Convivial systems have the potential to break down the counterproductive barrier between programming and using programs. Knowledge-based design environments are prototypes for convivial systems. These environments support human problem-domain communication, letting users work within their domains of expertise. One of the design rationales behind design environments is to ease the construction and modification of artifacts designed within the environment. But because design environments are intentionally not general purpose programming environments, situations will arise that require modifications to the design environment itself. The rationale and the techniques for these later modifications are discussed in this paper. Our conceptual framework for end-user modifiability is illustrated in the context of JANUS, an environment for architectural design. Evaluating our system building efforts against our objectives shows the subtleties of integrating end-user modifiability in these kinds of systems. },
author = {Fischer, Gerhard and Girgensohn, Andreas},
doi = {10.1145/97243.97272},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fischer, Girgensohn - 1990 - End-user modifiability in design environments.pdf:pdf},
isbn = {0-201-50932-6},
journal = {CHI '90: Proceedings of the SIGCHI conference on Human factors in computing systems},
pages = {183--192},
title = {{End-user modifiability in design environments}},
url = {http://doi.acm.org/10.1145/97243.97272},
year = {1990}
}
@article{Ghiani2011,
author = {Ghiani, Giuseppe and Patern{\`{o}}, Fabio and Spano, L},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ghiani, Patern{\`{o}}, Spano - 2011 - Creating mashups by direct manipulation of existing web applications.pdf:pdf},
journal = {End-User Development},
keywords = {end user development,mashups,web applications},
pages = {42--52},
title = {{Creating mashups by direct manipulation of existing web applications}},
url = {http://www.springerlink.com/index/D5X8150851042663.pdf},
year = {2011}
}
@article{Preciado2007,
author = {Preciado, J. C. and Linaje, M. and Comai, S. and Sanchez-Figueroa, F.},
doi = {10.1109/WSE.2007.4380240},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Preciado et al. - 2007 - Designing Rich Internet Applications with Web Engineering Methodologies.pdf:pdf},
isbn = {978-1-4244-1450-5},
journal = {2007 9th IEEE International Workshop on Web Site Evolution},
pages = {23--30},
title = {{Designing Rich Internet Applications with Web Engineering Methodologies}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4380240},
year = {2007}
}
@article{Pane2002,
abstract = {A programming system is the user interface between the programmer and the computer. Programming is a notoriously difficult activity, and some of this difficulty can be attributed to the user interface as opposed to other factors. Historically, the designs of programming languages and tools have not emphasized usability. This paper describes the process we used to design HANDS, a new programming system for children that focuses on usability, where HCI knowledge, principles, and methods guided all design decisions. The features of HANDS are presented along with their motivations from prior empirical research on programmers and new studies conducted by the authors. HANDS is an event-based language that features a concrete model for computation, provides operators that match the way non-programmers express problem solutions, and includes domain-specific features for the creation of interactive animations and simulations. In user tests, children using HANDS performed significantly better than children using a reduced-feature version of the system where more traditional methods were required to solve tasks.},
author = {Pane, J. F. and Myers, B. A. and Miller, L. B.},
doi = {10.1109/HCC.2002.1046372},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pane, Myers, Miller - 2002 - Using HCI techniques to design a more usable programming system.pdf:pdf;:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pane, Myers, Miller - 2002 - Using HCI techniques to design a more usable programming system(2).pdf:pdf},
isbn = {0769516440},
journal = {Proceedings - IEEE 2002 Symposia on Human Centric Computing Languages and Environments, HCC 2002},
number = {Hcc},
pages = {198--206},
title = {{Using HCI techniques to design a more usable programming system}},
year = {2002}
}
@inproceedings{Faaborg2006a,
address = {New York, New York, USA},
author = {Faaborg, Alexander and Lieberman, Henry},
booktitle = {Proceedings of the SIGCHI conference on Human Factors in computing systems  - CHI '06},
doi = {10.1145/1124772.1124883},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Faaborg, Lieberman - 2006 - A goal-oriented web browser.pdf:pdf},
isbn = {1595933727},
keywords = {ConceptNet,TAP,commonsense reasoning,context aware computing,data detectors,goal-oriented design,open mind,programming by example,software agents},
pages = {751},
publisher = {ACM Press},
title = {{A goal-oriented web browser}},
url = {http://portal.acm.org/citation.cfm?doid=1124772.1124883},
year = {2006}
}
@article{Lizcano2016,
abstract = {The Future Internet is expected to be composed of a mesh of interoperable web services accessed from all over the Web. This approach has been supported by many software providers who have provided a wide range of mash up tools for creating composite applications based on components prepared by the respective provider. These tools aim to achieve the end-user development (EUD) of rich internet applications (RIA); however, most, having failed to meet the needs of end users without programming knowledge, have been unsuccessful. Thus, many studies have investigated success factors in order to propose scales of success factor objectives and assess the adequacy of mashup tools for their purpose. After reviewing much of the available literature, this paper proposes a new success factor scale based on human factors, human-computer interaction (HCI) factors and the specialization-functionality relationship. It brings together all these factors, offering a general conception of EUD success factors. The proposed scale was applied in an empirical study on current EUD tools, which found that today's EUD tools have many shortcomings. In order to achieve an acceptable success rate among end users, we then designed a mashup tool architecture, called FAST-Wirecloud, which was built taking into account the proposed EUD success factor scale. The results of a new empirical study carried out using this tool have demonstrated that users are better able to successfully develop their composite applications and that FAST-Wirecloud has scored higher than all the other tools under study on all scales of measurement, and particularly on the scale proposed in this paper.},
author = {Lizcano, David and L{\'{o}}pez, Genoveva and Soriano, Javier and Lloret, Jaime},
doi = {10.1016/j.csi.2016.02.006},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lizcano et al. - 2016 - Implementation of end-user development success factors in mashup development environments.pdf:pdf},
issn = {09205489},
journal = {Computer Standards {\&} Interfaces},
keywords = {End-user development,HCI success factors,Mash up tool,Service front-ends,Service-oriented architectures,end-user development},
pages = {1--18},
publisher = {Elsevier B.V.},
title = {{Implementation of end-user development success factors in mashup development environments}},
url = {http://dx.doi.org/10.1016/j.csi.2016.02.006},
volume = {47},
year = {2016}
}
@article{Tzeremes2015,
author = {Tzeremes, Vasilios},
doi = {10.1109/PLEASE.2015.14},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tzeremes - 2015 - A Software Product Line Approach for End User Development of Smart Spaces.pdf:pdf},
isbn = {9781467370615},
pages = {5--8},
title = {{A Software Product Line Approach for End User Development of Smart Spaces}},
year = {2015}
}
@article{Little2006,
author = {Little, Greg and Miller, Robert C},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Little, Miller - 2006 - Translating Keyword Commands into Executable Code.pdf:pdf},
isbn = {1595933131},
keywords = {command languages,end-user programming,natural language processing,web automation},
pages = {135--144},
title = {{Translating Keyword Commands into Executable Code}},
year = {2006}
}
@article{Tam,
author = {Tam, R Chung-man and Maulsby, David and Puerta, Angel R},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tam, Maulsby, Puerta - Unknown - U-TEL A Tool for Eliciting User Task Models from Domain Experts.pdf:pdf},
isbn = {0897919556},
keywords = {knowledge elicitation,model-based user interface design,task models,user-centered design},
pages = {77--80},
title = {{U-TEL : A Tool for Eliciting User Task Models from Domain Experts}}
}
@article{Quirk2015,
author = {Quirk, Chris and Mooney, Raymond and Tx, Austin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Quirk, Mooney, Tx - 2015 - Language to Code Learning Semantic Parsers for If-This-Then-That Recipes.pdf:pdf},
pages = {878--888},
title = {{Language to Code : Learning Semantic Parsers for If-This-Then-That Recipes}},
year = {2015}
}
@article{Sales2017,
author = {Sales, Juliano Efson and Handschuh, Siegfried},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sales, Handschuh - 2017 - SemEval-2017 Task 11 End-User Development using Natural Language.pdf:pdf},
pages = {556--564},
title = {{SemEval-2017 Task 11 : End-User Development using Natural Language}},
year = {2017}
}
@article{Liu2005,
abstract = {illustrate the principles of Programmatic , we implemented  being entered; 2) an  log; 3  the basic character behaviors (excluding ) of the },
author = {Liu, Hugo},
doi = {10.1145/1056808.1056975},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Liu - 2005 - Programmatic semantics for natural language interfaces.pdf:pdf},
isbn = {1595930027},
journal = {Proceedings of the Acm Conference on Human Factors in Computing Systems, Chi 2005, April 5-7, 2005},
number = {2001},
pages = {1597 -- 1600},
title = {{Programmatic semantics for natural language interfaces}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.5634},
volume = {2005},
year = {2005}
}
@article{Massa2016,
author = {Massa, Daniele and Spano, Lucio},
doi = {10.3390/fi8020010},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Massa, Spano - 2016 - FaceMashup An End-User Development Tool for Social Network Data.pdf:pdf},
issn = {1999-5903},
journal = {Future Internet},
keywords = {direct manipulation,end user development,facebook,graph api,social network},
number = {2},
pages = {10},
title = {{FaceMashup: An End-User Development Tool for Social Network Data}},
url = {http://www.mdpi.com/1999-5903/8/2/10},
volume = {8},
year = {2016}
}
@article{Rode2004,
abstract = {We report an empirical study of nonprogrammers' mental models regarding particular concerns in Web application development such as input validation, database lookup, and overview-detail relationships. The goal of the study was to understand how nonprogrammers think about the data and logic underlying a Web application. In continuing work, we are using this understanding as a basis for the design of tools and development resources that are intuitive and easy to use. The current paper describes the empirical work that was done and discusses its implications for the design of end-user Web development tools that could be used to develop Web applications of intermediate complexity},
author = {Rode, Jochen and Rosson, Mary Beth and P{\'{e}}rez-Qui{\~{n}}ones, Manuel A.},
doi = {10.1109/VLHCC.2004.25},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rode, Rosson, P{\'{e}}rez-Qui{\~{n}}ones - 2004 - End-users' mental models of concepts critical to web application development.pdf:pdf},
isbn = {0780386965},
journal = {Proceedings - 2004 IEEE Symposium on Visual Languages and Human Centric Computing},
keywords = {End-user programming,Mental models,Web application development,Web engineering},
pages = {215--222},
title = {{End-users' mental models of concepts critical to web application development}},
year = {2004}
}
@article{McCutchen2016,
author = {McCutchen, Richard Matthew},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/McCutchen - 2016 - Object Spreadsheets an end-user development tool for web applications backed by entity-relationship data.pdf:pdf},
title = {{Object Spreadsheets: an end-user development tool for web applications backed by entity-relationship data}},
year = {2016}
}
@article{Aghaee2013,
abstract = {Live programming is a programming style in which the repetitive task of compiling and running the software being programmed is managed automatically. This style can be a helpful practice in End-User Development (EUD) where the nonprofessional end-users are to be supported through techniques and tools that empower them to create or modify software artifacts. Mashups a form of lightweight Web applications composing reusable content and functionalities available on the Web are a popular target for EUD activities on the Web. EUD for mashups is enabled by intuitive composition environments, called mashup tools. In this paper, we introduce live mashup tools, a new class of mashup tools based on the live programming style. We give a comprehensive definition and classification of live mashup tools, giving examples of how well existing tools fit in this category and discuss open research challenges and opportunities.},
author = {Aghaee, Saeed and Pautasso, Cesare},
doi = {10.1109/LIVE.2013.6617338},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aghaee, Pautasso - 2013 - Live mashup tools Challenges and opportunities.pdf:pdf},
isbn = {9781467362658},
journal = {2013 1st International Workshop on Live Programming, LIVE 2013 - Proceedings},
keywords = {End-user development,Live programming,Liveness,Mashup tools,Web mashups},
pages = {1--4},
title = {{Live mashup tools: Challenges and opportunities}},
year = {2013}
}
@article{Silva2009,
author = {Silva, Buddhima De and Ginige, Athula and Bajaj, Simi and Ekanayake, Ashini},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Silva et al. - 2009 - A Tool to Support End-User Development of Web Applications Based on a Use Case Model.pdf:pdf},
keywords = {requirement specification,use case model,web application},
pages = {527 -- 530},
title = {{A Tool to Support End-User Development of Web Applications Based on a Use Case Model}},
year = {2009}
}
@article{Paterno2013,
abstract = {The purpose of this paper is to introduce the motivations behind end user development, discuss its basic concepts and roots, and review the current state of art. Various approaches are discussed and classified in terms of their main features and the technologies and platforms for which they have been developed. Lastly, the paper provides an indication of interesting possibilities for further evolution.},
author = {Patern{\`{o}}, Fabio},
doi = {10.1155/2013/532659},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Patern{\`{o}} - 2013 - End User Development Survey of an Emerging Field for Empowering People.pdf:pdf},
issn = {2090-7680},
journal = {ISRN Software Engineering},
keywords = {eud},
pages = {1--11},
title = {{End User Development: Survey of an Emerging Field for Empowering People}},
url = {http://downloads.hindawi.com/journals/isrn.software.engineering/2013/532659.pdf{\%}5Cnhttp://www.hindawi.com/journals/isrn/2013/532659/{\%}5Cnpapers3://publication/doi/10.1155/2013/532659{\%}5Cnhttp://www.hindawi.com/journals/isrn.software.engineering/2013/532659/},
volume = {2013},
year = {2013}
}
@article{Comission2004,
author = {Comission, European},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Comission - 2004 - Supporting effective implementation of EC External Assistance European Commission.pdf:pdf},
keywords = {management},
pages = {149},
title = {{Supporting effective implementation of EC External Assistance European Commission}},
url = {http://ec.europa.eu/europeaid/multimedia/publications/publications/manuals-tools/t101{\_}en.htm},
volume = {Volume 1 -},
year = {2004}
}
@article{Vaishnavi2004,
abstract = {This page is dedicated to design science research in Information Systems (IS). Design science research is yet another "lens" or set of synthetic and analytical techniques and perspectives (complementing the Positivist and Interpretive perspectives) for performing research in IS. Design science research involves the creation of new knowledge through design of novel or innovative artifacts (things or processes that have or can have material existence) and analysis of the use and/or performance of such artifacts along with reflection and abstraction—to improve and understand the behavior of aspects of Information Systems. Such artifacts include—but certainly are not limited to—algorithms (e.g. for information retrieval), human/computer interfaces, and system design methodologies or languages. Design science researchers can be found in many disciplines and fields, notably Engineering and Computer Science; they use a variety of approaches, methods and techniques. In Information Systems, following a number of years of a general shift in IS research away from technological to managerial and organizational issues, an increasing number of observers are calling for a return to an exploration of the "IT" that underlies all IS research (Orlikowski and Iacono, 2001) thus underlining the need for IS design science research.},
author = {Vaishnavi, Vijay and Kuechler, Bill},
doi = {10.1007/978-1-4419-5653-8},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Vaishnavi, Kuechler - 2004 - Design Science Research in Information Systems Overview of Design Science Research.pdf:pdf},
isbn = {0849337968},
issn = {02767783},
journal = {Ais},
pages = {45},
pmid = {12581935},
title = {{Design Science Research in Information Systems Overview of Design Science Research}},
url = {http://www.desrist.org/design-research-in-information-systems/},
year = {2004}
}
@book{Daniel2014a,
abstract = {Mashups have emerged as an innovative software trend that re-interprets existing Web building blocks and leverages the composition of individual components in novel, value-adding ways. Additional appeal also derives from their potential to turn non-programmers into developers. Daniel and Matera have written the first comprehensive reference work for mashups. They systematically cover the main concepts and techniques underlying mashup design and development, the synergies among the models involved at different levels of abstraction and the way models materialize into composition paradigms and architectures of corresponding development tools. The book deliberately takes a balanced approach, combining a scientific perspective on the topic with an in-depth view on relevant technologies. To this end, the first part of the book introduces the theoretical and technological foundations for designing and developing mashups, as well as for designing tools that can aid mashup development. The second part then focuses more specifically on various aspects of mashups. It discusses a set of core component technologies, core approaches and architectural patterns, with a particular emphasis on tool-aided mashup development exploiting model-driven architectures. Development processes for mashups are also discussed and special attention is paid to composition paradigms for the end-user development of mashups and quality issues. Overall, the book is of interest to a wide range of readers. Students, lecturers, and researchers will find a comprehensive overview of core concepts and technological foundations for mashup implementation and composition. Even without low-level coding details, practitioners like software architects will find guidance on key implementation concepts, architectural patterns and development tools and approaches. A related website provides additional teaching material which can be used either as part of a course or for self study.},
author = {Daniel, Florian and Matera, Maristella},
booktitle = {Mashups: Concepts, Models and Architectures},
doi = {10.1007/978-3-642-55049-2},
isbn = {9783642550492},
issn = {3642550487},
keywords = {Application integration,Data integration,Mashups,Model-driven software development RESTful services,SOA,Service-oriented architectures,Software architectures,Web applications,Web services},
pages = {1--319},
title = {{Mashups: Concepts, models and architectures}},
url = {https://books.google.de/books?id=SFpDBAAAQBAJ{\&}source=gbs{\_}navlinks{\_}s},
year = {2014}
}
@book{Bourque2014,
abstract = {SWEBOK V3.0 is the most recent completely revised and updated version of the internationally respected Guide to the Software Engineering Body of Knowledge. Newly imagined as a living, changing document, and thoroughly rewritten, SWEBOK V3.0 has been developed and created by leading authorities, reviewed by professionals, and made available for public review and comment, continuing its 20-year reputation as the most authoritative, fundamental, and trusted definition of the software engineering profession. SWEBOK V3.0 is comprised of 15 Knowledge Areas, plus a new Appendix on Standards. SWEBOK V3.0 is now specifically designed to be constantly reviewed and updated as technology and the engineering profession changes over time, remaining consistently relevant. Be sure to register to receive notifications when the SWEBOK Guide is revised.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.1833v2},
author = {Bourque, Pierre and Fairley, Richard E.},
booktitle = {IEEE Computer Society},
doi = {10.1234/12345678},
eprint = {arXiv:1210.1833v2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bourque, Fairley - 2014 - Guide to the Software Engineering - Body of Knowledge.pdf:pdf},
isbn = {0-7695-2330-7},
issn = {07407459},
pages = {346},
pmid = {13861254},
title = {{Guide to the Software Engineering - Body of Knowledge.}},
url = {www.swebok.org},
year = {2014}
}
@article{Vaishnavi2004a,
abstract = {This page is dedicated to design science research in Information Systems (IS). Design science research is yet another "lens" or set of synthetic and analytical techniques and perspectives (complementing the Positivist and Interpretive perspectives) for performing research in IS. Design science research involves the creation of new knowledge through design of novel or innovative artifacts (things or processes that have or can have material existence) and analysis of the use and/or performance of such artifacts along with reflection and abstraction—to improve and understand the behavior of aspects of Information Systems. Such artifacts include—but certainly are not limited to—algorithms (e.g. for information retrieval), human/computer interfaces, and system design methodologies or languages. Design science researchers can be found in many disciplines and fields, notably Engineering and Computer Science; they use a variety of approaches, methods and techniques. In Information Systems, following a number of years of a general shift in IS research away from technological to managerial and organizational issues, an increasing number of observers are calling for a return to an exploration of the "IT" that underlies all IS research (Orlikowski and Iacono, 2001) thus underlining the need for IS design science research.},
annote = {NULL},
author = {Vaishnavi, Vijay and Kuechler, Bill},
doi = {10.1007/978-1-4419-5653-8},
isbn = {0849337968},
issn = {02767783},
journal = {Ais},
pages = {45},
pmid = {12581935},
title = {{Design Science Research in Information Systems Overview of Design Science Research}},
year = {2004}
}
@article{Comission2004a,
annote = {NULL},
author = {Comission, European},
pages = {149},
title = {{Supporting effective implementation of EC External Assistance European Commission}},
volume = {Volume 1 -},
year = {2004}
}
@book{Hevner2010,
author = {Hevner, Alan and Chatterjee, Samir},
isbn = {978-4419-5652-1},
publisher = {Springer},
title = {{Design Research in Information Systems: Theory and Practice - Alan Hevner, Samir Chatterjee - Google Books}},
url = {https://books.google.de/books?hl=en{\&}lr={\&}id=89w-scN7{\_}8MC{\&}oi=fnd{\&}pg=PR6{\&}dq=info:hfJWNut5my0J:scholar.google.com{\&}ots=-s5SnrEFw-{\&}sig=FUxnRn5o-E95BOa3blW5-FssOtU{\&}redir{\_}esc=y{\#}v=onepage{\&}q{\&}f=false},
year = {2010}
}
@article{Coronado2016,
abstract = {A simple model of mashup technology for combining services and connected devices is now becoming popular. This model is frequently known as {\{}{\&}{\}}{\#}039;task automation{\{}{\&}{\}}{\#}039; based on ECA (Event-Condition-Action) rules. The most popular online services that follow this approach are Ifttt and Zapier. In addition, this model is being followed by several mobile frameworks, such as on{\{}x{\}}, Automateit or Tasker, to automate how the phone deals with the incoming Internet events and phone sensors. This article outlines the features and components of task automation services, and proposes a generic architecture that supports the current challenges. Finally, as task automation services are a growing trend, this article surveys their characteristics, comparing existing platforms and discussing their evolution and future tendencies.},
author = {Coronado, Miguel and Iglesias, Carlos A.},
doi = {10.1109/MIC.2015.73},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Coronado, Iglesias - 2016 - Task automation services Automation for the masses.pdf:pdf},
issn = {10897801},
journal = {IEEE Internet Computing},
number = {1},
pages = {52--58},
title = {{Task automation services: Automation for the masses}},
volume = {20},
year = {2016}
}
@phdthesis{Soi2013,
author = {Soi, Stefano},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Soi - 2013 - DISI - University of Trento Domain Specific Mashup Platforms as a Service(2).pdf:pdf},
number = {March},
school = {University of Trento Domain},
title = {{DISI - University of Trento Domain Specific Mashup Platforms as a Service}},
year = {2013}
}
@article{Sutcliffe2005,
author = {Sutcliffe, Alistair},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sutcliffe - 2005 - Evaluating the Costs and Benefits of End-User Development.pdf:pdf},
isbn = {1595931317},
keywords = {additional design time,cost benefit analysis,effort to the end,end user development,eud essentially out-sources development,hence one element of,the cost is the,user},
number = {Weuse I},
pages = {1--4},
title = {{Evaluating the Costs and Benefits of End-User Development}},
year = {2005}
}
@incollection{Rode2006,
abstract = {This chapter investigates entry barriers and approaches for facilitating end-user web application development with the particular focus on shaping web programming technology and tools according to end-users' expectations and natural mental models. Our underlying assumption and motivation is that given the right tools and techniques even nonprogrammers may become successfulweb application developers. The main target audience for this research are “casual”webmasters without programming experience—a group likely to be interested in building web applications. As an important subset of web applications we focus on supporting the development of basic data collection, storage and retrieval applications such as online registrations forms, staff databases, or report tools. Firstwe analyze the factors contributing to the complexity ofweb application development through surveys and interviews of experienced programmers; then we explore the “natural mental models” of potential end-user web developers, and finally discuss our particular design solutions for lowering entry barriers, as embodied by a proof-of-concept development tool, called Click. Furthermore, we introduce and evaluate the concept of “Design-at-Runtime”—a new technique for facilitating and accelerating the development-test cycle when building web-based applications. Key words. end user development, web applications},
address = {Dordrecht},
annote = {NULL},
author = {Rode, Jochen and Rosson, Mb Mary Beth},
booktitle = {End User Development},
doi = {10.1007/1-4020-5386-X},
editor = {Lieberman, Henry and Patern{\`{o}}, Fabio and Wulf, Volker},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rode, Rosson - 2006 - End User Development.pdf:pdf},
isbn = {978-1-4020-4220-1},
pages = {161--182},
publisher = {Springer Netherlands},
series = {Human-Computer Interaction Series},
title = {{End User Development}},
url = {http://www.springerlink.com/index/L6T10H554J4X4213.pdf{\%}5Cnhttp://www.springerlink.com/index/l6t10h554j4x4213.pdf http://link.springer.com/10.1007/1-4020-5386-X},
volume = {9},
year = {2006}
}
@article{Haines2010,
abstract = {End-user development (EUD), the practice of users creating, modifying, or extending programs for personal use, is a valuable but often challenging task for nonprogrammers. From the beginning, EUD systems have shown that recommendations can improve the user experience. However, these usability improvements are limited by a reliance on handcrafted rules and heuristics to generate reasonable and useful suggestions. When the number of possible recommendations is large or the available context is too limited for traditional reasoning techniques, recommender technologies present a promising solution. In this paper, we provide an overview of the state of the art in end-user development, focusing on the different kinds of recommendations made to users. We identify four classes of suggestion that could most directly benefit from existing recommendation techniques. Along the way we explore straightforward applications of recommender algorithms as well as a few difficult but high-value recommendation problems in EUD. We discuss the ways that EUD systems have been evaluated in the past and suggest the modifications necessary to evaluate recommenders within the EUD context. We highlight EUD research as one area that can facilitate the transition of recommender system evaluation from algorithmic performance evaluation to a more user-centered approach. We conclude by restating our findings as a new set of research challenges for the recommender systems community.},
author = {Haines, Will and Gervasio, Melinda and Spaulding, Aaron and Peintner, Bart},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Haines et al. - 2010 - Recommendations for end-user development.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {End-user development,Recommender systems,motivation},
mendeley-tags = {motivation},
pages = {42--49},
title = {{Recommendations for end-user development}},
volume = {612},
year = {2010}
}
@inproceedings{Fischer2009,
abstract = {Mashups allow users to bring together data and services from various web applications in order to create a new integrated tool that serves their needs. In the last few years, a variety of mashups frameworks has been proposed that promise to simplify the mashup creation process so that every user is able to create mashups. In this paper, we give an overview about these approaches and identify their limitations. The main insight is that the average user will not possess the necessary skills to create mashups that meet his needs with these tools. We therefore propose that a tool is needed that allows for the automatic ad-hoc generation of mashups.},
annote = {NULL},
author = {Fischer, Thomas and Bakalov, Fedor and Nauerz, Andreas},
booktitle = {Proceedings of the Fifth Conference Professional Knowledge Management: Experiences and Visions},
doi = {10.1.1.403.1925},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fischer, Bakalov, Nauerz - Unknown - An Overview of Current Approaches to Mashup Generation.pdf:pdf},
isbn = {9783885792390},
issn = {16175468},
keywords = {Generation,Mashup,Overview},
pages = {254--259},
title = {{An overview of current approaches to mashup generation}},
url = {http://subs.emis.de/LNI/Proceedings/Proceedings145/gi-proc-145-023.pdf},
year = {2009}
}
@article{Aghaee2014,
author = {Aghaee, Saeed and Pautasso, Cesare},
doi = {10.1016/j.jvlc.2013.12.004},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aghaee, Pautasso - 2014 - Journal of Visual Languages and Computing End-User Development of Mashups with NaturalMash {\$}.pdf:pdf},
issn = {1045-926X},
journal = {Journal of Visual Language and Computing},
keywords = {End-User Development,Mashup tools,Mashups,Natural language programming,Programming by Demonstration,WYSIWYG},
number = {4},
pages = {414--432},
publisher = {Elsevier},
title = {{End-User Development of Mashups with NaturalMash}},
url = {http://dx.doi.org/10.1016/j.jvlc.2013.12.004},
volume = {25},
year = {2014}
}
@article{Picozzi2013,
author = {Picozzi, Matteo},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Picozzi - 2013 - D OCTORAL P ROGRAMME I N C OMPUTER S CIENCE AND E NGINEERING E ND - USER D EVELOPMENT OF M ASHUPS M ODELS , C OMPOSITI.pdf:pdf},
title = {{D OCTORAL P ROGRAMME I N C OMPUTER S CIENCE AND E NGINEERING E ND - USER D EVELOPMENT OF M ASHUPS : M ODELS , C OMPOSITION P ARADIGMS AND}},
year = {2013}
}
@phdthesis{Imran2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.7520v3},
author = {Imran, Muhammad},
booktitle = {University of Trento},
eprint = {arXiv:1312.7520v3},
file = {:Users/baharehzarei/Downloads/Imran{\_}phd-thesis{\_}old{\_}title.pdf:pdf},
number = {March},
school = {University of Trento},
title = {{An effective end-user development approach through domain-specific mashups for Research Impact Evaluation}},
year = {2013}
}
@article{Lenzerini2002,
abstract = {Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integra- tion systems is important in current real world applications, and is characterized by a number of issues that are interest- ing from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the the- oretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and rea- soning on queries.},
author = {Lenzerini, Maurizio},
doi = {10.1145/543613.543644},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lenzerini - 2002 - Data Integration A Theoretical Perspective.pdf:pdf},
isbn = {1581135076},
issn = {01679260},
journal = {Proceedings of the Twenty-first ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
keywords = {Data Integration,Tutorial,gav,incosistency,lav,multi source,query,theoretical},
pages = {233--246},
title = {{Data Integration : A Theoretical Perspective}},
url = {http://doi.acm.org/10.1145/543613.543644,{\%}5Cnhttp://www.acm.org/sigs/sigmod/pods/proc02/papers/233-Lenzerini.pdf},
year = {2002}
}
@article{Gray2007,
abstract = {There is an increasing amount of information being made available as data streams, e.g. stock tickers, data from sensor networks, smart homes, monitoring data, etc. In many cases, this data is generated by distributed sources under the control of many different organisations. Users would like to seamlessly query such data without prior knowledge of where it is located or how it is published. This is similar to the problem of integrating data residing in multiple heterogeneous stored data sources. However, the techniques developed for stored data are not applicable due to the continuous and long-lived nature of queries over data streams. This thesis proposes an architecture for a stream integration system. A key feature of the architecture is a republisher component that collects together distributed streams and makes the merged stream available for querying. A formal model for the system has been developed and is used to generate plans for executing continuous queries which exploit the redundancy introduced by the republishers. Additionally, due to the long-lived nature of continuous queries, mechanisms for maintaining the plans whenever there is a change in the set of data sources have been developed. A prototype of the system has been implemented and performance measures made. The work of this thesis has been motivated by the problem of retrieving monitoring information about Grid resources. However, the techniques developed are general and can be applied wherever there is a need to publish and query distributed data involving data streams.},
author = {Gray, A J G},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gray - 2007 - Integrating Distributed Data Streams.pdf:pdf},
journal = {October},
number = {1},
pages = {1--6},
title = {{Integrating Distributed Data Streams}},
url = {http://www.cs.man.ac.uk/{~}graya/Publications/thesis-final{\_}web-copy.pdf},
year = {2007}
}
@article{Srivastava2006,
abstract = {Efficient query processing in any data management system typically relies on: (a) A profiling component that gathers statistics used to evaluate possible query execution plans, and (b) A planning component that picks the plan with the best predicted performance. For query processing in a range of new data management scenarios, e.g., query processing over data streams, and web services, traditional profiling and planning techniques developed for conventional relational database management systems are inadequate. This thesis develops several novel profiling and planning techniques to enable efficient query processing in these new scenarios. When data is arriving rapidly in the formof streams, andmany registered queries must be continuously executed over this data, system resources such as memory and processing power may be stretched to their limit. First, for a class of computation-intensive queries, we describe how system throughput can be increased by exploiting sharing of computation among the registered queries. Then, for a class of memory-intensive queries, we consider the case when system memory is insufficient for obtaining exact answers, and give techniques for maximizing result accuracy under the given memory constraints. We then consider a distributed setting such as that of a sensor network, and give techniques for deciding the placement of query operators at network nodes in order to minimize systemwide consumption of resources. We then consider the scenario of web services, which have been emerging as a popular standard for sharing data and functionality among loosely-coupled systems. For queries involving multiple web services, we give algorithms for finding the optimal execution plan. Finally, we turn to the profiling component, and describe new techniques for gathering statistics by not looking at the data but only at the query results. Such a technique is required when data access for collecting statistics is infeasible, as for web services, but can also be useful in traditional databases.},
author = {Srivastava, Utkarsh},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava - 2006 - Efficient Query Processing for Modern Data Management.pdf:pdf},
keywords = {data streams, query optimization, query processing},
number = {September},
title = {{Efficient Query Processing for Modern Data Management}},
url = {http://ilpubs.stanford.edu:8090/786/},
year = {2006}
}
@article{Udupi2016,
author = {Udupi, Prakash Kumar and Malali, Puttaswamy and Noronha, Herald},
doi = {10.1109/ICBDSC.2016.7460379},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Udupi, Malali, Noronha - 2016 - Big data integration for transition from e-learning to smart learning framework.pdf:pdf},
isbn = {9781509013654},
journal = {2016 3rd MEC International Conference on Big Data and Smart City, ICBDSC 2016},
keywords = {Bigdata,e-learning,smart learning,smart systems},
pages = {268--271},
title = {{Big data integration for transition from e-learning to smart learning framework}},
year = {2016}
}
@article{Birgersson2016,
author = {Birgersson, Marcus and Hansson, Gustav},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Birgersson, Hansson - 2016 - Data Integration using Machine Learning ∗.pdf:pdf},
isbn = {9781467399333},
pages = {313--322},
title = {{Data Integration using Machine Learning ∗}},
year = {2016}
}
@article{Qiu,
author = {Qiu, Decheng},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Qiu - Unknown - Design and Application of Data Integration Platform Based on Web Services and XML.pdf:pdf},
isbn = {9781509019977},
keywords = {- data exchange,1,data integration,heterogeneous environment,language and cross-platform,soap,uddi,web services,wsdl,xml},
title = {{Design and Application of Data Integration Platform Based on Web Services and XML}}
}
@article{Aggoune2016,
author = {Aggoune, Aicha},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aggoune - 2016 - Big Data Integration A Semantic Mediation Architecture Using Summary.pdf:pdf},
isbn = {9781467385268},
keywords = {architecture,big data,matching,ontology,semantic integration,summarization},
pages = {21--25},
title = {{Big Data Integration : A Semantic Mediation Architecture Using Summary}},
year = {2016}
}
@article{Searls2005,
abstract = {The effective integration of data and knowledge from many disparate sources will be crucial to future drug discovery. Data integration is a key element of conducting scientific investigations with modern platform technologies, managing increasingly complex discovery portfolios and processes, and fully realizing economies of scale in large enterprises. However, viewing data integration as simply an 'IT problem' underestimates the novel and serious scientific and management challenges it embodies - challenges that could require significant methodological and even cultural changes in our approach to data.},
author = {Searls, David B},
doi = {10.1038/nrd1608},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Searls - 2005 - Data integration challenges for drug discovery.pdf:pdf},
isbn = {1474-1776 LA  - eng PT  - Journal Article},
issn = {1474-1776},
journal = {Nature reviews. Drug discovery},
number = {1},
pages = {45--58},
pmid = {15688072},
title = {{Data integration: challenges for drug discovery.}},
volume = {4},
year = {2005}
}
@article{Castanedo2013,
abstract = {The integration of data and knowledge from several sources is known as data fusion. This paper summarizes the state of the data fusion field and describes the most relevant studies. We first enumerate and explain different classification schemes for data fusion. Then, the most common algorithms are reviewed. These methods and algorithms are presented using three different categories: (i) data association, (ii) state estimation, and (iii) decision fusion.},
author = {Castanedo, Federico},
doi = {10.1155/2013/704504},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Castanedo - 2013 - A review of data fusion techniques.pdf:pdf},
isbn = {1537-744x},
issn = {1537744X},
journal = {The Scientific World Journal},
pmid = {24288502},
title = {{A review of data fusion techniques}},
volume = {2013},
year = {2013}
}
@article{OracleDataIntegrator2015,
abstract = {Key To Big Data Success ETL vs ELT To Extract, Load and Transform (ELT) data using the big data platform's capabilities is more efficient than to Extract, Transform and Load (ETL) data because it results in  Minimal middleware  Fast performance with set based processing, and  Reduced n/w traffic 5. Ignoring the data processing power of Hadoop/NoSQL when handling complex workloads.},
author = {{Oracle Data Integrator}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Oracle Data Integrator - 2015 - The Five Most Common Big Data Integration Mistakes To Avoid.pdf:pdf},
number = {April},
title = {{The Five Most Common Big Data Integration Mistakes To Avoid}},
year = {2015}
}
@article{Barber2014,
author = {Barber, Suzanne and Cook, William R},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Barber, Cook - 2014 - Automatic Data Integration with Generalized Mapping Definitions.pdf:pdf},
title = {{Automatic Data Integration with Generalized Mapping Definitions}},
year = {2014}
}
@article{Leadership2013,
author = {Leadership, Thought and Paper, White},
number = {October},
title = {{Who ' s afraid of the big ( data ) bad wolf ?}},
year = {2013}
}
@article{Abbes2016,
author = {Abbes, Hanen and Gargouri, Faiez},
doi = {10.1016/j.procs.2016.08.099},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Abbes, Gargouri - 2016 - Big Data Integration a MongoDB Database and Modular Ontologies based Approach.pdf:pdf},
issn = {1877-0509},
journal = {Procedia - Procedia Computer Science},
keywords = {big data,modular ontologies,mongodb,nosql,ontology learning},
number = {September},
pages = {446--455},
publisher = {The Author(s)},
title = {{Big Data Integration : a MongoDB Database and Modular Ontologies based Approach}},
url = {http://dx.doi.org/10.1016/j.procs.2016.08.099},
volume = {96},
year = {2016}
}
@article{Papazoglou2003,
author = {Papazoglou, M P and Georgakopoulos, D},
doi = {10.1145/944217.944233},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Papazoglou, Georgakopoulos - 2003 - Introduction Service-oriented Computing.pdf:pdf},
isbn = {9780262072960},
issn = {0001-0782},
journal = {Commun. ACM},
number = {10},
pages = {24--28},
title = {{Introduction: Service-oriented Computing}},
url = {http://doi.acm.org/10.1145/944217.944233},
volume = {46},
year = {2003}
}
@article{FujunZhu2004,
author = {{Fujun Zhu}, Fujun and Turner, M. and Kotsiopoulos, I. and Bennett, K. and Russell, M. and Budgena, D. and Breretona, P. and Keane, J. and Layzell, P. and Rigby, M. and {Jie Xu}, Jie},
doi = {10.1109/ICWS.2004.1314747},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fujun Zhu et al. - 2004 - Dynamic data integration using Web services.pdf:pdf},
isbn = {0-7695-2167-3},
journal = {Proceedings. IEEE International Conference on Web Services, 2004.},
keywords = {Application software,Communication effectiveness,Computer science,Data warehouses,Internet,Large scale integration,Prototypes,Service oriented architecture,Software maintenance,Software systems,UK,Web services,architecture requirements,broker architectural layer,data handling,data sources,demonstrator system,distributed databases,dynamic data integration,health care,health services data integration,large-scale data integration,medical information systems},
pages = {262--269},
title = {{Dynamic data integration using Web services}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1314747},
year = {2004}
}
@article{Sala,
author = {Sala, Antonio},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sala - Unknown - Data and Service Integration Architectures and Applications to Real Domains.pdf:pdf},
title = {{Data and Service Integration : Architectures and Applications to Real Domains}}
}
@article{Bhagattjee2014,
author = {Bhagattjee, Benoy},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bhagattjee - 2014 - Emergence and Taxonomy of Big Data as a Service.pdf:pdf},
number = {May},
pages = {1--84},
title = {{Emergence and Taxonomy of Big Data as a Service}},
year = {2014}
}
@book{Fensel1989,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fensel, Dieter},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fensel - 1989 - Semantic Web Services.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pages = {160},
pmid = {25246403},
title = {{Semantic Web Services}},
volume = {53},
year = {1989}
}
@article{McIlraith2001,
abstract = {The authors propose the markup of Web services in the DAML family of Semantic Web markup languages. This markup enables a wide variety of agent technologies for automated Web service discovery, execution, composition and interoperation. The authors present one such technology for automated Web service composition.},
author = {McIlraith, Sheila A. and San, Tran Cao and Zeng, Honglei},
doi = {10.1109/5254.920599},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/McIlraith, San, Zeng - 2001 - Semantic Web services.pdf:pdf},
isbn = {9783642191923},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
keywords = {Application software,Automation,DAML,Internet,Markup languages,Microstrip,Pervasive computing,Semantic Web,Semantic Web markup languages,Semantic Web services,Temperature sensors,Web pages,Web services,automated Web service composition,automated Web service discovery,information resources,page description languages,software agents},
pages = {46--53},
title = {{Semantic Web services}},
volume = {16},
year = {2001}
}
@article{Zeshan2011,
abstract = {Service composition is gaining popularity because the composite service presents the features that an individual service cannot present. There are multiple web services available over the web for different tasks. Semantic web is the advance form of the current web, where all the contents have well defined meanings, due to this nature; semantic web enables the automated processing of web contents by machines. At run time, the composition of these services based on the requester's functional and non-functional requirements is a difficult task due to the heterogeneous nature of results of the services. This paper introduced some requirements that when fulfilled, a successful composition process can be achieved. In order to find the best approach, various composition approaches on these requirements were evaluated. Suggestions were provided on what approach can be used in which scenario in order to gain the best results.},
author = {Zeshan, Furkh and Mohamad, Radziah},
doi = {10.5121/ijcses.2014.5402},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zeshan, Mohamad - 2011 - Semantic Web Service Composition Approaches Overview and Limitations.pdf:pdf},
issn = {09763252},
journal = {International Journal on New Computer Architectures and Their Applications (IJNCAA)},
keywords = {Composition Approaches,Semantic Web Services,Web Services},
number = {3},
pages = {640--651},
title = {{Semantic Web Service Composition Approaches : Overview and Limitations}},
volume = {1},
year = {2011}
}
@article{Khalaf2003,
abstract = {The Web services framework is enabling applications from different providers to be offered as services that can be used and composed in a loosely-coupled manner. Subsequently, the aggregation of services to form composite applications and maximize reuse is key. While choreography has received the most attention, services often need to be aggregated in a much less constrained manner. As a number of different mechanisms emerge to create these aggregations, their relation to each other and to prior work is useful when deciding how to create an aggregation, as well as in extending the models themselves and proposing new ones. In this paper, we discuss Web services aggregation by presenting a first-step classification based on the approaches taken by the different proposed aggregation techniques. Finally, a number of models are presented that are created from combinations of the above.},
author = {Khalaf, Rania and Leymann, Frank},
doi = {10.1007/978-3-540-39406-8_1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Khalaf, Leymann - 2003 - On Web Services Aggregation.pdf:pdf},
isbn = {3-540-20052-5},
issn = {03029743},
journal = {Technologies for EServices},
keywords = {aggregation,business process modeling,composition},
pages = {1--13},
title = {{On Web Services Aggregation}},
year = {2003}
}
@article{Cheptsov,
author = {Cheptsov, Alexey and Tenschert, Axel and Schmidt, Paul and Glimm, Birte},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cheptsov et al. - Unknown - Introducing a New Scalable Data-as-a-Service Cloud Platform for Enriching Traditional Text Mining Techniques.pdf:pdf},
pages = {62--74},
title = {{Introducing a New Scalable Data-as-a-Service Cloud Platform for Enriching Traditional Text Mining Techniques by Integrating Ontology Modelling and}}
}
@article{Lemos2015,
abstract = {Web services are a consolidated reality of the modern Web with tremendous, increasing impact on everyday computing tasks. They turned the Web into the largest, most accepted, and most vivid distributed computing platform ever. Yet, the use and integration of Web services into composite services or applications, which is a highly sensible and conceptually non-trivial task, is still not unleashing its full magnitude of power. A consolidated analysis framework that advances the fundamental understanding of Web service composition building blocks in terms of concepts, models, languages, productivity support techniques, and tools is required. This framework is necessary to enable effective exploration, understanding, assessing, comparing, and selecting service composition models, languages, techniques, platforms, and tools. This article establishes such a framework and reviews the state of the art in service composition from an unprecedented, holistic perspective. {\textcopyright} 2015 ACM.},
author = {Lemos, Angel Lagares and Daniel, Florian and Benatallah, Boualem},
doi = {10.1145/2831270},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lemos, Daniel, Benatallah - 2015 - Web Service Composition A Survey of Techniques and Tools.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {3},
pages = {1--41},
title = {{Web Service Composition: A Survey of Techniques and Tools}},
url = {http://dl.acm.org/citation.cfm?doid=2856149.2831270},
volume = {48},
year = {2015}
}
@article{Sheng2014,
abstract = {Service-oriented computing (SOC) represents a paradigm for building distributed computing applications over the Internet. In the past decade, Web services composition has been an active area of research and development endeavors for application integration and interoperation. Although Web services composition has been heavily investigated, several issues related to dependability, ubiquity, personalization, among others, still need to be addressed, especially giving the recent rise of several new computing paradigms such as Cloud computing, social computing, and Web of Things. This article overviews the life cycle of Web services composition and surveys the main standards, research prototypes, and platforms. These standards, research prototypes, and platforms are assessed using a set of assessment criteria identified in the article. The paper also outlines several research opportunities and challenges for Web services composition. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sheng, Quan Z. and Qiao, Xiaoqiang and Vasilakos, Athanasios V. and Szabo, Claudia and Bourne, Scott and Xu, Xiaofei},
doi = {10.1016/j.ins.2014.04.054},
eprint = {arXiv:1011.1669v3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sheng et al. - 2014 - Web services composition A decade's overview.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Composition life cycle,Composition requirement,Service composition challenge,Web services composition},
pages = {218--238},
pmid = {15003161},
title = {{Web services composition: A decade's overview}},
volume = {280},
year = {2014}
}
@article{Comai2012,
abstract = {This article presents a model-driven approach for the design of the layout in a complex Web application, where large amounts of data are accessed. The aim of this work is to reduce, as much as possible, repetitive tasks and to factor out common aspects into different kinds of rules that can be reused across different applications. In particular, exploiting the conceptual elements of the typical models used for the design of a Web application, it defines presentation and layout rules at different levels of abstraction and granularity. A procedure for the automatic layout of the content of a page is proposed and evaluated, and the layout of advanced Web applications is discussed.},
author = {Comai, Sara and Mazza, Davide},
doi = {10.1145/2344416.2344417},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Comai, Mazza - 2012 - A model-driven methodology to the content layout problem in web applications.pdf:pdf},
isbn = {1559-1131},
issn = {1559-1131},
journal = {{\{}ACM{\}} Transactions on the Web},
keywords = {Automatic content,Automatic contents layout,Automatic layout,Conceptual elements,Design,Graphical visualization,Large amounts of data,Layout problems,Levels of abstraction,Model driven approach,Model-driven methodology,Repetitive task,Typical model,Web applications design,World Wide Web,graphical visualization and rendering,{\{}WEB{\}} application},
number = {3},
pages = {10:1--10:38},
title = {{A model-driven methodology to the content layout problem in web applications}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84870197804{\&}partnerID=40{\&}md5=a917756aa9d180abf8f00c5027502853{\%}5Cnhttp://doi.acm.org/10.1145/2344416.2344417},
volume = {6},
year = {2012}
}
@article{Chandra2016,
abstract = {a b s t r a c t An attempt has been made to improve the performance of Deep Learning with Multilayer Perceptron (MLP). Tuning the learning rate or finding an optimum learning rate in MLP is a major challenge. De-pending on the value of the learning rate, classification accuracy can vary drastically. This issue has been taken as a challenge in this paper. In this paper, a new approach has been proposed to combine adaptive learning rate in conjunction with the concept of Laplacian score for varying the weights. Learning rate is taken as a function of parameter which itself is updated on the basis of error gradient by forming mini-batches. Laplacian score of the neuron is further used for updating the incoming weights. This removes the bottleneck involved in finding the optimum value for the learning rate in Deep Learning by using MLP. It is observed on benchmark datasets that this approach leads to increase in classification accuracy as compared to the existing benchmark levels achieved by the well known methods of deep learning.},
author = {Chandra, B and Sharma, Rajesh K},
doi = {10.1016/j.eswa.2016.05.022},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chandra, Sharma - 2016 - Deep learning with adaptive learning rate using laplacian score(2).pdf:pdf},
journal = {Expert Systems With Applications},
keywords = {Adaptive learning rate,Deep learning,Gradient descent,Laplacian score},
pages = {1--7},
title = {{Deep learning with adaptive learning rate using laplacian score}},
volume = {63},
year = {2016}
}
@article{Deng,
abstract = {Deep Learning: Methods and Applications provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been benefitting from recent research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning. Deep Learning: Methods and Applications is a timely and important book for researchers and students with an interest in deep learning methodology and its applications in signal and information processing. " This book provides an overview of a sweeping range of up-to-date deep learning methodologies and their application to a variety of signal and information processing tasks, including not only automatic speech recognition (ASR), but also computer vision, language modeling, text processing, multimodal learning, and information retrieval. This is the first and the most valuable book for " deep and wide learning " of deep learning, not to be missed by anyone who wants to know the breathtaking impact of deep learning on many facets of information processing, especially ASR, all of vital importance to our modern technological society. "},
author = {Deng, Li and Yu, Dong},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Deng, Yu - Unknown - the essence of knowledge Deep Learning Methods and Applications Foundations and Trends(2).pdf:pdf},
title = {{the essence of knowledge Deep Learning Methods and Applications Foundations and Trends}}
}
@article{YannLeCunYoshuaBengio2015,
author = {{Yann LeCun, Yoshua Bengio}, Geoffrey Hinton},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Yann LeCun, Yoshua Bengio - 2015 - Deep learning.pdf:pdf},
journal = {International weekly journal of science},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Huynh2015,
author = {Huynh, Khai T and Bui, Thang H and Quan, Tho T},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Huynh, Bui, Quan - 2015 - Instance-based Web Services Composition and Verification.pdf:pdf},
isbn = {9781467365475},
keywords = {because both validation and,instance-based web ser-,qos-based web ser-,temporal web service constraint,the production of system,vices composition,web services composition,web services verification},
pages = {245--249},
title = {{Instance-based Web Services Composition and Verification}},
year = {2015}
}
@article{Maoji2013,
author = {Maoji, Wang},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Maoji - 2013 - Web Services Composition Based on User Requirement.pdf:pdf},
isbn = {9781467344630},
keywords = {-semantic web,service composition,web service},
number = {Iccse},
pages = {1481--1484},
title = {{Web Services Composition Based on User Requirement}},
year = {2013}
}
@article{Edition2008,
author = {Edition, Second},
doi = {10.1007/978-1-4302-0176-2},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Edition - 2008 - The Definitive Guide to API.pdf:pdf},
isbn = {9781590595855},
title = {{The Definitive Guide to API}},
year = {2008}
}
@article{Rajapaksage2010,
author = {Rajapaksage, Jayampathi S},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rajapaksage - 2010 - Data Aggregation through Web Service Composition in Smart Camera Networks.pdf:pdf},
title = {{Data Aggregation through Web Service Composition in Smart Camera Networks}},
year = {2010}
}
@article{Ganesarajah2002,
abstract = {While SOAP/XML is perceived as the appropriate interoperability level for Web-services, companies compete to provide workflow-based tools for Web-service integration. This paper presents the design and implementation of a prototype workflow management system for building new Web-services from a workflow of existing Web-services. This enables the creation of multiple layers of value-added service providers and provides fast service creation, customisation and deployment. The system caters for multiple workflow paradigms, provides an extensible language for workflow specification and emphasises encapsulation and tight constraints on workflow execution. To expose a workflow of Web-services as a Web-service, several design steps have been required including the deployment as a Web-service of the generic workflow engine and a generalisation of the Visitor Pattern to concurrent visitors.},
author = {Ganesarajah, Dinesh and Lupu, Emil},
doi = {10.1109/EDOC.2002.1137716},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ganesarajah, Lupu - 2002 - Workflow-based composition of WEB-services A business model or a programming paradigm.pdf:pdf},
isbn = {0-7695-1742-0},
issn = {15417719},
journal = {Proceedings - 6th International Enterprise Distributed Object Computing Conference},
number = {January},
pages = {273--284},
title = {{Workflow-based composition of WEB-services: A business model or a programming paradigm?}},
volume = {2002-Janua},
year = {2002}
}
@article{Benatallah2003,
author = {Benatallah, Boualem},
doi = {10.1007/978-1-4471-0097-3_10},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Benatallah - 2003 - Towards Patterns of Web Services Composition.pdf:pdf},
isbn = {978-1-85233-506-9},
pages = {265--296},
title = {{Towards Patterns of Web Services Composition}},
year = {2003}
}
@article{Rusk2015,
abstract = {NATURE METHODS | VOL.13 NO.1 | JANUARY 2016 | 35 METHODS TO WATCH | SPECIAL FEATURE and high computational costs are being tackled. Researchers in academic settings as well as in startup companies such as Deep Genomics, launched July 22, 2015, by some of the authors of DeepBind, will increasingly apply deep learning to genome analysis and precision medicine. The goal is to predict the effect of genetic variants— both naturally occurring and introduced by genome editing—on a cell's regulatory landscape and how this in turn affects dis-ease development. Nicole Rusk ❯❯Deep learning New computational tools learn complex motifs from large sequence data sets. A powerful form of machine learning that enables computers to solve perceptual problems such as image and speech rec-ognition is increasingly making an entry into the biological sciences. These deep-learning methods, such as deep artificial neural networks, use multiple processing layers to discover patterns and structure in very large data sets. Each layer learns a concept from the data that subsequent lay-ers build on; the higher the level, the more abstract the concepts that are learned. Deep learning does not depend on prior data processing and automatically extracts features. To use a simple example, a deep neural network tasked with interpreting shapes would learn to recognize simple edges in the first layer and then add recog-nition of the more complex shapes com-posed of those edges in subsequent lay-ers. There is no hard and fast rule for how many layers are needed to constitute deep learning, but most experts agree that more than two are required. Recent examples show the power of deep learning to derive regulatory fea-tures in genomes from DNA sequence alone: DeepSEA (Nat. Methods 12, 931– 934, 2015) uses genomic sequence as input, trains on chromatin profiles from large consortia such as ENCODE and the Epigenomics Roadmap, and predicts the effect of single-nucleotide variants on reg-ulatory regions such as DNase hypersen-sitive sites, transcription factor–binding sites and histone marks. Basset (bioRxiv, doi:10.1101/028399, 2015) uses similar deep neural networks to predict the effect of single-nucleotide polymorphisms on chromatin accessibility. DeepBind (Nat. Biotechnol. 33, 831–838, 2015) finds protein-binding sites on RNA and DNA and predicts the effects of mutations. Deep learning will be invaluable in the context of big data, as it extracts high-level information from very large volumes of data. As it gains traction in genome analy-sis, initial challenges such as overfitting due to rare dependencies in the training data},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Rusk, Nicole},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Yann LeCun, Yoshua Bengio - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {1548-7091},
journal = {Nature Methods},
number = {1},
pages = {35--35},
pmid = {10463930},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539{\%}5Cnhttp://www.nature.com/doifinder/10.1038/nmeth.3707},
volume = {13},
year = {2015}
}
@article{Najafabadi2015,
abstract = {Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.},
author = {Najafabadi, Maryam M and Villanustre, Flavio and Khoshgoftaar, Taghi M and Seliya, Naeem and Wald, Randall and Muharemagic, Edin},
doi = {10.1186/s40537-014-0007-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Najafabadi et al. - 2015 - Deep learning applications and challenges in big data analytics(2).pdf:pdf},
isbn = {9783319115382},
issn = {2196-1115},
journal = {Journal of Big Data},
keywords = {big data,deep learning},
number = {1},
pages = {1},
title = {{Deep learning applications and challenges in big data analytics}},
url = {http://www.journalofbigdata.com/content/2/1/1},
volume = {2},
year = {2015}
}
@article{Hummer2011,
author = {Hummer, W and Satzger, B and Leitner, P and Dustdar, S and Hummer, Waldemar and Satzger, Benjamin and Leitner, Philipp and Dustdar, Schahram},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hummer et al. - 2011 - Aggregation Over Web Service Event Streams Under Review for Publication in - Distributed Continuous Data Aggregat.pdf:pdf},
keywords = {184-1,8,argentinierstr,c 2011,complex event processing,continuous query,data aggrega-,distributed systems group,event-based system,tion,vienna university of technology,ws-aggregation},
title = {{Aggregation Over Web Service Event Streams Under Review for Publication in - Distributed Continuous Data Aggregation Over Web Service Event Streams}},
year = {2011}
}
@article{Hummer2011a,
abstract = {Throughout the last years, the Service-Oriented$\backslash$nArchitecture (SOA) paradigm has been promoted as a means to$\backslash$ncreate loosely coupled distributed applications. In theory,$\backslash$nSOAs make use of a service registry, which can be used by$\backslash$nproviders to publish their services and by clients to$\backslash$ndiscover these services in order to execute them. However,$\backslash$nservice registries such as UDDI did not succeed and are$\backslash$nrarely used today. In practice, the binding often takes$\backslash$nplace at design time (for instance by generating$\backslash$nclient-side stubs), which leads to a tighter coupling$\backslash$nbetween service endpoints. Alternative solutions using$\backslash$ndynamic invocations often lack a data abstraction and$\backslash$nrequire developers to construct messages on XML or SOAP$\backslash$nlevel. In this paper we present VRESCo, the Vienna Runtime$\backslash$nEnvironment for Serviceoriented Computing, which addresses$\backslash$nseveral distinct issues that are currently prevalent in$\backslash$nService-Oriented Architecture (SOA) research and practice.$\backslash$nVRESCo reemphasizes the importance of registries to support$\backslash$ndynamic selection, binding and invocation of services.$\backslash$nService providers publish their services and clients$\backslash$nretrieve the data stored in the registry using a$\backslash$nspecialized query language. The data model distinguishes$\backslash$nbetween abstract features and concrete service$\backslash$nimplementations, which enables grouping of services$\backslash$naccording to their functionality.An abstracted message$\backslash$nformat allows VRESCo to mediate between services which$\backslash$nprovide the same feature but use a different message$\backslash$nsyntax. Furthermore, VRESCo allows for explicit versioning$\backslash$nof services. In addition to functional entities, the VRESCo$\backslash$nservice metadata model contains QoS (Quality of Service)$\backslash$nattributes. Clients can be configured to dynamically rebind$\backslash$nto different service instances based on the QoS data. The$\backslash$npaper presents an illustrative scenario taken from the$\backslash$ntelecommunications domain, which serves as the basis for$\backslash$nthe discussion of the features of VRESCo.},
author = {Hummer, Waldemar and Leitner, Philipp and Michlmayr, Anton and Rosenberg, Florian and Dustdar, Schahram},
doi = {http://dx.doi.org/10.1007/978-3-7091-0415-6_11},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hummer et al. - 2011 - VRESCo - Vienna Runtime Environment for Service-oriented Computing.pdf:pdf},
isbn = {978-3-7091-0414-9},
journal = {Service Engineering: European Research Results},
pages = {299--324},
title = {{VRESCo - Vienna Runtime Environment for Service-oriented Computing}},
year = {2011}
}
@article{Truong2009,
abstract = {Providing data as a service has not only fostered the access to data from anywhere at anytime but also reduced the cost of investment. However, data is often associated with various concerns that must be explicitly described and modeled in order to ensure that the data consumer can find and select relevant data services as well as utilize the data in the right way. In particular, the use of data is bound to various rules imposed by data owners and regulators. Although, technically Web services and database technologies allow us to quickly expose data sources as Web services, until now, research has not been focused on the description of data service concerns, thus hindering the discovery, selection and utilization of data services. In this paper, we analyze major concerns for data as a service, model these concerns, and discuss how they can be used to improve the search and utilization of data services.},
author = {Truong, Hong Linh and Dustdar, Schahram},
doi = {10.1109/APSCC.2009.5394136},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Truong, Dustdar - 2009 - On analyzing and specifying concerns for data as a service.pdf:pdf},
isbn = {9781424453368},
journal = {2009 IEEE Asia-Pacific Services Computing Conference, APSCC 2009},
pages = {87--94},
title = {{On analyzing and specifying concerns for data as a service}},
year = {2009}
}
@article{Terzo2013,
abstract = {Data as a Service (DaaS) is among the latest kind of services being investigated in the Cloud computing community. The main aim of DaaS is to overcome limitations of state-of-the-art approaches in data technologies, according to which data is stored and accessed from repositories whose location is known and is relevant for sharing and processing. Besides limitations for the data sharing, current approaches also do not achieve to fully separate/decouple software services from data and thus impose limitations in inter-operability. In this paper we propose a DaaS approach for intelligent sharing and processing of large data collections with the aim of abstracting the data location (by making it relevant to the needs of sharing and accessing) and to fully decouple the data and its processing. The aim of our approach is to build a Cloud computing platform, offering DaaS to support large communities of users that need to share, access, and process the data for collectively building knowledge from data. We exemplify the approach from large data collections from health and biology domains.},
author = {Terzo, Olivier and Ruiu, Pietro and Bucci, Enrico and Xhafa, Fatos},
doi = {10.1109/CISIS.2013.87},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Terzo et al. - 2013 - Data as a Service (DaaS) for sharing and processing of large data collections in the cloud.pdf:pdf},
isbn = {9780769549927},
journal = {Proceedings - 2013 7th International Conference on Complex, Intelligent, and Software Intensive Systems, CISIS 2013},
keywords = {Cloud computing,Data as a Service,Genomics,Health data collections,Large data collection,Sharing},
pages = {475--480},
title = {{Data as a Service (DaaS) for sharing and processing of large data collections in the cloud}},
year = {2013}
}
@article{Vu2012,
abstract = {Cloud computing based Data-as-a-Service (DaaS) has become popular. Several data assets have been released in DaaSes across different cloud platforms. Nevertheless, there are no well-defined ways to describe DaaSes and their associated data assets. On the one hand, existing DaaS providers simply use HTML documents to describe their service. This simple way of service description requires user to manually perform service lookup by reading the HTML documents to understand DaaSes as well as their provided data assets. On the other hand, existing service description techniques are not suitable for describing DaaSes because they consider only service information. The lack of well-structured/linked model to describe DaaSes hinders the automatic service lookup for DaaSes and the integration of DaaSes into data composition and analytic tools. In this paper, we propose DEMODS, a Description Model for DaaS, which introduces a general linked model to cover all basic information of a DaaS. Besides the basic DaaS description model, we also introduce an extended model that integrates existing work in describing quality of data, data and service contract, data dependency, and Quality of Service (QoS). We present a mechanism to incorporate DEMODS into both new and existing DaaSes. Finally, a prototype of DEMODS has been developed to evaluate the effectiveness of the proposed model.},
author = {Vu, Quang Hieu and Pham, Tran Vu and Truong, Hong Linh and Dustdar, Schahram and Asal, Rasool},
doi = {10.1109/AINA.2012.91},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Vu et al. - 2012 - DEMODS A description model for data-as-a-service.pdf:pdf},
isbn = {9780769546513},
issn = {1550445X},
journal = {Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
keywords = {Cloud computing,Data-as-a-Service (DaaS),data marketplace,discovery,service and data description},
pages = {605--612},
title = {{DEMODS: A description model for data-as-a-service}},
year = {2012}
}
@article{Zheng2013,
abstract = {With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.},
author = {Zheng, Zibin and Zhu, Jieming and Lyu, Michael R.},
doi = {10.1109/BigData.Congress.2013.60},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zheng, Zhu, Lyu - 2013 - Service-generated big data and big data-as-a-service An overview.pdf:pdf},
isbn = {9780768550060},
issn = {9780768550060},
journal = {Proceedings - 2013 IEEE International Congress on Big Data, BigData 2013},
keywords = {Big Data-as-a-Service,big data,service computing},
pages = {403--410},
title = {{Service-generated big data and big data-as-a-service: An overview}},
year = {2013}
}
@article{Amdouni2014,
author = {Amdouni, Soumaya and Barhamgi, Mahmoud and Benslimane, Djamal and Faiz, Rim},
doi = {10.1109/SCC.2014.91},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Amdouni et al. - 2014 - Handling Uncertainty in Data Services Composition.pdf:pdf},
isbn = {978-1-4799-5066-9},
journal = {2014 IEEE International Conference on Services Computing},
keywords = {- data services,composition,uncertainty},
pages = {653--660},
title = {{Handling Uncertainty in Data Services Composition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6930592},
year = {2014}
}
@article{Warschofsky2010,
abstract = {Service-oriented Architectures support the provision, discovery, and usage of services in different application contexts. The Web Service specifications provide a technical foundation to implement this paradigm and provide mechanisms to face the new security challenges raised by SOA. To enable the seamless usage of services, security requirements can be expressed as security policies (e.g. WS-Policy and WS-Security Policy) that enable the negotiation of these requirements between clients and services. However, the concept of policy negotiation has not been applicable in the scope of service compositions so far. Since each orchestrated Web Service in a service composition might demand the provision of specific user information and requires a particular security mechanism, the security policy of a service composition depends on the aggregated requirements of the orchestrated services. Current Web Service frameworks are not capable of resolving such policy dependencies. In this paper we present our solution to enable an automated creation of security policies from orchestrated services. Therefore, we present a policy model that is capable of capturing Web Service security requirements. Based on this model, we introduce an algorithm that performs the aggregation of security requirements stated by the orchestrated services and mappings to transform WS-Security Policy instances and the security model instances into each other.},
author = {Warschofsky, Robert and Menzel, Michael and Meinel, Christoph},
doi = {10.1109/ECOWS.2010.13},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Warschofsky, Menzel, Meinel - 2010 - Transformation and aggregation of Web Service security requirements.pdf:pdf},
isbn = {9780769543109},
journal = {Proceedings - 8th IEEE European Conference on Web Services, ECOWS 2010},
keywords = {Policy Generation,SOA Security,Service-oriented Architectures,WS-SecurityPolicy},
pages = {43--50},
title = {{Transformation and aggregation of Web Service security requirements}},
year = {2010}
}
@article{,
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2007 - Aggregation and Adaptation of Web.pdf:pdf},
number = {May},
title = {{Aggregation and Adaptation of Web}},
year = {2007}
}
@article{Moncrieff2016,
abstract = {In data exploration, several online data sources may need to be dynamically aggregated or summarised over spatial region, time interval, or set of attributes. With respect to thematic data, web services are mainly used to present results leading to a supplier driven service model limiting the exploration of the data. In this paper we propose a user need driven service model based on geo web processing services. The aim of the framework is to provide a method for the scalable and interactive access to various geographic data sources on the web. The architecture combines a data query, processing technique and visualisation methodology to rapidly integrate and visually summarise properties of a dataset. We illustrate the environment on a health related use case that derives Age Standardised Rate - a dynamic index that needs integration of the existing interoperable web services of demographic data in conjunction with standalone non-spatial secure database servers used in health research. Although the example is specific to the health field, the architecture and the proposed approach are relevant and applicable to other fields that require integration and visualisation of geo datasets from various web services and thus, we believe is generic in its approach.},
author = {Moncrieff, Simon and Turdukulov, Ulanbek and Gulland, Elizabeth Kate},
doi = {10.1016/j.isprsjprs.2016.01.015},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Moncrieff, Turdukulov, Gulland - 2016 - Integrating geo web services for a user driven exploratory analysis.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Geo web services,Geovisual analytics,Health research,Server side,User driven analysis},
pages = {294--305},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Integrating geo web services for a user driven exploratory analysis}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2016.01.015},
volume = {114},
year = {2016}
}
@article{Wang2009,
author = {Wang, Qiang and Wang, Jiayao},
doi = {10.1109/CINC.2009.213},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Wang - 2009 - Intelligent web map service aggregation.pdf:pdf},
isbn = {9780769536453},
journal = {Proceedings of the 2009 International Conference on Computational Intelligence and Natural Computing, CINC 2009},
keywords = {Agent,GIS,OGC,Service aggregation,Web map service},
number = {2},
pages = {229--231},
title = {{Intelligent web map service aggregation}},
year = {2009}
}
@article{Liu2014,
abstract = {Today, it is a big challenge to support on-demand Web data combination in accordance with situation changes. This paper proposes a service hyperlink model to describe loose data dependencies between data services. Service hyperlinks can be automatically discovered using semantic matching techniques, and then be utilized via an automatic algorithm to efficiently compose data services to respond situation changes. Analyses and applications show the feasibility of our approach.},
author = {Liu, C and Wang, J W and Han, Y B},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Wang, Han - 2014 - Situation-Aware Data Service Composition Based on Service Hyperlinks.pdf:pdf},
isbn = {0302-9743$\backslash$r978-3-642-54370-8; 978-3-642-54369-2},
journal = {Web Information Systems Engineering - Wise 2013 Workshops},
keywords = {automatic composition,data service,service hyperlink,situa-},
pages = {153--167},
title = {{Situation-Aware Data Service Composition Based on Service Hyperlinks}},
volume = {8182},
year = {2014}
}
@article{Li2009,
author = {Li, Xitong and Zhu, Hongwei},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Li, Zhu - 2009 - Improving Data Quality for Web Services Composition.pdf:pdf},
journal = {Context},
number = {September},
title = {{Improving Data Quality for Web Services Composition}},
year = {2009}
}
@article{Gekas2005,
author = {Gekas, John and Fasli, Maria},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gekas, Fasli - 2005 - Automatic Web Service Composition Using Web Connectivity Analysis Techniques 1 . Introduction - Position 2 . Frame.pdf:pdf},
journal = {Methodology},
title = {{Automatic Web Service Composition Using Web Connectivity Analysis Techniques 1 . Introduction - Position 2 . Framework Architecture}},
year = {2005}
}
@article{Rao2005,
abstract = {In today's Web, Web services are created and updated on the fly. It's already beyond the human ability to analysis them and generate the composition plan manually. A number of approaches have been proposed to tackle that problem. Most of them are inspired by the researches in cross-enterprise workflow and AI planning. This paper gives an overview of recent research efforts of automatic Web service composition both from the workflow and AI planning research community.},
author = {Rao, Jinghai and Su, Xiaomeng},
doi = {10.1007/b105145},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rao, Su - 2004 - A Survey of Automated Web Service Composition Methods.pdf:pdf},
isbn = {978-3-540-24328-1, 978-3-540-30581-1},
issn = {03029743},
journal = {Semantic Web Services and Web Process Composition},
pages = {43--54},
title = {{A survey of automated web service composition methods}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-30581-1{\_}5},
year = {2005}
}
@article{Hummer2011b,
abstract = {Recent trends of Web-based data processing (e.g., service$\backslash$nmashups, Data-as-a-Service) call for techniques to collect$\backslash$nand process heterogeneous data from distributed sources in$\backslash$na uniform way. In this paper we present WS-Aggregation, a$\backslash$ngeneral purpose framework for aggregation of data exposed$\backslash$nas Web services. WS-Aggregation provides clients with a$\backslash$nsingle-site interface to execute multi-site queries. The$\backslash$nframework autonomously collects and processes the requested$\backslash$ndata using a set of cooperative aggregator nodes. The query$\backslash$ndistribution is configurable using strategies, e.g.,$\backslash$nQoS-based or location-based. We introduce WAQL as a$\backslash$nspecialized query language for Web service data aggregation$\backslash$nthat is based on XQuery. 3-way querying is a possibility to$\backslash$noptimize requests by reducing the amount of data$\backslash$ntransferred between aggregator nodes. AWeb-based graphical$\backslash$nuser interface facilitates composing aggregation requests.$\backslash$nOur performance evaluation, which comprises aggregation$\backslash$nscenarios with different settings, shows the good$\backslash$nscalability of WS-Aggregation.},
author = {Hummer, Waldemar and Leitner, Philipp and Dustdar, Schahram},
doi = {10.1145/1982185.1982520},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hummer, Leitner, Dustdar - 2011 - WS-Aggregation Distributed Aggregation of Web Services Data.pdf:pdf},
isbn = {978-1-4503-0113-8},
journal = {Proceedings of the 2011 ACM Symposium on Applied Computing (SAC 2011)},
keywords = {Aggregation,Distribution,Query Language,Web Services},
pages = {1590--1597},
title = {{WS-Aggregation: Distributed Aggregation of Web Services Data}},
year = {2011}
}
@article{Lin2014,
author = {Lin, Hung Yu and Huang, Jiung Long},
doi = {10.1109/IIH-MSP.2014.25},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Huang - 2014 - A Web API Aggregation Service for Mobile Mashup Applications.pdf:pdf},
isbn = {978-1-4799-5390-5},
journal = {2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing},
keywords = {-mobile mashup applications,and latency,api,applications,is able to significantly,produced by mobile mashup,reduce the transfer size,smartphones,transfer size reduction,web,web resource},
pages = {73--76},
title = {{A Web API Aggregation Service for Mobile Mashup Applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6998271},
year = {2014}
}
@article{Wesley-Smith1800,
archivePrefix = {arXiv},
arxivId = {arXiv:1508.06655v1},
author = {Wesley-Smith, Ian},
doi = {10.1145/1235},
eprint = {arXiv:1508.06655v1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wesley-Smith - 1800 - Babel A platform for facilitating research in scholarly article.pdf:pdf},
isbn = {9781450321389},
keywords = {4d trajectory management,importance sampling,motion planning,separation assurance,tactical planning},
pages = {4503},
title = {{Babel: A platform for facilitating research in scholarly article}},
year = {1800}
}
@article{Jordan2014,
author = {Jordan, MI},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jordan - 2014 - Statistics and Machine Learning.pdf:pdf},
title = {{Statistics and Machine Learning}},
url = {https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama{\_}michael{\_}i{\_}jordan/ckelmtt?context=3},
year = {2014}
}
@article{Godugula2008,
author = {Godugula, Savithri},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Godugula - 2008 - Survey of Ontology Mapping Techniques Survey Quality and Assurance.pdf:pdf},
journal = {Software Quality and Assuranc},
pages = {1--14},
title = {{Survey of Ontology Mapping Techniques: Survey Quality and Assurance}},
url = {http://is.uni-paderborn.de/fileadmin/Informatik/AG-Engels/Lehre/SS08/Seminar{\_}Software-Qualit�tssicherung/Seminararbeiten/Comparsion{\_}of{\_}Ontology{\_}Matching{\_}Techniques{\_}Savitri{\_}Godugula{\_}Ver{\_}0{\_}9.pdf},
year = {2008}
}
@article{Rajman1997,
author = {Rajman, Martin and Besan, Romaric},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rajman, Besan - 1997 - Text Mining - Knowledge extraction from unstructured textual data.pdf:pdf},
journal = {Proceedings of the 6th Conference of International Federation of Classification Societies},
title = {{Text Mining - Knowledge extraction from unstructured textual data}},
year = {1997}
}
@article{Unbehauen2012,
author = {Unbehauen, J{\"{o}}rg and Hellmann, Sebastian and Auer, S{\"{o}}ren and Stadler, Claus},
doi = {10.1007/978-3-642-34213-4_3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unbehauen et al. - 2012 - Knowledge extraction from structured sources.pdf:pdf},
isbn = {9783642342127},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Knowledge Extraction,RDF,Triplification},
pages = {34--52},
title = {{Knowledge extraction from structured sources}},
volume = {7538},
year = {2012}
}
@article{Gangemi2013,
abstract = {In the last years, basic NLP tasks: NER, WSD, relation ex- traction, etc. have been configured for SemanticWeb tasks including on- tology learning, linked data population, entity resolution, NL querying to linked data, etc. Some assessment of the state of art of existing Knowl- edge Extraction (KE) tools when applied to the Semantic Web is then desirable. In this paper we describe a landscape analysis of several tools, either conceived specifically for KE on the Semantic Web, or adaptable to it, or even acting as aggregators of extracted data from other tools. Our aim is to assess the currently available capabilities against a rich palette of ontology design constructs, focusing specifically on the actual semantic reusability of KE output.},
author = {Gangemi, Aldo},
doi = {10.1007/978-3-642-38288-8-24},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gangemi - 2013 - A comparison of knowledge extraction tools for the semantic web.pdf:pdf},
isbn = {9783642382871},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {351--366},
title = {{A comparison of knowledge extraction tools for the semantic web}},
volume = {7882 LNCS},
year = {2013}
}
@article{See,
author = {See, Simon},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/See - Unknown - Semantic and Data Mining Technologies.pdf:pdf},
title = {{Semantic and Data Mining Technologies}}
}
@article{Tanimuraa1900,
author = {Tanimuraa, Y and Lyndena, S and Matonoa, a and Kojimaa, I},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tanimuraa et al. - 1900 - A Scalable RDF Data Processing Framework based on Pig and Hadoop.pdf:pdf},
journal = {Semantic-Web-Journal.Net},
keywords = {hadoop,parallel and distributed processing,pig,rdf,storage schema},
number = {0},
title = {{A Scalable RDF Data Processing Framework based on Pig and Hadoop}},
url = {http://www.semantic-web-journal.net/system/files/swj392.pdf},
volume = {0},
year = {1900}
}
@article{Abedjan2011,
author = {Abedjan, Ziawasch and Naumann, Felix},
doi = {10.1145/2064988.2064998},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Abedjan, Naumann - 2011 - Context and target configurations for mining RDF data.pdf:pdf},
isbn = {9781450309578},
journal = {Proceedings of the 1st international workshop on Search and mining entity-relationship data - SMER '11},
keywords = {association rules,rdf},
pages = {23},
title = {{Context and target configurations for mining RDF data}},
url = {http://dl.acm.org/citation.cfm?id=2064988.2064998},
year = {2011}
}
@book{Kumar2014,
abstract = {This brief provides methods for harnessing Twitter data to discover solutions to complex inquiries. The brief introduces the process of collecting data through Twitter's APIs and offers strategies for curating large datasets. The text gives examples of Twitter data with real-world examples, the present challenges and complexities of building visual analytic tools, and the best strategies to address these issues. Examples demonstrate how powerful measures can be computed using various Twitter data sources. Due to its openness in sharing data, Twitter is a prime example of social media in which researchers can verify their hypotheses, and practitioners can mine interesting patterns and build their own applications. This brief is designed to provide researchers, practitioners, project managers, as well as graduate students with an entry point to jump start their Twitter endeavors. It also serves as a convenient reference for readers seasoned in Twitter data analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kumar, Shamanth and Morstatter, Fred and Liu, Huan},
doi = {10.1007/978-1-4614-9372-3},
eprint = {arXiv:1011.1669v3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, Morstatter, Liu - 2014 - Twitter Data Analytics.pdf:pdf},
isbn = {978146149372-3},
issn = {1098-6596},
pages = {79},
pmid = {25246403},
title = {{Twitter Data Analytics}},
year = {2014}
}
@article{Lavalle2011,
abstract = {The article discusses highlights of the "MIT Sloan Management Review" and the IBM Institute for Business Value's study that determines how information and advanced analytics can help organizations. Some of the key findings are given, including the use of analytics by top-performing organizations five times more than lower performers. The study discovered a widespread belief that analytics offers value. Some reasons cited by respondents for improving information and analytics are discussed, including innovating to achieve competitive differentiation and to effectively use the growing data. The study also correlates performance and the competitive value of analytics.},
author = {Lavalle, Steve and Lesser, Eric and Shockley, Rebecca and Hopkins, Michael S and Kruschwitz, Nina},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lavalle et al. - 2011 - Big Data, Analytics and the Path From Insights to Value.pdf:pdf},
isbn = {15329194},
issn = {15329194},
journal = {MIT Sloan Management Review},
number = {2},
pages = {21--32},
pmid = {57750728},
title = {{Big Data, Analytics and the Path From Insights to Value}},
volume = {52},
year = {2011}
}
@article{Ascher2014,
author = {Ascher, By Brian},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ascher - 2014 - 8 Ways To Build And Use The New Breed Of Data-Driven Applications.pdf:pdf},
journal = {Forbes},
month = {apr},
pages = {6--11},
title = {{8 Ways To Build And Use The New Breed Of Data-Driven Applications}},
year = {2014}
}
@article{Probst2013,
author = {Probst, Laurent and Monfardini, Erica and Frideres, Laurent and Clarke, Steven and Etc.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Probst et al. - 2013 - Business Innovation Observatory Big Data Analytics {\&}.pdf:pdf},
journal = {EU Business Innovation Observatory},
title = {{Business Innovation Observatory Big Data Analytics {\&}}},
year = {2013}
}
@article{Abdmeziem2014,
abstract = {Internet of things (IoT) constitutes one of the most important technology that has the potential to affect deeply our way of life, after mobile phones and Internet. The basic idea is that every objet that is around us will be part of the network (Internet), interacting to reach a common goal. In another word, the Internet of Things concept aims to link the physical world to the digital one. Technology advances along with popular demand will foster the wide spread deployement of IoT's services, it would radically transform our corporations, communities, and personal spheres. In this survey, we aim to provide the reader with a broad overview of the Internet of things concept, its building blocks, its applications along with its challenges.},
archivePrefix = {arXiv},
arxivId = {1401.6877},
author = {Abdmeziem, Riad and Tandjaoui, Djamel},
eprint = {1401.6877},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Abdmeziem, Tandjaoui - 2014 - Internet of Things Concept, Building blocks, Applications and Challenges(2).pdf:pdf},
keywords = {internet of things,pervasive computing,rfid,smart environments,the pervasive presence around,us of various wire-,wsn},
title = {{Internet of Things: Concept, Building blocks, Applications and Challenges}},
url = {http://arxiv.org/abs/1401.6877},
year = {2014}
}
@article{Nain2010,
abstract = {There is a growing interest in leveraging Service Oriented Architectures (SOA) in domains such as home automation, automotive, mobile phones or e-Health. With the basic idea (supported in e.g. OSGi) that components provide services, it makes it possible to smoothly integrate the Internet of Things (IoT) with the Internet of Services (IoS). The paradigm of the IoS indeed offers interesting capabilities in terms of dynamicity and interoperability. However in domains that involve {\&}{\#}x201C;things{\&}{\#}x201D; (e.g. appliances), there is still a strong need for loose coupling and a proper separation between types and instances that are well-known in Component-Based approaches but that typical SOA fail to provide. This paper presents how we can still get the best of both worlds by augmenting SOA with a Component-Based approach. We illustrate our approach with a case study from the domain of home automation.},
author = {Nain, Gr{\'{e}}gory and Fouquet, Francois and Morin, Brice and Barais, Olivier and J{\'{e}}z{\'{e}}quel, Jean Marc},
doi = {10.1109/SEAA.2010.50},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Nain et al. - 2010 - Integrating IoT and IoS with a component-based approach.pdf:pdf},
isbn = {9780769541709},
issn = {1089-6503},
journal = {Proceedings - 36th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2010},
pages = {191--198},
title = {{Integrating IoT and IoS with a component-based approach}},
year = {2010}
}
@article{Gunes2014,
abstract = {The Cyber-Physical System (CPS) is a term describing a broad range of complex, multi-disciplinary, physically-aware next generation engineered system that integrates embedded computing technologies (cyber part) into the physical world. In order to define and understand CPS more precisely, this article presents a detailed survey of the related work, discussing the origin of CPS, the relations to other research fields, prevalent concepts, and practical applications. Further, this article enumerates an extensive set of technical challenges and uses specific applications to elaborate and provide insight into each specific concept. CPS is a very broad research area and therefore has diverse applications spanning different scales. Additionally, the next generation technologies are expected to play an important role on CPS research. All of CPS applications need to be designed considering the cutting-edge technologies, necessary system-level requirements, and overall impact on the real world. {\textcopyright} 2014 KSII.},
author = {Gunes, V and Peter, S and Givargis, T and Vahid, F},
doi = {10.3837/tiis.2014.12.001},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gunes et al. - 2014 - A survey on concepts, applications, and challenges in cyber-physical systems.pdf:pdf},
isbn = {19767277 (ISSN)},
issn = {19767277},
journal = {KSII Transactions on Internet and Information Systems},
keywords = {Cyber physical systems (CPSs),Cyber-Physical Systems,Cyber-physical systems,Embedded Computing Technologies,Embedded computing,Embedded computing technologies,Embedded systems,Engineered systems,INTERNET,Model- based designs,Model-Based Design,Model-based design,NETWORKS,Physically-aware Engineered Systems,Physically-aware engineered systems,Requirements,SECURITY,Surveys,System-Level,System-level requirements},
number = {12},
pages = {4242--4268},
title = {{A survey on concepts, applications, and challenges in cyber-physical systems}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84920971178{\&}partnerID=40{\&}md5=86c64fb1f7501656a7ef2f0297c1de53{\%}5Cn{\%}3CGo to ISI{\%}3E://WOS:000348048600001},
volume = {8},
year = {2014}
}
@article{Palem2014,
abstract = {The recent surge in big data technologies has left many executives, both of well-established organizations and emerging startups, wondering how best to harness big data. In particular, the analytics aspect of big data is enticing for both information technology (IT) service providers and non-IT firms because of its potential for high returns on investment, which have been heavily publicized, if not clearly demonstrated, by multiple whitepapers, webinars, and research surveys. Although executives may clearly perceive the benefits of big data analytics to their organizations, the path to the goal is not as clear or easy as it looks. And, it is not just the established organizations that have this challenge; even startups trying to take advantage of this big data analytics opportunity are facing the same problem of lack of clarity on what to do or how to formulate an executive strategy. This article is primarily for executives who are looking for help in formulating a strategy for achieving success with big data analytics in their operations. It provides guidelines to them plan an organization's short-term and long-term goals, and presents a strategy tool, known as the delta model, to develop a customer-centric approach to success with big data analytics.},
author = {Palem, Gopalakrishna},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Palem - 2014 - Formulating an Executive Strategy for Big Data Analytics.pdf:pdf},
journal = {Technology Innovation Management Review},
keywords = {Business And Economics,IT entrepreneurship,big data,business vision,executive strategy,predictive analytics},
number = {3},
pages = {25--34},
pmid = {1614470827},
title = {{Formulating an Executive Strategy for Big Data Analytics}},
volume = {4},
year = {2014}
}
@article{Gupta2014,
author = {Gupta, Divanshu and Sharma, Avinash and Unny, Narayanan and Manjunath, Geetha},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gupta et al. - 2014 - Graphical Analysis and Visualization of Big Data.pdf:pdf},
journal = {Bda 2014},
pages = {53--56},
title = {{Graphical Analysis and Visualization of Big Data}},
volume = {8883 LNCS},
year = {2014}
}
@book{Chen2014,
author = {Chen, Min and Mao, Shiwen and Zhang, Yin and Leung, Victor C.M.},
doi = {10.1007/978-3-319-06245-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Big Data-Related Technologies, Challenges and Future Prospects.pdf:pdf},
isbn = {9783319062440},
pages = {89},
title = {{Big Data-Related Technologies, Challenges and Future Prospects}},
year = {2014}
}
@article{Chen2014b,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Mao, Liu - 2014 - Big data A survey(2).pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
pmid = {1511170304},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@article{Chen2014a,
author = {Chen, Min and Mao, Shiwen and Zhang, Yin and Leung, Victor C.M.},
doi = {10.1007/978-3-319-06245-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Big Data.pdf:pdf},
isbn = {978-3-319-06244-0},
pages = {51--58},
title = {{Big Data}},
url = {http://link.springer.com/10.1007/978-3-319-06245-7},
year = {2014}
}
@book{Lyko2016,
abstract = {In this book readers will find technological discussions on the existing and emerging technologies across the different stages of the big data value chain. They will learn about legal aspects of big data, the social impact, and about education needs and requirements. And they will discover the business perspective and how big data technology can be exploited to deliver value within different sectors of the economy. The book is structured in four parts: Part I “The Big Data Opportunity” explores the value potential of big data with a particular focus on the European context. It also describes the legal, business and social dimensions that need to be addressed, and briefly introduces the European Commission's BIG project. Part II “The Big Data Value Chain” details the complete big data lifecycle from a technical point of view, ranging from data acquisition, analysis, curation and storage, to data usage and exploitation. Next, Part III “Usage and Exploitation of Big Data” illustrates the value creation possibilities of big data applications in various sectors, including industry, healthcare, finance, energy, media and public services. Finally, Part IV “A Roadmap for Big Data Research” identifies and prioritizes the cross-sectorial requirements for big data research, and outlines the most urgent and challenging technological, economic, political and societal issues for big data in Europe. This compendium summarizes more than two years of work performed by a leading group of major European research centers and industries in the context of the BIG project. It brings together research findings, forecasts and estimates related to this challenging technological context that is becoming the major axis of the new digitally transformed business environment.},
author = {Lyko, Klaus and Nitzschke, Marcus and Ngomo, Axel-Cyrille Ngonga},
booktitle = {New Horizons for a Data-Driven Economy},
doi = {10.1007/978-3-319-21569-3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lyko, Nitzschke, Ngomo - 2016 - Big Data Acquisition.pdf:pdf},
isbn = {331921568X},
pages = {39--61},
title = {{Big Data Acquisition}},
year = {2016}
}
@article{Hsieh2005,
abstract = {Content analysis is a widely used qualitative research technique. Rather than being a single method, current applications of content analysis show three distinct approaches: conventional, directed, or summative. All three approaches are used to interpret meaning from the content of text data and, hence, adhere to the naturalistic paradigm. The major differences among the approaches are coding schemes, origins of codes, and threats to trustworthiness. In conventional content analysis, coding categories are derived directly from the text data. With a directed approach, analysis starts with a theory or relevant research findings as guidance for initial codes. A summative content analysis involves counting and comparisons, usually of keywords or content, followed by the interpretation of the underlying context. The authors delineate analytic procedures specific to each approach and techniques addressing trustworthiness with hypothetical examples drawn from the area of end-of-life care.},
author = {Hsieh, Hsiu Fang and Shannon, Sarah E},
doi = {10.1177/1049732305276687},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hsieh, Shannon - 2005 - Three Approaches to Qualitative Content Analysis.pdf:pdf},
isbn = {1049-7323},
issn = {1049-7323},
journal = {Qualitative Health Research},
keywords = {Humans,Qualitative Research,Research,Terminal Care},
number = {9},
pages = {1277--1288},
pmid = {16204405},
title = {{Three Approaches to Qualitative Content Analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16204405{\%}5Cnhttp://qhr.sagepub.com/cgi/doi/10.1177/1049732305276687},
volume = {15},
year = {2005}
}
@article{Fensel2016,
author = {Fensel, Anna},
doi = {10.1007/978-3-319-21569-3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Fensel - 2016 - New Horizons for a Data-Driven Economy.pdf:pdf},
isbn = {978-3-319-21568-6},
number = {April},
pages = {303},
title = {{New Horizons for a Data-Driven Economy}},
year = {2016}
}
@article{Nutt2005,
abstract = {Nearly four hundred non-routine organizational decisions were investigated to discover search approaches - determining the frequency of use and success of each search approach uncovered. A "search approach" is made up of a direction and a means to uncover solution ideas. Direction indicates desired results and it can be either implicit or explicit, with an explicit direction offering either a problem or a goal-like target. Solutions can be uncovered by opportunity, bargaining, and chance as well as by rational approaches. Defining a search approach as a direction coupled with a means of search, search approaches were linked with indicators of success, measured by the decision's adoption, value and timeliness, noting frequency. A rational, goal-directed, search approach was more apt to produce successful outcomes. Bargaining with stakeholders to uncover solutions was always combined some of the search approaches in this study, and this merger improved the prospects of success. Searches with an opportunistic or chance (emergent opportunity) features and rational searches with a problem target were more apt to produce unsuccessful outcomes. The means used to come up with a solution had less bearing on success than did the type of direction, with goal-directed searches leading to the best outcomes. Each search approach is discussed to reveal best practices and to offer suggestions to improve practice. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Nutt, Paul C.},
doi = {10.1016/j.ejor.2003.07.009},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Nutt - 2005 - Search during decision making.pdf:pdf},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Decision making,Procedure,Search},
number = {3 SPEC. ISS.},
pages = {851--876},
title = {{Search during decision making}},
volume = {160},
year = {2005}
}
@article{Provost2013,
abstract = {Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even ‘‘sexy''—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Provost, Foster and Fawcett, Tom},
doi = {10.1089/big.2013.1508},
eprint = {arXiv:1011.1669v3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Provost, Fawcett - 2013 - Data Science and its Relationship to Big Data and Data-Driven Decision Making.pdf:pdf},
isbn = {2167-6461},
issn = {2167-6461},
journal = {Data Science and Big Data},
number = {1},
pages = {51--59},
pmid = {25246403},
title = {{Data Science and its Relationship to Big Data and Data-Driven Decision Making}},
url = {http://online.liebertpub.com/doi/abs/10.1089/big.2013.1508},
volume = {1},
year = {2013}
}
@article{RobertBoschGmbH2015,
author = {{Robert Bosch GmbH}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Robert Bosch GmbH - 2015 - IoT Business Model Builder.pdf:pdf},
journal = {Bosch Software Innovations},
title = {{IoT Business Model Builder}},
url = {http://enterprise-iot.org/book/enterprise-iot/part-ii-igniteiot-methodology/igniteiot-strategy-execution/se3-iot-opportunity-management/},
year = {2015}
}
@article{Omg2010,
abstract = {Bus model with means ends and a way to describe and define other constructs. A vocab is included},
author = {Omg},
doi = {formal/2008-08-02},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Omg - 2010 - Business Motivation Model(2).pdf:pdf},
isbn = {09187324 // 0917155X},
journal = {Omg},
number = {November},
pages = {/},
title = {{Business Motivation Model}},
year = {2010}
}
@article{Berkem2008,
abstract = {The purpose of this article is to provide a brief insight about how to link your business vision, goals, strategies, tactics as well as business rules according to BMM, then bridging the resulting business specifications towards components of a Service Oriented Architecture (SOA) in order to align IT according to your goals and directives. The Business Motivation Model (BMM) - Business Governance in a Volatile World was first voted by the OMG in 2005.},
author = {Berkem, Birol},
doi = {10.5381/jot.2008.7.8.c6},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Berkem - 2008 - From the business motivation model (BMM) to Service Oriented Architecture (SOA)(2).pdf:pdf},
issn = {16601769},
journal = {Journal of Object Technology},
number = {8},
pages = {57--70},
title = {{From the business motivation model (BMM) to Service Oriented Architecture (SOA)}},
volume = {7},
year = {2008}
}
@article{McAfee2012,
abstract = {Exploiting vast new fl ows of information can radically improve your company's performance. But fi rst you'll have to change your decision-making culture.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McAfee, Andrew and Brynjolfsson, Erik},
doi = {10.1007/s12599-013-0249-5},
eprint = {arXiv:1011.1669v3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/McAfee, Brynjolfsson - 2012 - Big Data. The management revolution.pdf:pdf},
isbn = {00178012},
issn = {00178012},
journal = {Harvard Buiness Review},
number = {10},
pages = {61--68},
pmid = {23074865},
title = {{Big Data. The management revolution}},
url = {http://www.buyukverienstitusu.com/s/1870/i/Big{\_}Data{\_}2.pdf},
volume = {90},
year = {2012}
}
@article{Sinha2013,
author = {Sinha, V. and Wegener, R.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sinha, Wegener - 2013 - The Value of Big Data How Analytics Differentiates Winners.pdf:pdf},
journal = {Bain {\&} Company},
pages = {1--8},
title = {{The Value of Big Data: How Analytics Differentiates Winners}},
year = {2013}
}
@article{Wee2015,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wee, D. and Kelly, R. and Cattel, J. and Breunig, M.},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wee et al. - 2015 - Industry 4.0 - how to navigate digitization of the manufacturing sector.pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
journal = {McKinsey {\&} Company},
pages = {1--62},
pmid = {15991970},
title = {{Industry 4.0 - how to navigate digitization of the manufacturing sector}},
year = {2015}
}
@article{Industry2016,
author = {Industry, Global and What, Survey},
doi = {10.1080/01969722.2015.1007734},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Industry, What - 2016 - Industry 4 . 0 Building the digital enterprise.pdf:pdf},
isbn = {9781467382465},
issn = {0196-9722},
keywords = {digitalisation, digitization, data, analytics, cap},
title = {{Industry 4 . 0 : Building the digital enterprise}},
year = {2016}
}
@article{Terzidis2012,
author = {Terzidis, Orestis and Oberle, Daniel and Kadner, Kay},
doi = {10.1007/978-1-4614-1864-1_1},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Terzidis, Oberle, Kadner - 2012 - The Internet of Services and USDL.pdf:pdf},
isbn = {978-1-4614-1863-4},
journal = {Handbook of Service Description: USDL and Its Methods},
keywords = {internet{\_}of{\_}services,usdl},
pages = {1--16},
title = {{The Internet of Services and USDL}},
url = {citeulike-article-id:10382935},
year = {2012}
}
@article{Lee2014,
abstract = {Today, in an Industry 4.0 factory, machines are connected as a collaborative community. Such evolution requires the utilization of advance-prediction tools, so that data can be systematically processed into information to explain uncertainties, and thereby make more "informed" decisions. Cyber-Physical System-based manufacturing and service innovations are two inevitable trends and challenges for manufacturing industries. This paper addresses the trends of manufacturing service transformation in big data environment, as well as the readiness of smart predictive informatics tools to manage big data, thereby achieving transparency and productivity. ?? 2014 Elsevier B.V.},
author = {Lee, Jay and Kao, Hung An and Yang, Shanhu},
doi = {10.1016/j.procir.2014.02.001},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Kao, Yang - 2014 - Service innovation and smart analytics for Industry 4.0 and big data environment.pdf:pdf},
isbn = {2212-8271},
issn = {22128271},
journal = {Procedia CIRP},
keywords = {Industrial big data,Manufacturing servitization,Predictive maintenance},
pages = {3--8},
publisher = {Elsevier B.V.},
title = {{Service innovation and smart analytics for Industry 4.0 and big data environment}},
url = {http://dx.doi.org/10.1016/j.procir.2014.02.001},
volume = {16},
year = {2014}
}
@article{Overview2016,
author = {Overview, An},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Overview - 2016 - at German Research Institutes Joint research for the future.pdf:pdf},
title = {{at German Research Institutes Joint research for the future}},
year = {2016}
}
@article{Jazdi2014,
abstract = {We are currently experiencing the fourth Industrial Revolution in terms of cyber physical systems. These systems are industrial automation systems that enable many innovative functionalities through their networking and their access to the cyber world, thus changing our everyday lives significantly. In this context, new business models, work processes and development methods that are currently unimaginable will arise. These changes will also strongly influence the society and people. Family life, globalization, markets, etc. will have to be redefined. However, the Industry 4.0 simultaneously shows characteristics that represent the challenges regarding the development of cyber-physical systems, reliability, security and data protection. Following a brief introduction to Industry 4.0, this paper presents a prototypical application that demonstrates the essential aspects.},
author = {Jazdi, N},
doi = {10.1109/AQTR.2014.6857843},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jazdi - 2014 - Cyber physical systems in the context of Industry 4.0.pdf:pdf},
isbn = {978-1-4799-3732-5},
journal = {Automation, Quality and Testing, Robotics, 2014 IEEE {\ldots}},
keywords = {CPS,Cloud Technology,Industry 4.0,Internet of Things},
pages = {2--4},
title = {{Cyber physical systems in the context of Industry 4.0}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6857843},
year = {2014}
}
@article{Golzer2015,
abstract = {Industry 4.0 stands for the 4th Industrial revolution and the new paradigm of autonomous and de- centralized control in production. Products and production systems are enhanced to Cyber Physical Systems which have the capability to communicate with each other, to build ad-hoc networks and for self-control and self-optimization. From the IT-perspective this involves a new level of networking, data integration and data processing in production. Established technologies like Internet of Things, Cloud or Big Data are propagated solution-components of Industry 4.0. So far, there is no founded elaboration of IT-requirements and no differentiated discussion on how solution-components fulfil these requirements. This research uses the method of content analysis to extract requirements of Industry 4.0 from current research publications. Objective of analysis is a structured compilation of requirements regarding data processing. The resulting category scheme enables further development of solution-components in the application domain of Industry 4.0. Furthermore, this paper shows how the requirements can be matched to the capabilities of Big Data software solutions. As a result, two general use cases for Big Data applications in Industry 4.0 were identified and characterized.},
author = {G{\"{o}}lzer, Philipp and Cato, Patrick and Amberg, Michael},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/G{\"{o}}lzer, Cato, Amberg - 2015 - Data Processing Requirements of Industry 4 . 0 –.pdf:pdf},
isbn = {9783000502842},
journal = {European Conference on Information Systems ECIS},
keywords = {0,big data,industry 4,production},
pages = {1--13},
title = {{Data Processing Requirements of Industry 4 . 0 –}},
volume = {0},
year = {2015}
}
@article{Deloitte2015,
abstract = {Industry 4.0 Solution (4 Layers)},
author = {Deloitte},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Deloitte - 2015 - Industry 4.0. Challenges and solutions for the digital transformation and use of exponential technologies.pdf:pdf},
journal = {Deloitte},
pages = {1--30},
title = {{Industry 4.0. Challenges and solutions for the digital transformation and use of exponential technologies}},
year = {2015}
}
@article{Lee2014a,
abstract = {— In today's competitive business environment, companies are facing challenges in dealing with big data issues for rapid decision making for improved productivity. Many manufacturing systems are not ready to manage big data due to the lack of smart analytics tools. Germany is leading a transformation toward 4th Generation Industrial Revolution (Industry 4.0) based on Cyber-Physical System based manufacturing and service innovation. As more software and embedded intelligence are integrated in industrial products and systems, predictive technologies can further intertwine intelligent algorithms with electronics and tether-free intelligence to predict product performance degradation and autonomously manage and optimize product service needs. This article addresses the trends of industrial transformation in big data environment as well as the readiness of smart predictive informatics tools to manage big data to achieve transparency and productivity.},
author = {Lee, Jay and Bagheri, Behrad and Kao, Hung-An},
doi = {10.13140/2.1.1464.1920},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Bagheri, Kao - 2014 - Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics.pdf:pdf},
journal = {Int. Conference on Industrial Informatics (INDIN) 2014},
keywords = {Big Data,Cyber Physical Systems,Prognostics and Health Management,—Industry 40},
number = {November 2015},
title = {{Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics}},
year = {2014}
}
@article{Chaudhuri2011,
abstract = {The article presents an overview of business intelligence technology and of business intelligence software, which features several decision support technologies which have been developed to help knowledge workers such as business executives, managers, and analysts, make better and faster decisions. A discussion of the impact that a decline in the costs of data acquisition has had on the desire of business professionals to acquire business intelligence technology and software is presented. Innovations in business information technology which have been caused by the increasing needs of business professionals are discussed.},
author = {Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek},
doi = {10.1145/1978542.1978562},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chaudhuri, Dayal, Narasayya - 2011 - An overview of business intelligence technology.pdf:pdf},
isbn = {00010782},
issn = {00010782},
journal = {Communications of the ACM},
number = {8},
pages = {88},
pmid = {63992001},
title = {{An overview of business intelligence technology}},
volume = {54},
year = {2011}
}
@article{Wu2007,
abstract = {Business intelligence is a business management term used to describe applications and technologies which are used to gather, provide access to and analyze data and information about the organization, to help make better business decisions. In other words, the purpose of business intelligence is to provide actionable insight Business intelligence technologies include traditional data warehousing technologies such as reporting, ad-hoc querying, online analytical processing (OLAP). More advanced business intelligence tools - such as HP Openview DecisionCenter - also include data-mining, predictive analysis using rule-based simulations, Web services and advanced visualization capabilities. In this paper we describe a service-oriented architecture for business intelligence that makes possible a seamless integration of technologies into a coherent business intelligence environment, thus enabling simplified data delivery and low-latency analytics. We compare our service-oriented approach with traditional BI architectures, illustrate the advantages of the service oriented paradigm and share our experience and the lessons learned in architecting and implementing the framework.},
author = {Wu, Liya and Barash, Gilad and Bartolini, Claudio},
doi = {10.1109/SOCA.2007.6},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Barash, Bartolini - 2007 - A Service-oriented Architecture for Business Intelligence.pdf:pdf},
isbn = {0-7695-2861-9},
issn = {2163-2871},
journal = {IEEE International Conference on Service-Oriented Computing and Applications (SOCA '07)},
keywords = {Business Intelligence,Data Warehousing,Service-Oriented Architecture},
pages = {279--285},
title = {{A Service-oriented Architecture for Business Intelligence}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4273437},
year = {2007}
}
@article{Liu2011,
abstract = {Enterprise mashups leverage various source of information to compose new situational applications. The architecture of such applications must address integration issues: it needs to deal with heterogeneous local and/or public data sources, and build value-added applications on existing corporate IT systems. In this paper, we leverage enterprise architecture integration patterns to compose reusable mashup components. We present a service oriented architecture that addresses reusability and integration needs for building enterprise mashup applications. Key techniques to customize this architecture are developed for mashups with themed data on location maps. The usage of this architecture is illustrated by a property valuation application derived from a real-world scenario. We demonstrate and discuss how this state-of-the-art architecture design method can be applied to enhance the design and development of emerging enterprise mashups. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Liu, Yan and Liang, Xin and Xu, Lingzhi and Staples, Mark and Zhu, Liming},
doi = {10.1016/j.jss.2011.01.030},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2011 - Composing enterprise mashup components and services using architecture integration patterns.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Design,Mashup,Patterns,Software architecture},
number = {9},
pages = {1436--1446},
publisher = {Elsevier Inc.},
title = {{Composing enterprise mashup components and services using architecture integration patterns}},
url = {http://dx.doi.org/10.1016/j.jss.2011.01.030},
volume = {84},
year = {2011}
}
@article{DeVrieze2011,
abstract = {Mashups combine web 2.0 and web services technology to provide end-user programming on the web. We explore how the existing data-oriented functionality can be extended for enterprise use. In particular to support business processes. Completing this, a design of a process-oriented enterprise mashup system is presented. ?? 2010 Published by Elsevier B.V. All rights reserved.},
author = {{De Vrieze}, Paul and Xu, Lai and Bouguettaya, Athman and Yang, Jian and Chen, Jinjun},
doi = {10.1016/j.future.2010.10.004},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/De Vrieze et al. - 2011 - Building enterprise mashups.pdf:pdf},
isbn = {0167-739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Mashup,Process-oriented mashup,Process-oriented service composition,Web 2.0},
number = {5},
pages = {637--642},
publisher = {Elsevier B.V.},
title = {{Building enterprise mashups}},
url = {http://dx.doi.org/10.1016/j.future.2010.10.004},
volume = {27},
year = {2011}
}
@article{Hoyer2011,
abstract = {Enterprise mashup (EM) is a new development paradigm which empowers users from the business units to create individual applications without the involvement of the IT department. Unlike the service-oriented architecture paradigm, which allows the automation of structured business processes, enterprise mashups represent a technology characterized by the peer production philosophy. It enables the automation of situational needs of knowledge workers which could not be modeled in advance and are typically not supported by available systems. An open and unaddressed question is the discussion of the benefits of this new paradigm. This paper closes this gap by designing a benefit model. By following the design science approach, we leverage the balance scorecard concept to structure and identify the benefits items from a holistic point of view. By means of a case study and an organized laboratory experiment, we demonstrate the applicability of the designed benefit model.},
author = {Hoyer, Volker and Stanoevska-Slabeva, Katarina and Kramer, Simone and Giessmann, Andrea},
doi = {10.1109/HICSS.2011.490},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hoyer et al. - 2011 - What are the business benefits of enterprise mashups.pdf:pdf},
isbn = {9780769542829},
issn = {15301605},
journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
keywords = {What Are the Business Benefits of Enterprise Mashu},
pages = {1--10},
title = {{What are the business benefits of enterprise mashups?}},
year = {2011}
}
@article{Pahlke2010,
abstract = {Currently, several Enterprise 2.0 platforms are beginning to emerge. This new generation ofweb-based enterprise platforms significantly influences application development and use. Apart fromthe IT department, the end users participate in the development of business applications by composing their ownwork environments based on their continuously changing needs. This paper introduces EnterpriseMashup technology as ameans to improve IT alignment of individualwork processes and changing business needs. Furthermore, organizational key drivers, technical challenges and inhibitors are discussed to assess the potential business value and explain the emerging expansion ofMashup platforms in companies.},
author = {Pahlke, Immanuel and Beck, Roman and Wolf, Martin},
doi = {10.1007/s12599-010-0121-9},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pahlke, Beck, Wolf - 2010 - Enterprise Mashup Systems as Platform for Situational Applications.pdf:pdf},
isbn = {1867-0202},
issn = {0937-6429},
journal = {Bise},
number = {5},
pages = {305--315},
title = {{Enterprise Mashup Systems as Platform for Situational Applications}},
volume = {2},
year = {2010}
}
@article{Kobielus2009,
author = {Kobielus, James and Karel, R. and Evelson, B. and Coit, C.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kobielus et al. - 2009 - Mighty mashups do-it-yourself business intelligence for the new economy.pdf:pdf},
journal = {Forrester Research},
title = {{Mighty mashups: do-it-yourself business intelligence for the new economy}},
url = {http://www.inetsoft.com.cn/literature/Forrester{\_}Research-Mighty{\_}Mashups.pdf{\%}5Cnhttp://www.corda.com/pdfs/mighty-mashups-article.pdf},
year = {2009}
}
@article{Carrier2008,
author = {Carrier, Nicole and Deutsch, Tom and Gruber, Chris and Heid, Mark and Jarrett, Lisa Lucadamo},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Carrier et al. - 2008 - The business case for enterprise mashups.pdf:pdf},
journal = {Web 2.0 technology solutions, IBM White Paper},
number = {August},
title = {{The business case for enterprise mashups}},
year = {2008}
}
@article{Clarkin2007,
author = {Clarkin, Larry and Holmes, Josh},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Clarkin, Holmes - 2007 - Enterprise Mashups.pdf:pdf},
journal = {The Architecture Journal},
pages = {1--8},
title = {{Enterprise Mashups}},
url = {https://msdn.microsoft.com/en-us/library/bb906060.aspx},
volume = {906060},
year = {2007}
}
@article{Analytics,
author = {Analytics, Visual},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Analytics - Unknown - for the Entire Organization Oracle Business Intelligence 12c.pdf:pdf},
pages = {1--5},
title = {{for the Entire Organization Oracle Business Intelligence 12c}}
}
@article{Samuel2009,
author = {Samuel, Robert E},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Samuel - 2009 - Using enterprise mashups as a business intelligence situational application.pdf:pdf},
keywords = {0,business,enterprise data services,intelligence,mashups,situated software,situational applications,web 2},
number = {2},
pages = {293--298},
title = {{Using enterprise mashups as a business intelligence situational application}},
volume = {X},
year = {2009}
}
@article{Joe2007,
abstract = {As a result of the proliferation of Web 2.0 style web sites, the practice of mashup services has become increasingly popular in the web development community. While mashup services bring flexibility and speed in delivering new valuable services to consumers, the issue of accountability associated with the mashup practice remains largely ignored by the industry. Furthermore, realizing the great benefits of mashup services, industry leaders are eagerly pushing these services into the enterprise arena. Although enterprise mashup services hold great promise in delivering a flexible SOA solution in a business context, the lack of accountability in current mashup solutions may render this ineffective in the enterprise environment. This paper defines accountability for mashup services, analyses the underlying issues in practice, and finally proposes a framework and ontology to model accountability. This model may then be used to develop effective accountability solutions for mashup environments.},
author = {Joe, Zou and Pavlovski, Christopher J.},
doi = {10.1109/ICEBE.2007.12},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Joe, Pavlovski - 2007 - Towards accountable enterprise mashup services.pdf:pdf},
isbn = {0769530036},
journal = {Proceedings - ICEBE 2007: IEEE International Conference on e-Business Engineering - Workshops: SOAIC 2007; SOSE 2007; SOKM 2007},
pages = {205--212},
title = {{Towards accountable enterprise mashup services}},
year = {2007}
}
@article{Chen2012,
abstract = {Business intelligence and analytics (BI{\&}A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI{\&}A. BI{\&}A 1.0, BI{\&}A 2.0, and BI{\&}A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI{\&}A is analyzed and challenges and opportunities associated with BI{\&}A research and education are identified. We also report a bibliometric study of critical BI{\&}A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI{\&}A research framework.},
author = {Chen, Hsinchun and Chiang, Roger H. L. and Storey, Veda C},
doi = {10.1145/2463676.2463712},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Chiang, Storey - 2012 - Business Intelligence and Analytics From Big Data To Big Impact.pdf:pdf},
isbn = {02767783},
issn = {0276-7783},
journal = {Mis Quarterly},
keywords = {Business intelligence and analytics,Web 2.0,big data analytics},
number = {4},
pages = {1165--1188},
pmid = {83466038},
title = {{Business Intelligence and Analytics: From Big Data To Big Impact}},
volume = {36},
year = {2012}
}
@article{Peenikal2009,
author = {Peenikal, Sunilkumar},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Peenikal - 2009 - Mashups and the Enterprise.pdf:pdf},
journal = {White Paper},
title = {{Mashups and the Enterprise}},
url = {http://www.ics.uci.edu/{~}projects/rcccs/Mashups{\_}and{\_}the{\_}Enterprise.pdf},
year = {2009}
}
@article{Tjoa2015,
author = {Tjoa, A. Min and Wetz, Peter and Kiesling, Elmar and Trinh, Tuan-Dat and Do, Ba-Lam},
doi = {10.1016/j.procs.2015.12.098},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tjoa et al. - 2015 - Integrating Streaming Data into Semantic Mashups.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
pages = {1--4},
publisher = {Elsevier Masson SAS},
title = {{Integrating Streaming Data into Semantic Mashups}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050915035590},
volume = {72},
year = {2015}
}
@article{Nguyen2012,
author = {Nguyen, Hoan and Quoc, Mau and Serrano, Martin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen, Quoc, Serrano - 2012 - Super Stream Collider–Linked Stream Mashups for Everyone.pdf:pdf},
journal = {Semantic Web Challenge co-located with ISWC2012},
title = {{Super Stream Collider–Linked Stream Mashups for Everyone}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.248.5925},
volume = {1380},
year = {2012}
}
@article{Phuoc2008,
abstract = {Making effective use of RDF data published online (such as RDF DBLP, DBpedia, FOAF profiles) is, in practice, all but straightforward. Data might be fragmented or incomplete so that multiple sources need to be joined, different identifiers (URIs) are usually employed for the same entities, ontologies need alignment, certain information might need to be ”patched”, etc. The only approach available to these problems so far has been custom programming such transformations for the specific task to be performed in a Semantic Web application. In this demo, we illustrate a paradigm for creating and reusing such transformation in an easy, visual web-based and collaborative way: DERI Pipes},
author = {Phuoc, Danh Le and Polleres, Axel and Tummarello, Giovanni and Morbidoni, Christian},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Phuoc et al. - 2008 - DERI Pipes visual tool for wiring Web data sources.pdf:pdf},
keywords = {data mashup,semantic web},
number = {January},
title = {{DERI Pipes: visual tool for wiring Web data sources}},
year = {2008}
}
@article{Bianchini2010,
abstract = {Techniques and tools for semantic description of mashup components are gaining more and more interest as an opportunity for a (semi-) automatic support of mashup design. We propose a recommendation system to design mashup applications, relying on the semantic description of mashup components for their proactive suggestion to the designer. A semantic organization of components is defined to propose ranked components, according to their similarity with designer's requirements and their mutual coupling.},
author = {Bianchini, Devis and {De Antonellis}, Valeria and Melchiori, Michele},
doi = {10.1109/DEXA.2010.48},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bianchini, De Antonellis, Melchiori - 2010 - A recommendation system for semantic mashup design.pdf:pdf},
isbn = {9780769541747},
issn = {1529-4188},
journal = {Proceedings - 21st International Workshop on Database and Expert Systems Applications, DEXA 2010},
keywords = {Proactive mashup construction,Recommendation system,Semantic-driven mashup},
pages = {159--163},
title = {{A recommendation system for semantic mashup design}},
year = {2010}
}
@book{Endres-niggemeyer2013,
abstract = {The web is growing quickly, substructures are coming up: a {\{}social, semantic, etc.{\}} web, or the {\{}business, services, etc.{\}} ecosystem which includes all resources of a specific web habitat. In the mashup ecosystem, developers are in intense scientific activity, what is easily measured by the number of their recent papers. Since mashups inherit an opportunistic (participatory) attitude, a main point of research is enabling users to create situation-specific mashups with little effort. After an overview, the chapter highlights areas of intensive discussion one by one: mashup description and modeling, semantic mashups, media mashups, ubiquitous mashups and end-user related development. Information is organized in two levels: right under the headings, a block of topic-related references may pop up. It is addressed to readers with deeper interest. After that, the text for everybody explains and illustrates innovative approaches. The chapter ends with an almost fail-safe outlook: given the growth of the web, the ecosystem of mashups will keep branching out. Core mashup features such as reuse of resources, user orientation, and versatile coordination (loose coupling) of components will propagate.},
author = {Endres-niggemeyer, Brigitte},
booktitle = {Semantic Mashups},
doi = {10.1007/978-3-642-36403-7},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Endres-niggemeyer - 2013 - Semantic Mashups.pdf:pdf},
isbn = {978-3-642-36402-0},
pages = {1--51},
title = {{Semantic Mashups}},
url = {http://link.springer.com/10.1007/978-3-642-36403-7},
year = {2013}
}
@article{Hoang2014,
abstract = {The introduction of semantic web and Linked Data helps facilitate sharing of data on the Internet more easily. Subsequently, the resource description framework (RDF) is the standard in publishing structured data resources on the Internet and is used in interconnecting with other data resources. To remedy the data integration issues of the traditional web mashups, the semantic web technology uses the LinkedData based on RDF data model as the unified datamodel for combining, aggregating, and transforming data fromheterogeneous data resources to build LinkedDatamashups.There have been tremendous amounts of efforts of semantic web community to enable Linked Data mashups but there is still lack of a systematic survey on concepts, technologies, applications, and challenges. Therefore, in this paper, we investigate in detail semantic mashups research and application approaches in the information integration. This paper also presents a Linked Data mashup application as an illustration of the proposed approaches.},
author = {Hoang, Hanh Huu and Cung, Tai Nguyen Phuoc and Truong, Duy Khanh and Hwang, Dosam and Jung, Jason J.},
doi = {10.1155/2014/813875},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hoang et al. - 2014 - Semantic information integration with linked data mashups approaches(2).pdf:pdf},
issn = {15501477},
journal = {International Journal of Distributed Sensor Networks},
title = {{Semantic information integration with linked data mashups approaches}},
volume = {2014},
year = {2014}
}
@article{Jarrar2009,
abstract = {This paper is motivated by the massively increasing structured data on the Web (Data Web), and the need for novel methods to exploit these data to their full potential. Building on the remarkable success of Web 2.0 mashups, this paper regards the internet as a database, where each web data source is seen as a table, and a mashup is seen as a query over these sources. We propose a data mashup language, which allows people to intuitively query and mash up structured and linked data on the web. Unlike existing query methods, the novelty of MashQL is that it allows people to navigate, query, and mash up a data source(s) without any prior knowledge about its schema, vocabulary, or technical details. We even do not assume even that a data source should an online or inline schema. Furthermore, MashQL supports query pipes as a built-in concept, rather than only a visualization of links between modules.},
author = {Jarrar, Mustafa and Dikaiakos, Marios D.},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Jarrar, Dikaiakos - 2009 - A data mashup language for the Data Web.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
title = {{A data mashup language for the Data Web}},
volume = {538},
year = {2009}
}
@article{Tran2014,
author = {Tran, Tuan Nhat and Truong, Duy Khanh and Hoang, Hanh Huu and Le, Thanh Manh},
doi = {10.1007/978-3-319-05458-2_27},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Tran et al. - 2014 - Linked data mashups A review on technologies, applications and challenges.pdf:pdf},
isbn = {9783319054575},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Linked Data,Mashups,RDF,Semantic Web},
number = {PART 2},
pages = {253--262},
title = {{Linked data mashups: A review on technologies, applications and challenges}},
volume = {8398 LNAI},
year = {2014}
}
@article{Klimek,
author = {Kl{\'{i}}mek, Jakub and {\v{S}}koda, Petr and Necask$\backslash$`y, Martin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kl{\'{i}}mek, {\v{S}}koda, Necask`y - Unknown - Requirements on Linked Data Consumption Platform.pdf:pdf},
keywords = {BT-Linked Data,BT-Open Refine,BT-applications,BT-link discovery,BT-methodology,BT-visualization},
title = {{Requirements on Linked Data Consumption Platform}}
}
@article{Sabou2012,
abstract = {Decision makers in the tourism domain routinely need to combine and compare statistical indicators about tourism and other related areas (e.g., economic). While many organizations offer relevant data sets, their automatic access and reuse is hampered (i) by them being offered as data dumps in non-semantic encodings; (ii) by them assuming some implicit knowledge that is necessary to build applications (e.g., that a city is situated in a certain country) and (iii) by the use of incompatible ways to measure the same indicator without formally specifying the assumptions behind the measurement technique. We explore the use of linked data technologies to solve these issues by triplifying the content of TourMIS, a broadly used data source of European tourism statistics and by building a prototype system using this data. Keywords},
author = {Sabou, Marta and Braşoveanu, Adrian M. P. and Arsal, Irem},
doi = {10.1145/2362499.2362533},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sabou, Braşoveanu, Arsal - 2012 - Supporting tourism decision making with linked data.pdf:pdf},
isbn = {9781450311120},
journal = {Proceedings of the 8th International Conference on Semantic Systems - I-SEMANTICS '12},
keywords = {tourism,tourism indicators,tourism statistics,triplification},
number = {September},
pages = {201},
title = {{Supporting tourism decision making with linked data}},
url = {http://dl.acm.org/citation.cfm?doid=2362499.2362533},
year = {2012}
}
@article{Cesta,
author = {Cesta, a and Fratini, S and Rasconi, R and Martino, Via S},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cesta et al. - Unknown - Supporting Increment Planning Processes within the ULISSE Framework.pdf:pdf},
title = {{Supporting Increment Planning Processes within the ULISSE Framework}}
}
@article{,
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - BALSAC Annual Report 2013-2014.pdf:pdf},
isbn = {292080345X},
title = {{BALSAC Annual Report 2013-2014}}
}
@article{Taylor2014,
author = {Taylor, James},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Taylor - 2014 - Real-Time Responses with Big Data.pdf:pdf},
journal = {Decision Management Solutions},
keywords = {big data,james taylor,real-time responses},
pages = {1--23},
title = {{Real-Time Responses with Big Data}},
year = {2014}
}
@book{St.LaurenceGlobalObservatorySLGO2008,
author = {{St. Laurence Global Observatory (SLGO)}},
booktitle = {Technology},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/St. Laurence Global Observatory (SLGO) - 2008 - Environmental Data Management Best Practices - Basic concepts.pdf:pdf},
isbn = {2007001799},
number = {August},
pages = {1--36},
title = {{Environmental Data Management Best Practices - Basic concepts}},
year = {2008}
}
@article{Davenport2013,
author = {Davenport, Thomas H and Dyche, Jill},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Davenport, Dyche - 2013 - Big Data in Big Companies.pdf:pdf},
journal = {International Institute for Analytics},
keywords = {Big data},
number = {May},
title = {{Big Data in Big Companies}},
year = {2013}
}
@article{Yao2015,
abstract = {The emerging Web of Things (WoT) will comprise billions of Web-enabled objects (or "things") where such objects can sense, communicate, compute and potentially actuate. WoT is essentially the embodiment of the evolution from systems linking digital documents to systems relating digital information to real-world physical items. It is widely understood that significant technical challenges exist in developing applications in the WoT environment. In this paper, we report our practical experience in the design and development of a smart home system in a WoT environment. Our system provides a layered framework for managing and sharing the information produced by physical things as well as the residents. We particularly focus on a research prototype named WITS, that helps the elderly live independently and safely in their own homes, with minimal support from the decreasing number of individuals in the working-age population. WITS enables an unobtrusive monitoring of elderly people in a real-world, inhabituated home environment, by leveraging WoT technologies in building context-aware, personalized services.},
archivePrefix = {arXiv},
arxivId = {1512.06257},
author = {Yao, Lina and Sheng, Quan Z. and Benatallah, Boualem and Dustdar, Schahram and Shemshadi, Ali and Wang, Xianzhi and Ngu, Anne H. H.},
eprint = {1512.06257},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Yao et al. - 2015 - Up in the Air When Homes Meet the Web of Things.pdf:pdf},
number = {May 2016},
title = {{Up in the Air: When Homes Meet the Web of Things}},
url = {http://arxiv.org/abs/1512.06257},
year = {2015}
}
@article{Halilaj2016,
archivePrefix = {arXiv},
arxivId = {1601.01556},
author = {Halilaj, Lavdim and Collarana, Diego},
doi = {10.1109/ICSC.2016.58},
eprint = {1601.01556},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Halilaj, Collarana - 2016 - Towards a Semantic Administrative Shell for Industry 4.0 Components.pdf:pdf},
isbn = {978-1-5090-0662-5},
number = {February},
title = {{Towards a Semantic Administrative Shell for Industry 4.0 Components}},
year = {2016}
}
@misc{,
title = {{End User Development | Henry Lieberman | Springer}},
url = {http://www.springer.com/gp/book/9781402042201},
urldate = {2016-05-11}
}
@book{Cimiano2015,
address = {Cham},
doi = {10.1007/978-3-319-19890-3},
editor = {Cimiano, Philipp and Frasincar, Flavius and Houben, Geert-Jan and Schwabe, Daniel},
isbn = {978-3-319-19889-7},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Engineering the Web in the Big Data Era}},
url = {http://link.springer.com/10.1007/978-3-319-19890-3},
volume = {9114},
year = {2015}
}
@book{Babazadeh2015,
address = {Cham},
author = {Babazadeh, Masiar and Gallidabino, Andrea and Pautasso, Cesare},
booktitle = {International Conference on Web Engineering},
doi = {10.1007/978-3-319-19890-3},
editor = {Cimiano, Philipp and Frasincar, Flavius and Houben, Geert-Jan and Schwabe, Daniel},
isbn = {978-3-319-19889-7},
language = {en},
month = {jun},
pages = {24--33},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Liquid Stream Processing Across Web Browsers and Web Servers}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-19890-3{\_}3},
volume = {9114},
year = {2015}
}
@book{Desolda2016a,
address = {Cham},
author = {Desolda, Giuseppe and Ardito, Carmelo and Matera, Maristella},
doi = {10.1007/978-3-319-28727-0},
editor = {Daniel, Florian and Pautasso, Cesare},
isbn = {978-3-319-28726-3},
language = {en},
pages = {63--81},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Rapid Mashup Development Tools}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-28727-0{\_}5/fulltext.html},
volume = {591},
year = {2016}
}
@book{Daniel2016,
address = {Cham},
doi = {10.1007/978-3-319-28727-0},
editor = {Daniel, Florian and Pautasso, Cesare},
isbn = {978-3-319-28726-3},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Rapid Mashup Development Tools}},
url = {http://link.springer.com/10.1007/978-3-319-28727-0},
volume = {591},
year = {2016}
}
@article{Liu2007,
abstract = {Mashup is a hallmark of Web 2.0 and attracts both industry and academia. It refers to an ad hoc composition technology of Web applications that allows users to draw upon content retrieved from external data sources to create entirely new services. Compared to traditional "developer-centric" composition technologies, e.g., BPEI and WSCI, mashup provides a flexible and easy-of-use way for service composition on Web. It makes the consumers free to compose services as they wish as well as simplifies the composition task. This paper makes two contributions. Firstly, we propose the mashup architecture, extend current SOA model with mashup and analyze how it facilitates service composition. Secondly, we propose a mashup component model to help developers leverage to create their own composite services. A case study is given to illustrate how to do service composition by mashup. This paper also discusses about some interesting topics about mashup.},
author = {Liu, Xuanzhe Liu Xuanzhe and Hui, Yi Hui Yi and Sun, Wei Sun Wei and Liang, Haiqi Liang Haiqi},
doi = {10.1109/SERVICES.2007.67},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2007 - Towards Service Composition Based on Mashup.pdf:pdf},
isbn = {978-0-7695-2926-4},
journal = {2007 IEEE Congress on Services (Services 2007)},
number = {Services},
pages = {0--7},
title = {{Towards Service Composition Based on Mashup}},
year = {2007}
}
@article{Pascalau2013,
abstract = {With the new technological advances and strong move towards Future Internet and Internet as a Platform a new environment is emerging. This environment is generative, social, strongly interactive and collaborative, so users play a fundamen- tal role in it. Business applications are simplifying, webifying and getting more user-centric. In this environment, context and context-awareness plays a fundamental role, as context gives meaning and accurately describes the situation of an user. This paper introduces the basis for a new research methodology that aims to address and visualize the topic of context and context- awareness from a holistic point of view, by means of text mining and},
author = {Pascalau, Emilian and Nalepa, Grzegorz J. and Kluza, Krzysztof},
doi = {10.1017/S174392130801867X},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pascalau, Nalepa, Kluza - 2013 - Towards a Better Understanding of.pdf:pdf},
isbn = {9781467344715},
issn = {1743-9213},
journal = {Proceedings of the 2013 Federated Conference on Computer Science and Information Systems},
pages = {959--962},
pmid = {15973898},
title = {{Towards a Better Understanding of}},
year = {2013}
}
@book{Freund,
author = {Freund, Jakob},
title = {{Real-Life BPMN: Using BPMN 2.0 to Analyze, Improve, and Automate Processes in Your Company}}
}
@article{Schmidt1999,
abstract = {Context is a key issue in interaction between human and computer, describing the surrounding facts that add meaning. In mobile computing location is usually used to approximate context and to implement context-aware applications. We propose that ultra-mobile computing, characterized by devices that are operational and operated while on the move (e.g. PDAs, mobile phones, wearable computers), can significantly benefit from a wider notion of context. To structure the field we introduce a working model for context, discuss mechanisms to acquire context beyond location, and application of context-awareness in ultra-mobile computing. We investigate the utility of sensors for context-awareness and present two prototypical implementations - a light-sensitive display and an orientation-aware PDA interface. The concept is then extended to a model for sensor fusion to enable more sophisticated context recognition. Based on an implementation of the model an experiment is described and the feasibility of the approach is demonstrated. Further, we explore fusion of sensors for acquisition of information on more sophisticated contexts.},
author = {Schmidt, Albrecht and Beigl, Michael and Gellersen, Hans W.},
doi = {10.1016/S0097-8493(99)00120-X},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Schmidt, Beigl, Gellersen - 1999 - There is more to context than location.pdf:pdf},
isbn = {0097-8493},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {adaptive user interface,context awareness,handheld computing,sensor-based ui,ultra-mobile computing,wearable},
number = {6},
pages = {893--901},
title = {{There is more to context than location}},
volume = {23},
year = {1999}
}
@article{Decker2007,
abstract = {Process choreographies describe interactions between different business partners and the dependencies between these interactions. While different proposals were made for capturing choreographies at an implementation level, it remains unclear how choreographies should be described on a conceptual level. While the Business Process Modeling Notation (BPMN) is already in use for describing choreographies in terms of interconnected interface behavior models, this paper will introduce interaction modeling using BPMN. Such interaction models do not suffer from incompatibility issues and are better suited for human modelers. BPMN extensions are proposed and a mapping from interaction models to interface behavior models is presented.},
author = {Decker, Gero and Barros, Alistair},
doi = {10.1007/978-3-540-78238-4_22},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Decker, Barros - 2007 - Interaction Modeling using BPMN.pdf:pdf},
isbn = {3540782370,},
journal = {Proceedings of the 2007 international conference on Business process management},
pages = {208--219},
title = {{Interaction Modeling using BPMN}},
volume = {4928},
year = {2007}
}
@article{SooKim,
abstract = {In the business process management, many business process execution languages such as XPDL, BPML, BPEL4WS have been specified with different origins and goals. Most of all, XPDL proposed by WfMC has been widely used in the related applications, especially workflows whose concepts are currently interchangeable with those of business processes. On the other hand, Business Process Modeling Notation (BPMN) driven by BPMI has recently been specified as a standardized graphical notation for a business process. We can therefore commonly design and analyze various business processes using the design tools to support BPMN. Notice that a BPMN-formed business process should be converted to its semantically equivalent business process languages such as XPDL which can consequently be executed by business process engines. In this regard, we propose a transformation mechanism from BPMN-formed business processes to corresponding XPDL processes.},
author = {{Soo Kim}, Hak and {Hyun Jo Kyung Hyun Tak}, Myung and {Seok Cha}, Hyun and {Hyun Son}, Jin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Soo Kim et al. - Unknown - Mapping from BPMN-Formed Business Processes to XPDL Business Processes(2).pdf:pdf},
keywords = {BPMN,XPDL,business process},
title = {{Mapping from BPMN-Formed Business Processes to XPDL Business Processes}}
}
@inproceedings{Chudnovskyy2010,
abstract = {Abstract—Data integration and content publishing in terms of Linked Data is a complex and time-consuming task while developing Web 2.0 applications. Considering this problem separately from architecture design increases application maintenance effort and causes additional overhead to provide public access functions. In this paper, we present the WebComposition/ Data Grid Service and its data management capabilities to meet demands of modern Web 2.0 applications. We show how to facilitate the application implementation and shorten development time by applying the Data Grid Service as Web Service-based storage solution.},
author = {Chudnovskyy, Olexiy and Gaedke, Martin},
booktitle = {The Second International Conferences on Advanced Service Computing (Service Computation 2010)},
keywords = {-rest,0,linked data,web 2},
pages = {55--61},
publisher = {Xpert Publishing Services},
title = {{Development of Web 2.0 Applications using WebComposition / Data Grid Service}},
year = {2010}
}
@article{GermanyTrade&Ivest,
author = {{Germany Trade {\&} Ivest}},
title = {{INDUSTRIE 4.0. Smart Manufacturing for the Future}},
url = {http://www.gtai.de/GTAI/Content/EN/Invest/{\_}SharedDocs/Downloads/GTAI/Brochures/Industries/industrie4.0-smart-manufacturing-for-the-future-en.pdf}
}
@article{HenningKagermannNationalAcademyofScienceandEngineering.WolfgangWahlsterGermanResearchCenterforArtificialIntelligence.Johannes2013,
abstract = {Germany has one of the most competitive manufacturing industries in the world and is a global leader in the manufacturing equipment sector. This is in no small measure due to Germany's specialisation in research, development and production of innovative manufacturing technologies and the management of complex industrial processes. Germany's strong machinery and plant manufacturing industry, its globally significant level of IT competences and its know-how in embedded systems and automation engineering mean that it is extremely well placed to develop its position as a leader in the manufacturing engineering industry. Germany is thus uniquely positioned to tap into the potential of a new type of industrialisation: Industrie 4.0.},
author = {{Henning, Kagermann(National Academy of Science and Engineering). Wolfgang, Wahlster (German Research Center for Artificial Intelligence). Johannes}, Helbig (Deutsche Post Ag).},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Henning, Kagermann(National Academy of Science and Engineering). Wolfgang, Wahlster (German Research Center for Artificial Intelligence).pdf:pdf},
number = {April},
pages = {82},
title = {{Recommendations for implementing the strategic initiative INDUSTRIE 4.01}},
year = {2013}
}
@article{Ruppert,
author = {Ruppert, Tobias and Dambruch, Jens and Kr, Michel and Balke, Tina and Gavanelli, Marco and Bragaglia, Stefano and Chesani, Federico and Milano, Michela},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ruppert et al. - Unknown - Visual Decision Support for Policy Making – Advancing Policy Analysis with Visualization.pdf:pdf},
pages = {1--32},
title = {{Visual Decision Support for Policy Making – Advancing Policy Analysis with Visualization}}
}
@article{Groß1994,
abstract = {Contents: Light in the computer -- Light and surface -- Artificial perspective -- An infinity of pyramids -- A sorcerer's apprentice -- Beyond appearances -- Conscious and preconscious},
author = {Gro{\ss}, M},
doi = {10.1007/978-3-642-55131-4},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gro{\ss} - 1994 - Visual Computing.pdf:pdf},
isbn = {3-540-57222-8},
journal = {Computer Graphics: Systems and Applications},
keywords = {Graphik},
title = {{Visual Computing}},
year = {1994}
}
@article{HenningKagermannNationalAcademyofScienceandEngineering.WolfgangWahlsterGermanResearchCenterforArtificialIntelligence.Johannes2013a,
abstract = {Germany has one of the most competitive manufacturing industries in the world and is a global leader in the manufacturing equipment sector. This is in no small measure due to Germany's specialisation in research, development and production of innovative manufacturing technologies and the management of complex industrial processes. Germany's strong machinery and plant manufacturing industry, its globally significant level of IT competences and its know-how in embedded systems and automation engineering mean that it is extremely well placed to develop its position as a leader in the manufacturing engineering industry. Germany is thus uniquely positioned to tap into the potential of a new type of industrialisation: Industrie 4.0.},
author = {{Henning, Kagermann(National Academy of Science and Engineering). Wolfgang, Wahlster (German Research Center for Artificial Intelligence). Johannes}, Helbig (Deutsche Post Ag).},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Henning, Kagermann(National Academy of Science and Engineering). Wolfgang, Wahlster (German Research Center for Artificial Intelligence).pdf:pdf},
number = {April},
pages = {82},
title = {{Recommendations for implementing the strategic initiative INDUSTRIE 4.01}},
year = {2013}
}
@article{Manufacturing,
author = {Manufacturing, Smart and The, F O R},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Manufacturing, The - Unknown - SMART MANUFACTURING FOR THE FUTURE.pdf:pdf},
title = {{SMART MANUFACTURING FOR THE FUTURE}}
}
@article{Lee2014b,
author = {Lee, Jay and Bagheri, Behrad and Kao, Hung-An},
journal = {IEEE Int. Conference on Industrial Informatics (INDIN) 2014},
title = {{Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics}},
url = {https://www.researchgate.net/profile/Behrad{\_}Bagheri/publication/266375284{\_}Recent{\_}Advances{\_}and{\_}Trends{\_}of{\_}Cyber-Physical{\_}Systems{\_}and{\_}Big{\_}Data{\_}Analytics{\_}in{\_}Industrial{\_}Informatics/links/542dc0100cf27e39fa948a7d?origin=publication{\_}detail},
year = {2014}
}
@article{,
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - RE -I M A G I N I N G GREEN PA P E R.pdf:pdf},
title = {{RE -I M A G I N I N G GREEN PA P E R}}
}
@article{Wetz,
author = {Wetz, Peter and Trinh, Tuan-dat and Do, Ba-lam and Anjomshoaa, Amin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wetz et al. - Unknown - Towards an Environmental Decision-Making System A Vocabulary to Enrich Stream Data.pdf:pdf},
keywords = {data cube vocabulary,environmental data streams,rdf,semantic sensor network ontology,stream processing},
pages = {1--19},
title = {{Towards an Environmental Decision-Making System : A Vocabulary to Enrich Stream Data}}
}
@article{Pena2014,
author = {Pe{\~{n}}a, O and Aguilera, Unai and L{\'{o}}pez-de-Ipi{\~{n}}a, D},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Pe{\~{n}}a, Aguilera, L{\'{o}}pez-de-Ipi{\~{n}}a - 2014 - Linked Open Data Visualization Revisited A Survey.pdf:pdf},
journal = {Semantic-Web-Journal},
keywords = {analytics,linked open data,semantic web,visual representations,visualization},
number = {0},
title = {{Linked Open Data Visualization Revisited: A Survey}},
volume = {0},
year = {2014}
}
@article{Millar2001,
author = {Millar, Annie and Simeone, Ronald S. and Carnevale, John T.},
doi = {10.1016/S0149-7189(00)00048-3},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Millar, Simeone, Carnevale - 2001 - Logic models a systems tool for performance management.pdf:pdf},
isbn = {0149-7189},
issn = {01497189},
journal = {Evaluation and Program Planning},
number = {1},
pages = {73--81},
title = {{Logic models: a systems tool for performance management}},
url = {http://www.sciencedirect.com/science/article/pii/S0149718900000483},
volume = {24},
year = {2001}
}
@article{Amaratunga2002,
author = {Amaratunga, Dilanthi and Baldry, David},
doi = {10.1108/02632770210426701},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Amaratunga, Baldry - 2002 - Moving from performance measurement to performance management.pdf:pdf},
isbn = {0263277021042},
issn = {0263-2772},
journal = {Facilities},
number = {5/6},
pages = {217--223},
title = {{Moving from performance measurement to performance management}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/02632770210426701},
volume = {20},
year = {2002}
}
@article{As2005,
author = {As, Unifob},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/As - 2005 - Practice – the Norwegian Way.pdf:pdf},
number = {November},
title = {{Practice – the Norwegian Way}},
year = {2005}
}
@article{Budgeting2000,
author = {Budgeting, J O F Public},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Budgeting - 2000 - BENCHMARKING AS A PERFORMANCE MANAGEMENT TOOL EXPERIENCES AMONG MUNICIPALITIES IN NORTH CAROLINA David N. Ammons.pdf:pdf},
number = {1},
pages = {106--124},
title = {{BENCHMARKING AS A PERFORMANCE MANAGEMENT TOOL: EXPERIENCES AMONG MUNICIPALITIES IN NORTH CAROLINA David N. Ammons*}},
volume = {12},
year = {2000}
}
@article{Coronado2015,
author = {Coronado, Miguel and Iglesias, Carlos a. and Serrano, Emilio},
doi = {10.1016/j.eswa.2015.06.031},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Coronado, Iglesias, Serrano - 2015 - Modelling rules for automating the Evented WEb by semantic technologies.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
number = {21},
pages = {7979--7990},
publisher = {Elsevier Ltd},
title = {{Modelling rules for automating the Evented WEb by semantic technologies}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417415004339},
volume = {42},
year = {2015}
}
@article{Harth,
author = {Harth, Andreas and Hose, Katja and Schenkel, Ralf},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Harth, Hose, Schenkel - Unknown - Linked Data Management Principles and Techniques.pdf:pdf},
title = {{Linked Data Management : Principles and Techniques}}
}
@article{Heil2014,
abstract = {An increasing share of today's work is knowledge work. Adaptive Case Management (ACM) assists knowledge workers in handling this collaborative, emergent and unpredictable type of work. Finding suitable workers for specific functions still relies on manual assessment and assignment by persons in charge, which does not scale well. In this paper we discuss a tool for ACM to facilitate this expert finding leveraging existing Web technology. We propose a method to automatically recommend a set of eligible workers utilizing linked data, enriched user profile data from distributed social networks and information gathered from case descriptions. This semantic recommendation method detects similarities between case requirements and worker profiles. The algorithm traverses distributed social graphs to retrieve a ranked list of suitable contributors to a case according to adaptable metrics. For this purpose, we introduce a vocabulary to specify case requirements and a vocabulary to describe skill sets and personal attributes of workers. The semantic recommendation method is demonstrated by a prototypical implementation using a WebID-based distributed social network.},
author = {Heil, Sebastian and Wild, Stefan and Gaedke, Martin},
doi = {10.1145/2567948.2577030},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Heil, Wild, Gaedke - 2014 - Collaborative Adaptive Case Management with Linked Data.pdf:pdf},
isbn = {9781450327459},
journal = {WWW '14 Companion: Proceedings of the 23rd International Conference on World Wide Web Companion},
keywords = {acm,expert finding,linked data,social web,webid},
pages = {99--102},
title = {{Collaborative Adaptive Case Management with Linked Data}},
year = {2014}
}
@article{Trinh2014,
abstract = {Seven years after Linked Data has been introduced as a concept to publish data on the web, an abundant cloud of Linked Open Data (LOD) built upon standard web technologies has emerged. To facilitate and encourage widespread use of that data, a critical step is now to streamline the process for creating applications on top of LOD. This paper discusses lessons learned while developing an open standards-based platform that aims to achieve that by means of Linked Widgets. Whereas resources are already connected in the LOD cloud, Linked Widgets in a similar vein aim to alleviate LOD application development in an open and interlinked fashion. Through reuse, we aim to foster both users' and developers' productivity and creativity.},
author = {Trinh, T.-D. and Wetz, P and Do, B.-L. and Anjomshoaa, A and Kiesling, E and Tjoa, A M},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Trinh et al. - 2014 - Implementing linked widgets Lessons learned for linked data developers.pdf:pdf},
isbn = {16130073 (ISSN)},
journal = {ISWC Developers Workshop 2014, ISWC-DEV 2014, Co-Located with the 13th International Semantic Web Conference, ISWC 2014},
keywords = {Application development,Critical steps,Data handling,Linked datum,Linked open data (LOD),Open Standards,Semantic Web,Social networking (online),Web technologies},
pages = {25--30},
title = {{Implementing linked widgets: Lessons learned for linked data developers}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84908676222{\&}partnerID=40{\&}md5=e6c2d1709649ad7fa41bec1e451a2deb},
volume = {1268},
year = {2014}
}
@article{Krug,
author = {Krug, Michael and Wiedemann, Fabian and Gaedke, Martin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Krug, Wiedemann, Gaedke - Unknown - SmartComposition Extending Web Applications to Multi-Screen Mashups.pdf:pdf},
keywords = {component-based web engineering,composition,dis-,html5,multi-screen mashup,reusable components,tributed multi-device web applications,web application development,web components},
title = {{SmartComposition : Extending Web Applications to Multi-Screen Mashups}}
}
@article{Aguinis2005,
author = {Aguinis, Herman Dr},
doi = {10.1007/SpringerReference_21210},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aguinis - 2005 - Performance management.pdf:pdf},
isbn = {9780131866157},
issn = {0815-936X},
journal = {Human resource management},
number = {1033},
pages = {1 -- 256},
pmid = {24187802},
title = {{Performance management}},
volume = {2011},
year = {2005}
}
@article{Brunetti2012,
author = {Brunetti, Josep Maria and Auer, S{\"{o}}ren and Garc{\'{i}}a, Roberto},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Brunetti, Auer, Garc{\'{i}}a - 2012 - The Linked Data Visualization Model.pdf:pdf},
journal = {International Semantic Web {\ldots}},
keywords = {interaction,linked data,semantic web,visualization},
title = {{The Linked Data Visualization Model}},
url = {http://svn.aksw.org/papers/2013/WWW{\_}LDVM/public.pdf},
year = {2012}
}
@article{Hendrik2014,
abstract = {Big Data is generally characterized by three V's: volume, velocity, and variety. For the Semantic Web community, the variety dimension could be the most appropriate and interesting aspect to contribute in. Since the real-world use of Big Data is for data analytics purposes of knowledge workers in different domains, we can consider mashup approach as an effective tool to create user-generated solution based on available private/public resources. This paper gives brief overview and comparison of some semantic mashup tools which can be employed to mash up various data sources in heterogenous data format. {\&}copy; 2014 IFIP International Federation for Information Processing.},
author = {Hendrik and Anjomshoaa, Amin and Tjoa, A Min},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hendrik, Anjomshoaa, Tjoa - 2014 - Towards semantic mashup tools for big data analysis.pdf:pdf},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Communication systems;Semantic Web;Tools;},
number = {Ld},
pages = {129--138},
title = {{Towards semantic mashup tools for big data analysis}},
url = {http://dx.doi.org/10.1007/978-3-642-55032-4-13},
volume = {8407 LNCS},
year = {2014}
}
@article{TheEconomistIntelligenceUnit2012,
author = {{The Economist Intelligence Unit}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/The Economist Intelligence Unit - 2012 - The Deciding Factor Big Data {\&} Decision Making.pdf:pdf},
journal = {Capgemini},
pages = {1--24},
title = {{The Deciding Factor: Big Data {\&} Decision Making}},
url = {http://www.capgemini.com/sites/default/files/resource/pdf/The{\_}Deciding{\_}Factor{\_}{\_}Big{\_}Data{\_}{\_}{\_}Decision{\_}Making.pdf},
year = {2012}
}
@book{Bauer2012,
abstract = {This introductory text describes the principles of linking data; defines important terms such as Open Government, Open (Government) Data and Linked Open (Government) Data; and explains relevant mechanisms to ensure a solid foundation before going more in-depth. It makes the case for using Linked Open Data, and offers a set of case studies from the energy industry and relating to legislation.},
author = {Bauer, Florian and Kaltenbock, Martin},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Bauer, Kaltenbock - 2012 - Linked Open Data The Essentials.pdf:pdf},
isbn = {9783902796059},
pages = {3--56},
title = {{Linked Open Data : The Essentials}},
url = {http://www.semantic-web.at/LOD-TheEssentials.pdf},
year = {2012}
}
@article{Mosterman2015a,
author = {Mosterman, Pieter J. and Zander, Justyna},
doi = {10.1007/s10270-015-0469-x},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mosterman, Zander - 2015 - Cyber-physical systems challenges a needs analysis for collaborating embedded software systems.pdf:pdf},
isbn = {1619-1366},
issn = {1619-1366},
journal = {Software {\&} Systems Modeling},
keywords = {Challenges,Computation,Cyber-physical systems,Embedded systems,Internet of Things,Modeling and simulation,challenges,computation,cyber-physical systems,embedded systems,internet of things,modeling and simulation},
publisher = {Springer Berlin Heidelberg},
title = {{Cyber-physical systems challenges: a needs analysis for collaborating embedded software systems}},
url = {http://link.springer.com/10.1007/s10270-015-0469-x},
year = {2015}
}
@article{Mosterman2015,
author = {Mosterman, Pieter J. and Zander, Justyna},
doi = {10.1007/s10270-015-0493-x},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Mosterman, Zander - 2015 - Industry 4.0 as a Cyber-Physical System study.pdf:pdf},
issn = {1619-1366},
journal = {Software {\&} Systems Modeling},
keywords = {0,Cyber-Physical Systems,Industrial practice,Industry 4.0,Modeling and simulation,cyber-physical systems,industrial practice,industry 4,modeling and simulation},
publisher = {Springer Berlin Heidelberg},
title = {{Industry 4.0 as a Cyber-Physical System study}},
url = {http://link.springer.com/10.1007/s10270-015-0493-x},
year = {2015}
}
@article{Klimek2014,
author = {Kl{\'{i}}mek, Jakub and Helmich, J and Ne{\v{c}}ask{\'{y}}, M},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kl{\'{i}}mek, Helmich, Ne{\v{c}}ask{\'{y}} - 2014 - Application of the Linked Data Visualization Model on Real World Data from the Czech LOD Cloud.pdf:pdf},
issn = {16130073},
journal = {WWW2014 workshop: Linked Data on the Web (LDOW2014)},
keywords = {linked data,semantic web,visualization},
title = {{Application of the Linked Data Visualization Model on Real World Data from the Czech LOD Cloud}},
url = {http://events.linkeddata.org/ldow2014/papers/ldow2014{\_}paper{\_}13.pdf},
year = {2014}
}
@article{Check,
author = {Check, Reality},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Check - Unknown - Goal Science Best Practices.pdf:pdf},
title = {{Goal Science Best Practices}}
}
@article{Abused2014,
abstract = {The fastener design for the transfer of concentrated transverse (out of plane, pull-out) loads to random glass fiber reinforced thermoset polymers was investigated. The elastic material properties, void content,a nd glass content of the composite were determined and a finite element model was used to analyze and compare the performance of the various washer designs for reducing the stress and strain levels near the edge of the washer at a bolted joint. Experimental studies were conducted to verify the finite element model.},
archivePrefix = {arXiv},
arxivId = {0803973233},
author = {Abused, Who and Doe, Jane and Ad, G R and Lo, B I O and Man, Gy- H U and Oach, Appr and Hi, G T- and Lo, P H I and Hy, S O P and Iety, S O C and Ashmore, M. and Assmann, Jan and Barnett, Jonathan M and Based, Mprovements and The, O N and Of, Oices and Bebchuk, Lucian a and Columbia, Source and Review, Law and October, No and Binder, Arnold and Boyle, Elizabeth Heger and Busse, Erika and Brooks, Rosa Ehrenreich and Bruckner, Tim a. and Snowden, Lonnie and Subbaraman, Meenakshi S. and Brown, Timothy T. and Bullard, R D and Johnson, G S and Burgard, Sarah and Buz, Jose and Byrne, James M. and Stowell, Jacob and Chaney, Carole Kennedy and Saltzstein, Grace Hall and Claridge, Karen and Cohen, Dov O V and Vandello, Joseph and Puente, Sylvia and Rantilla, Adrian and Cohen, Dov O V and Cole, Simon a and Confino, Alon and Conley, M and Barr, William M O and Review, Stanford Law Society and Mertz, Elizabeth and Engle, Sally and Gettingjustice, Merry and Conley, John M and Relation-, Rules and Copes, Heith and Hochstetler, Andy and Forsyth, Craig J. and Cornell, Scholarship and Digital, Law A and Law, Cornell and Publications, Faculty and Bowman, Cynthia Grant and Mertz, Elizabeth and Culture, Public and For, Uhvv and Dalenberg, Constance and Davis, Nora and Duntley, Q and Weissman, S and Stevens, S S and Kinney, J a S and Lythgoe, J N and Diver, Skin and Bridgman, C and Ross, H E and Luria, M and Neuro-, The and Harris, Casey T. Celia B S and Taylor, J G and Haven, New and Smith, K U and Feb, Mon and Pdf, Heinonline and Fletcher, Camille L and ForsterLee, R and Horowitz, I a and Ho, R and ForsterLee, L and McGovern, a and Frankenberg, Elizabeth and Sikoki, Bondan and Sumantri, Cecep and Suriastini, Wayan and Thomas, Duncan and Garc{\'{i}}a-Villegas, Mauricio and Geraerts, Elke and Schooler, Jonathan W. and Merckelbach, Harald and Jelicic, Marko and Hauer, Beatrijs J a and Ambadar, Zara and Glaeser, Edward L. and Glendon, Spencer and Goodman, Gail S and Ghetti, Simona and Quas, Jodi a and Edelstein, Robin S and Alexander, Kristen Weede and Redlich, Allison D and Cordon, Ingrid M and Jones, David P H and Gordon, Jonathan D and Grant, a and Griffin, L. J. and Han, S. Y. and Harding, David J and Newman, Katherine and Winship, Christoper Christopher and Jr, Young and Anspach, Renee and Clarkwest, Andrew and Harris, Casey T. Celia B S and Paterson, Helen M and Kemp, Richard I and Hart, Rhiannon E. and Schooler, Jonathan W. and Herman, Judith L. and Schatzow, Emily and Hirst, William and Manier, David and History, Source and Summer, No Spring and Funkenstein, Amos and Husbands, Winston and Makoroka, Lydia and Walcott, Rinaldo and Adam, Barry D and George, Clemon and Remis, Robert S and Rourke, Sean B and Hyman and Pentland, Joel and Information, Financial and Policy, Live-in and Jepperson, Ronald L. and Swidler, Ann and Kahan, D. M. and Braman, D. and Gastil, J. and Slovic, P and Mertz, C. K. and Kansteiner, Wolf and Karon, Bertram P. and Widener, Anmarie J. and Kendall-Tackett, K a and Williams, Linda Meyer and Finkelhor, D and Kimball, Bruce A and Krause, Katharine and Kihlstrom, John F and Krimmel, John T. and Tartaro, Christine and Law, Public and Theory, Legal and Paper, Working and Fagan, Jeffrey and Wilkinson, Deanna L and Davies, Garth and Leigh, Jeanna Parsons and Loftus, Elizabeth F and Luckenbill, David F. and Doyle, Daniel P. and Lynch, Mona and Lyons, Christopher J. and Velez, Maria B. and Santoro, Wayne a. and McHugh, Paul R. and Lief, Harold I. and Freyd, Pamela P. and Fetkewicz, Janet M. and McNally, Richard J. and Lasko, Natasha B. and Clancy, Susan a. and Macklin, Michael L. and Pitman, Roger K. and Orr, Scott P. and Medicine, P Pavlov Leningrad and Melchert, Timothy P. and Minow, Martha and Nash, Shondrah Tarrezz and Hesterberg, Latonya and Oberweis, Trish and Musheno, Michael and Olick, Jeffrey K and Levy, Daniel and Robbinsl, Joyce and Pattavina, April and Hirschel, David and Buzawa, Eve and Faggiani, Don and Bentley, Helen and Pearce, Susan C. and Sokoloff, Natalie J. and Perrings, Charles and Peterson, Carole and Sales, Jessica Mcdermott and Rees, Michelle and Fivush, Robyn and Phi, Alpha and Fraternity, Alpha and Leaders, Develops and Brotherhood, Promotes and Excellence, Academic and Service, While Providing and Porter, S and Birt, a R and Yuille, J C and Lehman, D R and Pratt, James Bernard and Press, Chicago California and Pridemore, William Alex and Freilich, Joshua D. and {Raj, A., Silverman}, J. and Ramey, David M. and Reed, Presider Wornie and Tech, Virginia and Dunn, Ronnie and Remedy, Title V I I and Remedy, Title V I I and White, Transparently and White, Transparently and Decisionmaking, Subjective and Decisionmaking, Subjective and Source, Barbara J Flagg and Source, Barbara J Flagg and Yale, The and Yale, The and Journal, Law and Journal, Law and Url, Stable and Url, Stable and Review, Stanford Law Society and Alfieri, V and Rind, B and Tromovitch, P and Bauserman, R and Sampson, Robert J and Bean, Lydia and Hall, William James and Santoro, Emilio and Sarat, Austin and Saunders, David and Scurich, Nicholas and Monahan, John and John, Richard S. and Sokoloff, Natalie J. and Pearce, Susan C. and St, Penn and Nov, Tue and Pdf, Heinonline and Stewart, Eric a. and Simons, Ronald L. and Stokols, Daniel and Lejano, Raul Perez and Hipp, John and Swidler, Ann and Terr, C and Thompson, William C. and Clarke-Stewart, K. Alison and Lepore, Stephen J. and Tiersma, Peter Meijes and William, Source The and Quarterly, Mary and Series, Third and Jan, No and Williams, Linda Meyer and Wynne-Edwards, V C and Brother, Alumni and Burrell, Kenneth and Hawkins, Ron and Chapman, Kenney and Simon, Kevin and Eastman, Kevin and Davis, Justin and Forree, Torrell and Rio, Raymond Del and Pratt, James Bernard and Beckom, Christopher and Tailor, Don and Davis, Turhan and Mcmillon, Edward and Baker, Kevin and Lowe, Ron and Williams, Andrew and Jernigan, William and Brothers, Visiting and Norman, Craig and Jackson, Ronald and Jackson, Phillip D and Brothers, Undergraduate and Kim, Leonardo and Brown, William and Ikeme, Chima and Richardson, Daren and Anne, Papae and Befikadu, Dagem and Russell, Charles and Abubakor, Jibril and Lash, Aaron and Jackson, David and Jr, Stephen Kemp and Allison, Paul D and Ph, D and DeMaris, Alfred and Winship, Christoper Christopher and Morgan, Stephen and Gordon, Rachel a. and Hamilton, Lawrence C and Leslie, Dean and Rev, Fordham L and May, Thu and Pdf, Heinonline and Next, What S and Pratt, James Bernard and Jones, Chantal and Park, Kempton and Contract, Acceptance and Parker, C and Ward, Prof Geoff and Quinlan, Alicia Cole- and Baker, Brianna and Kang, Connie and Quinlan, Alicia Cole- and Simon, Carlisa and Lee, Matthew R T. and Ousey, Graham C and Petersen, N. and Ward, Geoff and Roucek, Joseph and Manza, Jeff and Peterson, Ruth D and Krivo, Lauren J and McCluskey, John D. and McCluskey, Cynthia Perez and Enriquez, Roger and Diamond, J and Ebaugh, Helen Rose and Chafetz, Janet Saltzman and Hayslett-Mccall, K. L. and Bernard, T. J. and Zhou, M. and Kim, S. and Espenshade, Thomas J and Hempstead, Katherine and International, Source and Review, Migration and Summer, No Spring and Espenshade, Thomas J and Velez, Maria B. and Correia, Mark E. and DeSipio, L. and Schrover, Marlou and Vermeulen, Floris and Waldinger, R and Culver, Leigh and Running, D.M. and Ligon, J.B. and Miskioglu, I and Shihadeh, E. S. and Barranco, R. E. and Menjvar, Cecilia and Bejarano, Cynthia and Harris, Casey T. Celia B S and Feldmeyer, Ben and Shihadeh, E. S. and Barranco, R. E. and Presdee, M and Salant, Talya and Lauderdale, Diane S. and Lyons, Christopher J. and Velez, Maria B. and Santoro, Wayne a. and Lee, Matthew R T. and Martinez, Ramiro and Chin, John J. and Neilands, Torsten B. and Weiss, Linda and Mantell, Joanne E. and Wu, Yuning and Sun, Ivan Y. and Smith, Brad W. and McClain, Paula D. and Carter, Niambi M. and {DeFrancesco Soto}, Victoria M. and Lyle, Monique L. and Grynaviski, Jeffrey D. and Nunnally, Shayla C. and Scotto, Thomas J. and Kendrick, J. Alan and Lackey, Gerald F. and Cotton, Kendra Davenport and Stein, R. M. and Post, S. S. and Rinden, a. L. and Kandel, William and Parrado, Emilio a and Running, D.M. and Ligon, J.B. and Miskioglu, I and Kivisto, P. and Running, D.M. and Ligon, J.B. and Miskioglu, I and Feliciano, Cynthia and Running, D.M. and Ligon, J.B. and Miskioglu, I and Neal, Micki and Bohon, Stephanie a. and Responses, Native-born and Foner, Nancy and Desmond, Scott a. and Kubrin, Charis E. and Schwartz, Seth J. and Montgomery, Marilyn J. and Briones, Ervin and Pdq, H and Heyman, Josiah Mcc and Kane, Stephanie C. and {Raj, A., Silverman}, J. and Davis, Robert C. and Henderson, Nicole J. and Osgood, D Wayne and Petersen, N. and Ward, Geoff},
doi = {0803973233},
eprint = {0803973233},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Abused et al. - 2014 - from the SAGE Social Science Collections . All Rights Reserved .pdf:pdf},
isbn = {0893-3200},
issn = {1059-6011},
journal = {Journal of Composite Materials},
keywords = {192,1996,2004,3 million naturalized u,525,531,ANIMAL,Acculturation,Adult,African Continental Ancestry Group,African Continental Ancestry Group: psychology,Americas,Asian immigrants,Attitude,BEHAVIOR,Behavior,Bisexuality,Bisexuality: ethnology,Bisexuality: psychology,Blacks,Child sexual abuse,Codes for violence,Communities,Community culture,Concept Formation,Concept Formation: physiology,Cooperative Behavior,Cultural Background,Cultural identity,Culture,Culture as values,Culture in action,Demographic Factors,Developed Countries,Development,Disaster,Domestic violence,Ecology,Economic Factors,Education,Environment,Ethnic Groups,Ethnicity,Ethnomethodology,Excluded - Not Migration,Excluded on the basis of title and abstract,FEEDBACK,FOOD SUPPLY,Female,Fighting,Gateway,Geographic Factors,Google Scholar,H Social Sciences,HIV Infections,HIV Infections: prevention {\&} control,HIV Infections: psychology,HM Sociology,HN Social history and conditions. Social problems.,HV Social pathology. Social and public welfare,Health,Hispanic,Historical Survey,Homosexuality,Housing,Human Resources,Humans,ICPSR 27501,Immigrant,Immigrants,Immigration,International Migration,Interpersonal Relations,Intersectionality,Intimate partner violence,Labor Force,Latino,Latino Paradox,Life Change Events,Male,Male: ethnology,Male: psychology,Memory,Memory: physiology,Middle Aged,Migrants,Migration,Migration Policy,Models,Neighborhoods,North America,Northern America,Ontario,POPULATION,Personal identity,Police-community relations,Policy,Population,Population Characteristics,Population Dynamics,Population Policy,Power (Psychology),Prison culture,Prison violence community violence,Psychological Factors,Psychological Theory,Psychology,Public Opinion,REPRODUCTION,Race,Racial invariance hypothesis,Recognition (Psychology),Recovered memories,Relational theory of culture,Research,Research Design,Research Methodology,Research Report,Research: trends,Residence Characteristics,Resilience,Rural police,Self Concept,Sexual Partners,Social,Social Policy,Social identity,Spatial Distribution,Street culture,Theoretical,United States,Urban Population,Urban Spatial Distribution--changes,Violence,Violent delinquency,Vulnerability,Women,Young Adult,a series of positive,abuse,accusations of,actuarial risk assessment,aggregate analysis,and involuntary civil com-,and most consistent correlate,apparently,asians,assimilation,attachment,bail determinations,bartusch and matsueda,behavior,behavior transactions,bias crimes,borderlands,citizens voted,coping,crime rates,crime reporting,criminological theories,cross-cultural comparison,decision making,dered logit models,detachment,directed forgetting,dissociation,dissociative amnesia,electoral politics,environment,environment–behavior transactions,established ethnic minorities,ethnic,ethnography,eyewitness testimony,false memories,fieldwork,gender is the strongest,grants in u,hate,hiv,homicide,homicides,how neighborhood,immigrant incorporation,immigrant politics,immigrants,immigrants as well as,immigration,in the 1997 abner,in the recent decade,indicators of the incorporation,institutions,interracial crime,intimate partner assault,ized the understanding of,j nerv ment dis,law rely on assessments,logistic regression,logit models,louima case,lynching,male crime rates,many areas in the,mapping social space,masculinity,matic amnesia has come,more than 8,narrative analysis,naturalization,negative binomial,neighborhoods,numeracy,observation,odds ratios,of crime and delin-,of immi-,of the risk of,on police dealings with,or-,ow does a family,parole decisions,participant,place,poisson,police in,police misconduct in new,police response,policing,political opportunity structures,polytomous logistic regression,posttraumatic stress disorder,predators,probability,psychotherapy,qualitative methods,quency,race and death penalty,racial violence,recovered memory,rehabilitation,religion,repressed memory,repression,resilience,s,same-sex couples,self-control,sex abuse,sexual,slow violence,social capital,social ecology,sociology revolution-,spousal violence,state crime,state dependent learning,stigma,subculture of violence,the 2008 election offers,the chicago school of,the concept of trau-,the explanation of gendered,the postincarcera-,threw a national spotlight,tion hospitalization of sexual,torn asunder by false,transitional justice,transnationalism,two notorious allegations of,under attack,urban studies,victimization,violence,violent,with,york city},
number = {4},
pages = {928--940},
pmid = {803973233},
title = {{from the SAGE Social Science Collections . All Rights Reserved .}},
url = {http://raj.sagepub.com/lookup/doi/10.1177/2153368714567577{\%}5Cnhttp://dx.doi.org/10.1023/A:1007521427059{\%}5Cnhttp://onlinelibrary.wiley.com.prox.lib.ncsu.edu/doi/10.1111/j.1728-4457.2005.00079.x/pdf{\%}5Cnhttp://www.tandfonline.com/doi/abs/10.1080/00380237.200},
volume = {16},
year = {2014}
}
@article{Klimek2015,
abstract = {There is a vast amount of Linked Data on the web spread across a large number of datasets. One of the visions behind Linked Data is that the published data is conveniently reusable by others. This, however, depends on many details such as conformance of the data with commonly used vocabularies and adherence to best practices for data modeling. Therefore, when an expert wants to reuse existing datasets, he still needs to analyze them to discover how the data is modeled and what it actually contains. This may include analysis of what entities are there, how are they linked to other entities, which properties from which vocabularies are used, etc. What is missing is a convenient and fast way of seeing what could be usable in the chosen unknown dataset without reading through its RDF serialization. In this paper we describe use cases based on this problem and their realization using our Linked Data Visualization Model (LDVM) and its new implementation. LDVM is a formal base that exploits the Linked Data principles to ensure interoperability and compatibility of compliant analytic and visualization components. We demonstrate the use cases on examples from the Czech Linked Open Data cloud.},
author = {Kl{\'{i}}mek, Jakub and Necask{\'{y}}, Martin and Helmich, Jir{\'{i}}},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kl{\'{i}}mek, Neˇ, More - Unknown - Use Cases for Linked Data Visualization Model What Can I Combine My Data With To.pdf:pdf},
journal = {WWW2015Workshop: Linked Data on theWeb (LDOW2015)},
keywords = {Linked Data,RDF,discover,visualization},
title = {{Use Cases for Linked Data Visualization Model}},
year = {2015}
}
@article{Berler2005,
abstract = {The advantages of the introduction of information and communication technologies in the complex health-care sector are already well-known and well-stated in the past. It is, nevertheless, paradoxical that although the medical community has embraced with satisfaction most of the technological discoveries allowing the improvement in patient care, this has not happened when talking about health-care informatics. Taking the above issue of concern, our work proposes an information model for knowledge management (KM) based upon the use of key performance indicators (KPIs) in health-care systems. Based upon the use of the balanced scorecard (BSC) framework (Kaplan/Norton) and quality assurance techniques in health care (Donabedian), this paper is proposing a patient journey centered approach that drives information flow at all levels of the day-to-day process of delivering effective and managed care, toward information assessment and knowledge discovery. In order to persuade health-care decision-makers to assess the added value of KM tools, those should be used to propose new performance measurement and performance management techniques at all levels of a health-care system. The proposed KPIs are forming a complete set of metrics that enable the performance management of a regional health-care system. In addition, the performance framework established is technically applied by the use of state-of-the-art KM tools such as data warehouses and business intelligence information systems. In that sense, the proposed infrastructure is, technologically speaking, an important KM tool that enables knowledge sharing amongst various health-care stakeholders and between different health-care groups. The use of BSC is an enabling framework toward a KM strategy in health care.},
author = {Berler, Alexander and Pavlopoulos, Sotiris and Koutsouris, Dimitris},
doi = {10.1109/TITB.2005.847196},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Berler, Pavlopoulos, Koutsouris - 2005 - Using key performance indicators as knowledge-management tools at a regional health-care author.pdf:pdf},
isbn = {1089-7771},
issn = {1089-7771},
journal = {IEEE transactions on information technology in biomedicine : a publication of the IEEE Engineering in Medicine and Biology Society},
keywords = {Greece,Health Care,Information Management,Quality Indicators,Regional Health Planning},
number = {2},
pages = {184--92},
pmid = {16138535},
title = {{Using key performance indicators as knowledge-management tools at a regional health-care authority level.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16138535},
volume = {9},
year = {2005}
}
@article{Betterworks2015,
author = {Betterworks},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Betterworks - 2015 - Getting Started with Objectives {\&} Key Results (OKRs) Best Practices for Implementing OKRs in Your Business.pdf:pdf},
pages = {28},
title = {{Getting Started with Objectives {\&} Key Results (OKRs): Best Practices for Implementing OKRs in Your Business}},
year = {2015}
}
@article{Trinh2014a,
abstract = {Since the emergence of the mashup concept on the web around 2005, a large stream of academic research and industrial development resulted in numerous architecture proposals, platforms and editing tools. This strong initial interest in mashup technologies and promising use case demonstrations notwithstanding, however, commercial platforms such as Microsoft Popfly, IBM Mashup Center, and Google Mashup Editor failed to gain widespread adoption by consumers and enterprises and were eventually discontinued. This failure may be attributed to a number of common limitations of these platforms: (i) they are each useful only for a single or a limited number of restricted problems in specific domains; (ii) they are closed, i.e., developers cannot contribute and share their widgets; (iii) widgets, which are crucial elements of any mashup platform, are usually not modeled in sufficient semantic detail to support widget search and composition features that facilitate reuse. This paper addresses these limitations by introducing an open mashup platform based on semantic web technologies. We present a novel architecture in which widgets equipped with a semantic, graph-based model can cooperate with each other in a mashup created by end users through simple drag and drop operations. Widgets created freely by independent developers and hosted on arbitrary servers can be discovered and combined easily through our introduced semantic search feature.},
author = {Trinh, T.-D. and Wetz, P and Do, B.-L. and Anjomshoaa, A and Kiesling, E and Tjoa, A M},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Trinh et al. - 2014 - Open linked widgets mashup platform.pdf:pdf},
isbn = {16130073 (ISSN)},
journal = {AI Mashup Challenge 2014, AIMashup 2014 - Co-located with 11th Extended Semantic Web Conference, ESWC 2014},
keywords = {Academic research,Graph-based modeling,Industrial development,Industrial research,Mashup platforms,Mashup technologies,Novel architecture,Restricted problem,Semantic Web,Semantic Web technology,Social networking (online)},
title = {{Open linked widgets mashup platform}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84908302021{\&}partnerID=40{\&}md5=bca2fb0281dadd65ae10e84ee01a1960},
volume = {1200},
year = {2014}
}
@article{Gebhardt2012,
abstract = {Given their increasing popularity and novel requirements and characteristics, telco mashups deserve an analysis that goes beyond what's available for mashups in general. Here, the authors cluster telco services into different types, analyze their features, derive a telco mashup reference architecture, and survey how well existing mashup tools can respond to these mashups' novel needs.},
author = {Gebhardt, H and Gaedke, M and Daniel, F and Soi, S and Casati, F and Iglesias, C A and Wilson, S},
doi = {10.1109/MIC.2012.19},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Gebhardt et al. - 2012 - From Mashups to Telco Mashups A Survey.pdf:pdf},
isbn = {1089-7801 VO  - 16},
issn = {10897801},
journal = {Internet Computing, IEEE},
keywords = {Architecture,Architecture Overview Diagram.,Collaboration,Computer architecture,Computer conferencing,Emerging technologies,GSM,Information technology,Internet,Mashups,Miscellaneous,Protocols,Services Architectures,Web-based interaction,Web-based services,and videoconferencing,mashup tools,telco mashup reference architecture,telco services clustering,telecommunication,telecommunication computing,telecommunication services,teleconferencing},
number = {3},
pages = {70--76},
title = {{From Mashups to Telco Mashups: A Survey}},
volume = {16},
year = {2012}
}
@book{Rossi2014,
author = {Rossi, Gustavo and Eds, Marco Winckler and Conference, International and Hutchison, David},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Rossi et al. - 2014 - LNCS 8541 - Web Engineering.pdf:pdf},
isbn = {9783319082448},
title = {{LNCS 8541 - Web Engineering}},
year = {2014}
}
@article{Hoang2014a,
abstract = {The introduction of semantic web and Linked Data helps facilitate sharing of data on the Internet more easily. Subsequently, the resource description framework (RDF) is the standard in publishing structured data resources on the Internet and is used in interconnecting with other data resources. To remedy the data integration issues of the traditional web mashups, the semantic web technology uses the LinkedData based on RDF data model as the unified datamodel for combining, aggregating, and transforming data fromheterogeneous data resources to build LinkedDatamashups.There have been tremendous amounts of efforts of semantic web community to enable Linked Data mashups but there is still lack of a systematic survey on concepts, technologies, applications, and challenges. Therefore, in this paper, we investigate in detail semantic mashups research and application approaches in the information integration. This paper also presents a Linked Data mashup application as an illustration of the proposed approaches.},
author = {Hoang, Hanh Huu and Cung, Tai Nguyen-Phuoc and Truong, Duy Khanh and Hwang, Dosam and Jung, Jason J.},
doi = {10.1155/2014/813875},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Hoang et al. - 2014 - Semantic Information Integration with Linked Data Mashups Approaches.pdf:pdf},
issn = {1550-1329},
journal = {International Journal of Distributed Sensor Networks},
pages = {1--12},
title = {{Semantic Information Integration with Linked Data Mashups Approaches}},
url = {http://www.hindawi.com/journals/ijdsn/2014/813875/},
volume = {2014},
year = {2014}
}
@phdthesis{Tschudnowsky2016,
author = {Tschudnowsky, Alexey},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 1985 - End-User Development of Web-based Decision Support Systems.pdf:pdf},
school = {Technische Universit{\"{a}}t Chemnitz},
title = {{End-User Development of Web-based Decision Support Systems}},
year = {2016}
}
@article{Zhu2022,
abstract = {The maximizing decision-making style describes the style of one who pursues maximum utility in decision-making, in contrast to the satisficing style, which describes the style of one who is satisfied with good enough options. The current research concentrates on the within-person variation in the maximizing decision-making style and provides an explanation through three studies. Study 1 (N = 530) developed a domain-specific maximizing scale and found that individuals had different maximizing tendencies across different domains. Studies 2 (N = 162) and 3 (N = 106) further explored this mechanism from the perspective of subjective task value through questionnaires and experiments. It was found that the within-person variation of maximization in different domains is driven by the difference in the individuals' subjective task value in the corresponding domains. People tend to maximize more in the domains they value more. Our research contributes to a comprehensive understanding of maximization and provides a new perspective for the study of the maximizing decision-making style.},
author = {Zhu, Minfan and Wang, Jun and Xie, Xiaofei},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Wang, Xie - 2022 - Maximize when valuable The domain specificity of maximizing decision-making style.pdf:pdf},
journal = {Judgment and Decision Making},
keywords = {domain specificity,maximizing-satisficing decision-making style,subjec-tive task value},
number = {3},
pages = {574--597},
title = {{Maximize when valuable: The domain specificity of maximizing decision-making style}},
url = {https://orcid.org/0000-0003-0467-9930.},
volume = {17},
year = {2022}
}
@article{Group2023,
author = {Group, Self-organizing Systems},
file = {:Users/baharehzarei/Downloads/End User Development of Smart Home Web of Things Compositions (1).pdf:pdf},
title = {{Goal-oriented End User Development of Web of Things Compositions}},
year = {2023}
}
@book{EUIntegrationOffice.2011,
abstract = {Jim passed away peacefully at home,$\backslash$nwith family by his bedside, on September$\backslash$n13, 2009. Jim was born in Revelstoke$\backslash$nwhere his family worked a farm$\backslash$nin the Big Eddy area. In winters$\backslash$nhe helped run the family trapline up the Columbia River. It$\backslash$nwas during those years the seeds were planted for his two$\backslash$nlifetime sporting passions, bird hunting and fishing.$\backslash$nAfter schooling, Jim worked on the CPR trains until$\backslash$nCanada's involvement in WW II. He enlisted, took pilot training$\backslash$nand was commissioned as a Pilot Officer in late 1943.$\backslash$nHe was awarded the Distinguished Flying Cross for piloting$\backslash$nhis heavily damaged bomber back to safety in England$\backslash$nafter being attacked by a German fighter. A few months$\backslash$nlater his plane was shot down over Berlin. Jim was captured$\backslash$nand spent the remaining months of the war as a POW.$\backslash$nAfter the war Jim enrolled in forestry at UBC, lived in Fort$\backslash$nCamp on the UBC site, married and graduated with the class of$\backslash$n'50. He worked in logging camps on Vancouver Island and the$\backslash$nInterior, before moving to a job in Alberta. In 1956, he returned$\backslash$nto Vancouver to work for the federal Western Forest Products Lab$\backslash$n(now Forintek), received his RPF in 1964 and retired in 1984.$\backslash$nAfter retirement, he volunteered at the Seymour$\backslash$nDemonstration Forest and he made a scholarship endowment$\backslash$nto UBC Forestry. Forestry was in Jim's blood, often a subject of conversation$\backslash$nor debate, and he took great pride knowing$\backslash$nhe was to be part of a three generation RPF family.},
address = {Belgrade},
author = {{EU Integration Office}},
file = {:Users/baharehzarei/Desktop/Papers/Guide to the logical framework approach.pdf:pdf},
isbn = {9788691448509},
pages = {1--78},
publisher = {Republic of Serbia Government European Integration Office},
title = {{the Logical Framework Approach Framework}},
year = {2011}
}
@book{Wild,
author = {Wild, Stefan},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Wild - Unknown - Stefan Wild Enhancing Security in Managing Personal Data by Web Systems.pdf:pdf},
isbn = {9783961000104},
title = {{Stefan Wild Enhancing Security in Managing Personal Data by Web Systems}}
}
@book{Heil,
author = {Heil, Sebastian},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Heil - Unknown - Sebastian Heil Web Migration Revisited Addressing Effort and Risk Concerns.pdf:pdf},
isbn = {9783961001255},
title = {{Sebastian Heil Web Migration Revisited: Addressing Effort and Risk Concerns}}
}
@article{Ponce2022,
author = {Ponce, Victor},
file = {:Users/baharehzarei/Downloads/applsci-12-00479-v2.pdf:pdf},
journal = {Appl. Sci.},
keywords = {context-aware,end-user development,interaction style,literature review,metaphor},
number = {0},
pages = {1--29},
title = {{applied sciences Context-Aware End-User Development Review}},
volume = {12},
year = {2022}
}
@article{Vkdml2012,
author = {Vkdml, M Rwkl and Frp, Jpdlo and Fxvdw, Sklolsv and Lq, D F},
doi = {10.1109/ISDA.2012.6416568},
file = {:Users/baharehzarei/Desktop/Papers/Domain{\_}ontology{\_}based{\_}class{\_}diagram{\_}generation{\_}from{\_}functional{\_}requirements.pdf:pdf},
isbn = {9781467351195},
pages = {380--385},
title = {{Domain Ontology Based Class Diagram Generation from Functional Requirements}},
year = {2012}
}
@article{Zarei2018,
abstract = {Empowering end users to develop web applications for temporal needs is a well-researched web engineering challenge. Increasing usability, lowering the learning overhead and minimizing the gap between user's intention and the provided solution are the ultimate goals of EUD practices. In this paper, we propose an architecture which addresses these challenges by leveraging Semantic Web and Natural Language Processing techniques. Users can directly express their intentions in natural language. The framework identifies the user's domain based on the input intention and then can fetch the domain-related components. Additionally, our approach assists domain experts to generate domain ontologies.},
author = {Zarei, Bahareh and Heil, Sebastian and Gaedke, Martin},
doi = {10.1007/978-3-319-91662-0_38},
file = {:Users/baharehzarei/Desktop/Papers/468391{\_}Print.indd.pdf:pdf},
isbn = {9783319916613},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {End-user development,Natural language processing,Ontology-based development},
pages = {473--476},
title = {{Natural-language-enabled end-user tool endowed with ontology-based development}},
volume = {10845 LNCS},
year = {2018}
}
@article{Santos2019,
abstract = {The End-User Development (EUD) consists of a research area that has been under study for some time and covers a wide variety of domains and types of end users. However, there is still a lack of studies that analyze how EUD research has been reflected in practice. Therefore, this paper contributes to enable an understanding of the current scenario of EUD solutions, revealing trends that are emerging and gaps to be addressed. For this, a systematic literature review was carried out, aiming to characterize the solutions that have been developed using the EUD approach. The results show that most of EUD solutions are for web platform and focus on customizing existing applications, using visual programming techniques as interaction style. However, issues related to quality of use found in some results indicate that more research approaching IHC models, methods and techniques in design and evaluation of EUD tools is needed.},
author = {Santos, Mariana and Villela, Maria Lucia Bento},
doi = {10.1007/978-3-030-22646-6_14},
file = {:Users/baharehzarei/Downloads/485844{\_}1{\_}En{\_}Print.indd.pdf:pdf},
isbn = {9783030226459},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {End-user development,End-user programming,Systematic literature review},
pages = {194--209},
title = {{Characterizing End-User Development Solutions: A Systematic Literature Review}},
volume = {11566 LNCS},
year = {2019}
}
@book{DeFreitas2022,
abstract = {The new ways of manipulating computers, smartphones and other devices have brought challenges such as the need to ensure a good usability when different user types use the same system. Adaptive user interface (AUI) systems are a possible solution. They change the user interface to better meet the needs of different users. However, developing such systems is not trivial. It is necessary to capture the users' characteristics and preferences and constantly adapt the system accordingly. In this paper, we discuss the use of ontologies to support the development of AUI systems. We argue that by providing structured knowledge about such systems, ontologies help understand how they work and offer a basis to structure them, identify the necessary adaptations and implement mechanisms to make them happen in run-time. We have explored the use of ontologies from an ontology network to develop a social network about academic subjects that automatically adapts its interface according to the low vision and colorblind user's needs and usage characteristics. The first version of an ontology-based process to guide the development of AUI systems raised from this experience.},
author = {{De Freitas}, Alexandre A.C. and Scalser, Murilo B. and Costa, Simone D. and Barcellos, Monalessa P.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3554364.3559139},
file = {:Users/baharehzarei/Downloads/Papers/OADAPT{\_}{\_}{\_}IHC{\_}2022{\_}{\_}{\_}LaTeX{\_}Template{\_}{\_}Versao{\_}Final{\_}.pdf:pdf},
isbn = {9781450395069},
keywords = {adaptive user interface,ontology,ontology network},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Towards an ontology-based approach to develop software systems with adaptive user interface}},
volume = {1},
year = {2022}
}
@inproceedings{LakshmiTulasi2017,
author = {{Lakshmi Tulasi}, R. and Rao, Meda Sreenivasa and Ankita, K. and Hgoudar, R.},
booktitle = {Proceedings of the First International Conference on Computational Intelligence and Informatics, Advances in Intelligent Systems and Computing 507},
doi = {10.1007/978-981-10-2471-9_32},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Lakshmi Tulasi et al. - 2017 - Ontology-Based Automatic Annotation An Approach for Efficient Retrieval of Semantic Results of Web Docume.pdf:pdf},
pages = {331--339},
publisher = {Springer, Singapore},
title = {{Ontology-Based Automatic Annotation: An Approach for Efficient Retrieval of Semantic Results of Web Documents}},
url = {http://link.springer.com/10.1007/978-981-10-2471-9{\_}32},
year = {2017}
}
@article{Cremaschi2018,
abstract = {Services composition has been much investigated over the last decade without reaching shared and consolidated results mainly for the lack of interoperable descriptions of services and the consequent need of extensive user intervention. In this paper, we propose a light and practical approach to create machine-readable descriptions of output data that can be merged or used (as-is or adapted) as input data to other services. The solution relies on the popular and standard OpenAPI descriptions augmented with annotations based on JSON-LD format. Services descriptions are created by table annotations techniques applied on sets of given or retrieved output values. The approach has been implemented in a tool and validated with a set of real services.},
author = {Cremaschi, Marco and {De Paoli}, Flavio},
doi = {10.1007/978-3-319-99819-0_10},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Cremaschi, De Paoli - 2018 - A practical approach to services composition through light semantic descriptions.pdf:pdf},
isbn = {9783319998183},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {130--145},
title = {{A practical approach to services composition through light semantic descriptions}},
volume = {11116 LNCS},
year = {2018}
}
@book{Comai2007,
abstract = {Rich Internet Applications (RIAs) are reshaping the way in which the Web works. They change not only the appearance of the$\backslash$n Web interfaces, but also the behavior of applications, permitting novel operations, like data distribution, partial page computation,$\backslash$n and disconnected work. In this paper we try to understand the differences between the behavior of traditional dynamic Web$\backslash$n applications and RIAs, considering the WebML modeling language and its actual implementation.},
author = {Comai, Sara and {Toffetti Carughi}, Giovanni},
booktitle = {Web Engineering},
doi = {10.1007/978-3-540-73597-7},
file = {:Users/baharehzarei/Downloads/icwe2018 (1).pdf:pdf},
isbn = {978-3-540-73596-0},
pages = {364 -- 369},
title = {{Web Engineering}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-73597-7},
volume = {4607},
year = {2007}
}
@article{Kalou2013,
abstract = {The current trends for the future evolution of the web are without doubt the Semantic Web and Web 2.0. A common perception for these two visions is that they are competing. Nevertheless, it becomes more and more obvious that these two concepts are complementary. Semantic web technologies have been considered as a bridge for the technological evolution from Web 2.0 to Web 3.0, the web about recommendation and personalisation. Towards this perspective, in this work, we introduce a framework based on a three-tier architecture that illustrates the potential for combining Web 2.0 mashups and Semantic Web technologies. Based on this framework, we present an application for searching books from Amazon and Half eBay with a focus on personalisation. This implementation purely depends on ontology development, writing of rules for the personalisation, and on creation of a mashup with the aid of web APIs. However, there are several open issues that must be addressed before such applications can become commonplace. The aim of this work is to be a step towards supporting the development of applications which combine the two trends so as to conduce to the term Web 3.0, which is used to describe the next generation web.},
author = {Kalou, Aikaterini K. and Koutsomitropoulos, Dimitrios A. and Papatheodorou, Theodore S.},
doi = {10.1504/IJKWI.2013.056367},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kalou, Koutsomitropoulos, Papatheodorou - 2013 - Semantic web rules and ontologies for developing personalised mashups.pdf:pdf},
issn = {1755-8255},
journal = {International Journal of Knowledge and Web Intelligence},
keywords = {0,3-tier architecture,knowledge representation,mashups,ontology,personalization,rules,semantic web,swrl,web 2,web 3,web apis},
number = {2/3},
pages = {142},
title = {{Semantic web rules and ontologies for developing personalised mashups}},
url = {http://dl.acm.org/citation.cfm?id=2528346.2528348},
volume = {4},
year = {2013}
}
@book{Wroblewska2012,
abstract = {One of the main goals of the SYNAT project is to equip scientific community with a knowledge-based infrastructure providing fast access to relevant scientific information. We have started building an experimental platform where different kinds of stored knowledge will be modeled with the use of ontologies, e.g. reference/system ontology, domain ontologies and auxiliary knowledge including lexical language ontology layers. In our platform we use system ontology defining "system domain" (a kind of meta knowledge) for the scientific community, covering concepts and activities related to the scientific life and domain ontologies dedicated to specific areas of science. Moreover the platform is supposed to include a wide range of tools for building and maintenance of ontologies throughout their life cycle as well as interoperation among the different introduced ontologies. The paper makes a contribution to understanding semantically modeled knowledge and its incorporation into the SYNAT project. We present a review of ontology building, learning, and integration methods and their potential application in the project. {\textcopyright} 2012 Springer-Verlag GmbH Berlin Heidelberg.},
author = {Wr{\'{o}}blewska, Anna and Podsiad{\l}y-Marczykowska, Teresa and Bembenik, Robert and Protaziuk, Grzegorz and Rybi{\'{n}}ski, Henryk},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/978-3-642-24809-2_9},
file = {:Users/baharehzarei/Downloads/Papers/MethodsandTools.pdf:pdf},
isbn = {9783642248085},
issn = {1860949X},
keywords = {Ontology building,ontology integration,ontology maintenance,ontology-based systems,semantic modeling},
number = {January},
pages = {121--151},
title = {{Methods and tools for ontology building, learning and integration - Application in the SYNAT project}},
volume = {390},
year = {2012}
}
@article{Boufrida,
abstract = {Due to the considerable increase in freely available data (especially on the Web), extracting relevant information from textual content is a critical challenge. Most of the available data is embedded in unstructured texts and is not linked to formalized knowledge structures such as ontologies or rules. A potential solution to this problem is to acquire such knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring complex relationships from texts and coding these in the form of rules. Our approach begins with existing domain knowledge represented as an OWL ontology, and applies NLP tools and text matching techniques to deduce different atoms, such as classes, properties and literals, to capture deductive knowledge in the form of new rules. For the reason, to enrich the existing domain ontology by these rules, in order to obtain higher relational expressiveness, make reasoning and produce new facts. The approach was tested using medical reports, specifically, in the specialty of gynecology. It reports an F-measure of 95.83{\%} on test our corpus. {\'{O}} 2020 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
author = {Boufrida, Amina and Boufaida, Zizette},
doi = {10.1016/j.jksuci.2020.05.008},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Boufrida, Boufaida - Unknown - Rule extraction from scientific texts Evaluation in the specialty of gynecology.pdf:pdf},
keywords = {Knowledge extraction,Language (OWL) ontology,Language (SWRL) rules,Natural language processing,Ontology,Rule,Rule acquisition,Semantic,Text mining,Web},
title = {{Rule extraction from scientific texts: Evaluation in the specialty of gynecology}},
url = {https://doi.org/10.1016/j.jksuci.2020.05.008}
}
@techreport{Youn2006,
author = {Youn, Seongwook and McLeod, Dennis},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Youn, McLeod - 2006 - Ontology Development Tools for Ontology-Based Knowledge Management.pdf:pdf},
title = {{Ontology Development Tools for Ontology-Based Knowledge Management}},
url = {http://research.create.usc.edu/nonpublished{\_}reports/100},
year = {2006}
}
@article{Haav2018,
abstract = {Ontology Driven Software Development (ODSD) combines traditional Model Based Software Development (MBSD) techniques with ontology technology in order to provide extensions to and advantages over MBSD. The goal of the paper is to identify current ODSD approaches and to provide qualitative and comparative analysis of the collection of identified approaches. Main research questions of the paper concern the ways of how ontologies are integrated to MBSD process and how their usage advances MBSD. Benefits and challenges of each of the discussed approaches are presented. The analysis is based on literature and projects reviews in the fields of ontology engineering, MBSD and ODSD. The result of the analysis provides understanding of what is the role of ontologies in ODSD and shows whether application of ontology technologies to the MBSD process gives rise to a new paradigm called consistency preserving software development or not.},
author = {Haav, Hele Mai},
doi = {10.15388/Informatica.2018.175},
file = {:Users/baharehzarei/Downloads/Papers/INFO1185{\_}pub.pdf:pdf},
issn = {08684952},
journal = {Informatica (Netherlands)},
keywords = {MBSD,ODSD,model-based software development,ontology,ontology driven software development},
number = {3},
pages = {439--466},
title = {{A comparative study of approaches of ontology driven software development}},
volume = {29},
year = {2018}
}
@article{Muhamad2022,
abstract = {Web services are provided with documents that at the very least specify the endpoint, input parameters, and output or response of each operation to expose their capabilities. This should be considered through an understandable format for humans and/or machines. In the Representational State Transfer (REST) architectural style, the OpenAPI Specification (OAS) is used as a reference to create web service descriptions. However, it only supports syntactic interoperability, leading to the incapability of supporting the automated selection process. To overcome this, OAS documents must be enhanced by including semantics to each resource to provide “understandable” services. Therefore, this study aims to develop a system capable of transforming resources in OAS documents into RDF-based semantic web services. To begin, a relational database schema based on the OAS structure is created to store all objects in the OAS document. The published open-linked vocabulary was then used to create the ontology, which maps resources and their relationships on the RDF data model. To build RDF-based semantic web services, R2RML was used to generate the relational database model into triple RDF. The proposed system was also tested through prototyping and using a dataset of 106 OAS documents, which were downloaded from APIs.guru between 5–10 May 2021. The number of triple RDFs generated per document varied with resource rate. An OAS document generates 36 to 16,505 triple RDF in a dataset. The end product was a triple RDF knowledge base maintained by a graph management database. It is now possible to find service operations, input and output parameters, and service composition requirements utilizing the repository semantic web services using SPARQL. On the other hand, the use of relational databases to store OAS resources increased reuse efficiency by approximately 48{\%}, owing to service developers designing interoperability between uniform parameter services, which were then used as input and output.},
author = {Muhamad, Wardani and Suhardi and Bandung, Yoanes},
doi = {10.1186/s40537-022-00600-8},
file = {:Users/baharehzarei/Downloads/s40537-022-00600-8.pdf:pdf},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {OpenAPI Specification,RDF,Semantic ontology,Semantic web services,Service composition},
number = {1},
publisher = {Springer International Publishing},
title = {{Transforming OpenAPI Specification 3.0 documents into RDF-based semantic web services}},
url = {https://doi.org/10.1186/s40537-022-00600-8},
volume = {9},
year = {2022}
}
@article{Karavisileiou2020,
abstract = {OpenAPI Specification (OAS) defines a description format for REST APIs. In order for a machine to understand the meaning of REST services, OpenAPI service descriptions must be unambiguous. In a previous work we analysed the reasons that cause ambiguities in OpenAPI and showed that, in order to eliminate ambiguities, OpenAPI properties must be semantically annotated and mapped to a semantic model. Leveraging latest results for hypermedia-based construction of Web APIs (i.e. Hydra), the present work forwards this approach and proposes a reference ontology for REST services along with a formal procedure for converting OpenAPI service descriptions to instances of this ontology.},
author = {Karavisileiou, Aikaterini and Mainas, Nikolaos and Petrakis, Euripides G.M.},
doi = {10.1109/ICTAI50040.2020.00016},
file = {:Users/baharehzarei/Downloads/ICTAI2020a.pdf:pdf},
isbn = {9781728192284},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Hydra,Ontology,OpenAPI,REST,Web service},
number = {January 2021},
pages = {35--40},
title = {{Ontology for OpenAPI REST Services Descriptions}},
volume = {2020-Novem},
year = {2020}
}
@article{Liu2023,
abstract = {Tabular data often refers to data that is organized in a table with rows and columns. We observe that this data format is widely used on the Web and within enterprise data repositories. Tables potentially contain rich semantic information that still needs to be interpreted. The process of extracting meaningful information out of tabular data with respect to a semantic artefact, such as an ontology or a knowledge graph, is often referred to as Semantic Table Interpretation (STI) or Semantic Table Annotation. In this survey paper, we aim to provide a comprehensive and up-to-date state-of-the-art review of the different tasks and methods that have been proposed so far to perform STI. First, we propose a new categorization that reflects the heterogeneity of table types that one can encounter, revealing different challenges that need to be addressed. Next, we define five major sub-tasks that STI deals with even if the literature has mostly focused on three sub-tasks so far. We review and group the many approaches that have been proposed into three macro families and we discuss their performance and limitations with respect to the various datasets and benchmarks proposed by the community. Finally, we detail what are the remaining scientific barriers to be able to truly automatically interpret any type of tables that can be found in the wild Web.},
author = {Liu, Jixiong and Chabot, Yoan and Troncy, Rapha{\"{e}}l and Huynh, Viet Phi and Labb{\'{e}}, Thomas and Monnin, Pierre},
doi = {10.1016/j.websem.2022.100761},
file = {:Users/baharehzarei/Desktop/Papers/1-s2.0-S1570826822000452-main.pdf:pdf},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Knowledge graph,Semantic table interpretation,Table annotation,Tabular data},
pages = {100761},
publisher = {Elsevier B.V.},
title = {{From tabular data to knowledge graphs: A survey of semantic table interpretation tasks and methods}},
url = {https://doi.org/10.1016/j.websem.2022.100761},
volume = {76},
year = {2023}
}
@article{DeAlwis1900,
abstract = {The web contains a vast amount of tables which provide useful information across multiple domains. Interpreting these tables contribute to a wide range of Semantic Web applications. Aligning web tables against an ontology to understand their semantics is known as Semantic Table Interpretation (STI). This paper presents a survey on Semantic Table Interpretation(STI). Goal of this paper is to provide an overview of STI algorithms, data-sets used, and their evaluation strategies and critically evaluate prior approaches. In the effort of providing the overview we developed a generic framework to analyze STI algorithms. Using this framework we analyzed the existing algorithms and point out their strengths and weakness. Additionally this enables us to categorize the prior works and be able to point out the key attributes of each categories. Our analysis reveals that search based approaches are better in terms of accuracy and overall completeness, while other categories perform better only in annotating columns with high precision. Also, We present the evaluation methodology utilized in algorithms and discuss the limitations of it while providing suggestions for future improvements. In addition, we point out the design choices in building an STI and their associated trade-offs, which could be of value for the future STI algorithm developers and users.},
author = {{De Alwis}, Lahiru and Dissanayake, Achala and Pallewatte, Manujith and Silva, Kalana and Thayasivam, Uthayasanker},
file = {:Users/baharehzarei/Desktop/Papers/swj1946.pdf:pdf},
keywords = {Semantic Table Interpretation,Semantic Web},
number = {0},
title = {{Semantic Web 0 (0) 1 1 IOS Press Survey on Semantic Table Interpretation}},
volume = {0},
year = {2018}
}
@inproceedings{Zarei2018a,
abstract = {Each day, a vast amount of data is published on the web. In addition , the rate at which content is being published is growing, which has the potential to overwhelm users, particularly those who are technically unskilled. Furthermore, users from various domains of expertise face challenges when trying to retrieve the data they require. They may rely on IT experts, but these experts have limited knowledge of individual domains, making data extraction a time-consuming and error-prone task. It would be beneficial if domain experts were able to retrieve needed data and create relatively complex queries on top of web documents. The existing query solutions either are limited to a specific domain or require beginning with a predefined knowledge base or sample ontologies. To address these limitations, we propose a goal-oriented platform that enables users to easily extract data from web documents. This platform enables users to express their goals in natural language, after which the platform elicits the corresponding result type using the algorithm proposed. The platform also applies the concept of ontology to semantically improve search results. To retrieve the most relevant results from web documents, the segments of a user's query are mapped to the entities of the ontology. Two types of ontologies are used: goal ontologies and domain-specific ones, which comprise domain concepts and the relationships among them. In addition, the platform helps domain experts to generate the domain ontolo-gies that will be used to extract data from web documents. Placing ontologies at the center of the approach integrates a level of semantics into the platform, resulting in more-precise output. The main contributions of this research are that it provides a goal-oriented platform for extracting data from web documents and integrates ontology-based development into web-document searches.},
address = {Halifax},
author = {Zarei, Bahareh and Gaedke, Martin},
booktitle = {DocEng '18 Proceedings of the ACM Symposium on Document Engineering },
doi = {10.1145/3209280.3229099},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Zarei, Gaedke - 2018 - GOWDA Goal-oriented Web Documents Querying tool.pdf:pdf},
isbn = {9781450357692},
pages = {No.47},
publisher = {ACM New York, NY, USA},
title = {{GOWDA: Goal-oriented Web Documents Querying tool}},
url = {https://doi.org/10.1145/3209280.3229099},
volume = {18},
year = {2018}
}
@article{Minhas2017,
author = {Minhas, Sumaira Sultan and Sampaio, Pedro and Mehandjiev, Nikolay},
doi = {10.1145/3147213.3149211},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Minhas, Sampaio, Mehandjiev - 2017 - GO-MaDE Goal Oriented Mashup Development Editor to Provide Extended End User Support for Developing.pdf:pdf},
isbn = {978-1-4503-5149-2},
journal = {Proceedings of the10th International Conference on Utility and Cloud Computing},
keywords = {end users,goals,mashup design tool,service mashups},
pages = {193--194},
title = {{GO-MaDE: Goal Oriented Mashup Development Editor to Provide Extended End User Support for Developing Service Mashups}},
url = {http://doi.acm.org/10.1145/3147213.3149211},
year = {2017}
}
@phdthesis{Aghaee2014a,
abstract = {The emergence of the long-tail in the market of software applications is shifting the role of end-users from mere consumers to becoming developers of applications addressing their unique, personal, and transient needs. On the Web, a popular form of such applications is called mashup, built out of the lightweight composition of Web APIs (reusable software components delivered as a service through the Web). To enable end-users to build mashups, there is a key problem that must be overcome: End-users lack programming knowledge as well as the interest to learn how to master the complex set of Web technologies required to develop mashups. End-User Development (EUD) is an emerging research field dealing with this type of problems. Its main goal is to design tools and techniques facilitating the development of software applications by non-programmers.$\backslash$n$\backslash$nIn this dissertation, we designed and implemented NaturalMash, an EUD system that empowers end-users to develop mashups. NaturalMash adopts a novel hybrid end-user programming technique combining natural language programming with a what-you-see-is-what-you-get interface in a live programming environment. We followed an iterative user-centered design process, in which three formative evaluations drove the incremental design of our system. At the end of the process, we conducted a summative usability evaluation, whose results suggest that the system is highly usable by non-programmers. Also, we proposed a novel benchmarking framework to evaluate mashup tools against each other. Using the framework, we conducted a comparative evaluation of 28 state-of-the-art mashup tools (NaturalMash included) against their expressive power. According to the results, our proposed system has a moderate yet competitive level of expressiveness. All in all, NaturalMash contributes a novel design featuring a unique combination of end-user programming techniques, a suitable metaphor, and the ability to enable an optimal learning experience. Our extensive evaluation results indicate that NaturalMash is located at a sweet spot along the classical trade-off between expressiveness and usability/learnability.$\backslash$n},
author = {Aghaee, Saeed},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Aghaee - 2014 - End-User Development of Mashups Using Live Natural Language Programming(2).pdf:pdf},
keywords = {NaturalMash,Web mashups,end-user software engineering,live programming,natural language programming},
number = {February},
school = {Universit{\`{a}} della Svizzera Italiana},
title = {{End-User Development of Mashups Using Live Natural Language Programming}},
url = {http://doc.rero.ch/record/210369/files/2014INFO002.pdf},
year = {2014}
}
@article{Zarei2021,
author = {Zarei, Bahareh and Gaedke, Martin},
file = {:Users/baharehzarei/Downloads/202018202.pdf:pdf},
journal = {IADIS International Journal on WWW/Internet},
keywords = {chat-bot,conversational interface,natural language,processing,query expansion,service-based systems,web service discovery},
number = {2},
pages = {16--28},
title = {{DISCO : WEB SERVICE DISCOVERY CHATBOT}},
volume = {18},
year = {2021}
}
@article{Behr2023,
abstract = {Ontologies store semantic knowledge in a machine-readable way and represent domain knowledge in controlled vocabulary. In this work, a workflow is set up to derive classes from a text dataset using natural language processing (NLP) methods. Furthermore, ontologies and thesauri are browsed for those classes and corresponding existing textual definitions are extracted. A base ontology is selected to be extended with knowledge from catalysis science, while word similarity is used to introduce new classes to the ontology based on the class candidates. Relations are introduced to automatically reference them to already existing classes in the selected ontology. The workflow is conducted for a text dataset related to catalysis research on methanation of CO 2 and seven semantic artifacts assisting ontology extension by domain experts. Undefined concepts and unstructured relations can be more easily introduced automatically into existing ontologies. Domain experts can then revise the resulting extended ontology by choosing the best fitting definition of a class and specifying suggested relations between concepts of catalyst research. A structured extension of ontologies supported by NLP methods is made possible to facilitate a Findable, Accessible, Interoperable, Reusable (FAIR) data management workflow.},
author = {Behr, Alexander S. and V{\"{o}}lkenrath, Marc and Kockmann, Norbert},
doi = {10.1007/s10115-023-01919-1},
file = {:Users/baharehzarei/Downloads/s10115-023-01919-1.pdf:pdf},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Automated ontology annotation,CO2 methanation,Catalytic conversion,Information extraction,Natural language processing,Ontology},
number = {12},
pages = {5503--5522},
publisher = {Springer London},
title = {{Ontology extension with NLP-based concept extraction for domain experts in catalytic sciences}},
url = {https://doi.org/10.1007/s10115-023-01919-1},
volume = {65},
year = {2023}
}
@article{Ardito2018,
abstract = {Domain specificity is largely recognized as a means to foster the adoption of systems by specific communities of non-technical users. This paper presents an architecture for the development of Task-Automation Systems that can be customized in specific domains. It is one of the results of a human-centred design process we performed to support non-technical people to program the behaviour of smart objects by defining event-condition-action (ECA) rules. We illustrate the main modules of the proposed architecture, also describing how it supports the creation of ECA rules constrained by means of temporal and spatial conditions. Finally, we report on the development of a Task-Automation System customized by developing and comparing three different composition paradigms.},
author = {Ardito, Carmelo and Desolda, Giuseppe and Matera, Maristella},
doi = {10.1007/978-3-319-74433-9_9},
file = {:Users/baharehzarei/Downloads/Current Trends in Web Engineering.pdf:pdf},
isbn = {9783319744322},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Domain specificity,End-User Development,Internet of Thing,Task-Automation Systems},
pages = {108--119},
title = {{Engineering Task-Automation Systems for Domain Specificity}},
volume = {10544 LNCS},
year = {2018}
}
@inproceedings{Mathews2017,
address = {Austin, TX, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.08448v3},
author = {Mathews, Kevin Alex and Kumar, P. Sreenivasa},
booktitle = {K-CAP '17: Proceedings of the 9th Knowledge Capture Conference},
doi = {10.1145/3148011.3148034},
eprint = {arXiv:1709.08448v3},
file = {:Users/baharehzarei/Desktop/Papers/notes.pdf:pdf},
keywords = {knowledge extraction,ontology learning,text analytics},
pages = {1--4},
publisher = {Association for Computing Machinery},
title = {{Extracting Ontological Knowledge from Textual Descriptions through Grammar-based Transformation}},
url = {https://doi.org/10.1145/3148011.3148034},
year = {2017}
}
@article{Ed-douibi2017,
abstract = {Representational State Transfer (REST) has become the dominant approach to designWeb APIs nowadays, resulting in thousands of public REST Web APIs offering access to a variety of data sources (e.g., open-data initiatives) or advanced functionalities (e.g., geolocation services). Unfortunately, most of these APIs do not come with any specification that developers (and machines) can rely on to automatically understand and integrate them. Instead, most of the time we have to rely on reading its ad-hoc documentation web pages, despite the existence of languages like Swagger or, more recently, OpenAPI that developers could use to formally describe their APIs. In this paper we present an example-driven discovery process that generates model-based OpenAPI specifications for REST Web APIs by using API call examples. A tool implementing our approach and a community-driven repository for the discovered APIs are also presented.},
author = {Ed-douibi, Hamza and Izquierdo, Javier Luis C{\'{a}}novas and Cabot, Jordi},
doi = {10.1007/978-3-319-61482-3_16},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Ed-douibi, Izquierdo, Cabot - 2017 - Example-driven web API specification discovery.pdf:pdf},
isbn = {9783319614816},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Discovery process,OpenAPI,REST web APIs,Repository},
pages = {267--284},
title = {{Example-driven web API specification discovery}},
volume = {10376 LNCS},
year = {2017}
}
@article{Shao2021,
abstract = {A lot of new scientific documents are being published on various platforms every day. It is more and more imperative to quickly and efficiently discover new words and meanings from these documents. However, most of the related works rely on labeled data, and it is quite difficult to deal with unlabeled new documents efficiently. For this, we have introduced an unsupervised method based on sentence patterns and part of speech (POS) sequences. Our method just needs a few initial learnable patterns to obtain the initial terminology tokens and their POS sequences. In this process, new patterns are constructed and can match more sentences to find more POS sequences of terminology. Finally, we use obtained POS sequences and sentence patterns to extract terminology terms in new scientific text. Experiments on paper abstracts from Web of Knowledge show that this method is practical and can achieve a good performance on our test data.},
author = {Shao, Wei and Hua, Bolin and Song, Linqi},
doi = {10.2478/dim-2021-0005},
file = {:Users/baharehzarei/Downloads/1-s2.0-S2543925122000031-main.pdf:pdf},
issn = {25439251},
journal = {Data and Information Management},
keywords = {auto-learning,scientific text,terminology extraction,unsupervised method},
number = {3},
pages = {329--335},
publisher = {Wei Shao et al., published by Sciendo},
title = {{A Pattern and POS Auto-Learning Method for Terminology Extraction from Scientific Text}},
url = {http://dx.doi.org/10.2478/dim-2021-0005},
volume = {5},
year = {2021}
}
@book{Kim2016a,
abstract = {As the web technology rapidly advances, web services are making significant strides in the Web 2.0 era which goes beyond the existing closed web services and enables one to open and share information freely. Web services are usually provided in a form that uses Open Application Programming Interfaces (Open APIs) as the web service interface. As the number of open APIs increases, mash-up services that combine several Open APIs have begun to proliferate in the Web 2.0 environment. However, the current mash-up service generation method poses the problem in that normal users, who are not web service developers, have trouble generating mash-up services because Open API analysis and the programming skills are required. This kind of developer-centric service generation method is unable to satisfy normal service user demands for personalized services. As a consequence, we propose an ontology-based Open API composition method that can easily generate the mash-up services requested by users, based on ontology that can support semantic characteristics.},
author = {Kim, Sang Il and Kim, Hwa Sung},
booktitle = {2016 International Conference on Information Networking (ICOIN)},
doi = {10.1109/ICOIN.2016.7427130},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Kim - 2016 - Ontology-based open API composition method for automatic mash-up service generation Ontology-based open API compositio.pdf:pdf},
isbn = {9781509017249},
keywords = {Composition,Open API,mash-up,web service},
title = {{Ontology-based open API composition method for automatic mash-up service generation; Ontology-based open API composition method for automatic mash-up service generation}},
year = {2016}
}
@article{Sangsanit2018,
abstract = {Today, web technology is very important for various sectors. Business, education, communication, the military and medicine all use web technology. Web services are an aspect of web technology important for workflow creation. Web services are widely used in various sectors, such as business, education, communication and medicine. Web services allow the creation of new work processes or provide access to existing processes, as well as facilitating data exchange via a network. Combining the various processes and operations of a service also creates challenges, whether in the form of human errors that need to be eliminated or the creation of duplicate services. By using automatic operation to complete tasks - from gathering components of a web service together, to planning and developing a system's drive - facilitates and controls the operations of a web service's composition and reduces human errors that appear during web service composition. This research surveys various automations and techniques for web service composition and compares 18 methods, detailing each method's advantages, disadvantages and challenges. This research offers the opportunity to study this field and its problems, provide more diverse data and examine the problems of traffic networks.},
author = {Sangsanit, Krisada and Kurutach, Werasak and Phoomvuthisarn, Suronapee},
doi = {10.1109/ICOIN.2018.8343096},
file = {:Users/baharehzarei/Library/Application Support/Mendeley Desktop/Downloaded/Sangsanit, Kurutach, Phoomvuthisarn - 2018 - REST web service composition A survey of automation and techniques.pdf:pdf},
isbn = {9781538622896},
issn = {19767684},
journal = {International Conference on Information Networking},
keywords = {Automation,REST service composition,REST web service,RESTFul web service composition automation,Web service,Web service composition,Web service composition automation},
pages = {116--121},
publisher = {IEEE},
title = {{REST web service composition: A survey of automation and techniques}},
volume = {2018-Janua},
year = {2018}
}
